{'batch_size': 256, 'epochs': 200, 'lr': 0.001, 'parent_path': '../data_for_GMM-Master/', 'loc_dim': 32, 'layer': 4, 'beam_size': 5, 'wd': 0.0, 'dev_id': 1, 'use_gcn': 1, 'atten_flag': 1, 'tf_ratio': 0.0}
Loading Dataset Done!!!
get graph extra data finished!
Loading model Done!!!
================Epoch: 1================
Iteration 1: train_loss 9.064269065856934
Iteration 2: train_loss 9.029722213745117
Iteration 3: train_loss 8.992411613464355
Iteration 4: train_loss 8.930716514587402
Iteration 5: train_loss 8.890149116516113
Iteration 6: train_loss 8.809330940246582
Iteration 7: train_loss 8.738338470458984
Iteration 8: train_loss 8.656997680664062
Iteration 9: train_loss 8.572724342346191
Iteration 10: train_loss 8.4874267578125
Iteration 11: train_loss 8.36566162109375
Iteration 12: train_loss 8.2533540725708
Iteration 13: train_loss 8.182608604431152
Iteration 14: train_loss 8.122541427612305
Iteration 15: train_loss 8.049745559692383
Iteration 16: train_loss 7.91421365737915
Iteration 17: train_loss 7.861715316772461
Iteration 18: train_loss 7.791923522949219
Iteration 19: train_loss 7.724503517150879
Iteration 20: train_loss 7.608750820159912
Iteration 21: train_loss 7.555843830108643
Iteration 22: train_loss 7.522557735443115
Iteration 23: train_loss 7.456165313720703
Iteration 24: train_loss 7.3965067863464355
Iteration 25: train_loss 7.2392096519470215
Iteration 26: train_loss 7.253358364105225
Iteration 27: train_loss 7.097543239593506
Iteration 28: train_loss 7.15078067779541
Iteration 29: train_loss 7.104259490966797
Iteration 30: train_loss 7.060638904571533
Iteration 31: train_loss 6.903105735778809
Iteration 32: train_loss 6.961905002593994
Iteration 33: train_loss 6.942474365234375
Iteration 34: train_loss 6.927676200866699
Iteration 35: train_loss 6.826291084289551
Iteration 36: train_loss 6.897294521331787
Iteration 37: train_loss 6.863150119781494
Iteration 38: train_loss 6.818942070007324
Iteration 39: train_loss 6.708952903747559
Iteration 40: train_loss 7.297067165374756
Epoch 1: train_avg_loss 7.750770699977875 eval_avg_acc: 0.023809568338424743 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:41:10] [32mIntermediate result: 0.023809568338424743  (Index 0)[0m
================Epoch: 2================
Iteration 1: train_loss 6.6591477394104
Iteration 2: train_loss 6.6290693283081055
Iteration 3: train_loss 6.464404106140137
Iteration 4: train_loss 6.463015556335449
Iteration 5: train_loss 6.372105598449707
Iteration 6: train_loss 6.569003105163574
Iteration 7: train_loss 6.407749176025391
Iteration 8: train_loss 6.4068217277526855
Iteration 9: train_loss 6.404270648956299
Iteration 10: train_loss 6.30637788772583
Iteration 11: train_loss 6.340972423553467
Iteration 12: train_loss 6.301321506500244
Iteration 13: train_loss 6.310051441192627
Iteration 14: train_loss 6.316188335418701
Iteration 15: train_loss 6.3707475662231445
Iteration 16: train_loss 6.205101013183594
Iteration 17: train_loss 6.222532749176025
Iteration 18: train_loss 6.226069450378418
Iteration 19: train_loss 6.172103404998779
Iteration 20: train_loss 6.2356977462768555
Iteration 21: train_loss 6.052027702331543
Iteration 22: train_loss 6.253375053405762
Iteration 23: train_loss 6.140446662902832
Iteration 24: train_loss 5.999635696411133
Iteration 25: train_loss 6.084719657897949
Iteration 26: train_loss 6.1113362312316895
Iteration 27: train_loss 6.039367198944092
Iteration 28: train_loss 5.895883560180664
Iteration 29: train_loss 5.922552108764648
Iteration 30: train_loss 6.011919021606445
Iteration 31: train_loss 5.877999782562256
Iteration 32: train_loss 5.9052252769470215
Iteration 33: train_loss 5.926578044891357
Iteration 34: train_loss 5.790863990783691
Iteration 35: train_loss 5.852047443389893
Iteration 36: train_loss 5.7220892906188965
Iteration 37: train_loss 5.887810707092285
Iteration 38: train_loss 5.806136131286621
Iteration 39: train_loss 5.685256004333496
Iteration 40: train_loss 6.346369743347168
Epoch 2: train_avg_loss 6.167359745502472 eval_avg_acc: 0.04660060715421307 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:41:22] [32mIntermediate result: 0.04660060715421307  (Index 1)[0m
================Epoch: 3================
Iteration 1: train_loss 5.658897399902344
Iteration 2: train_loss 5.722680568695068
Iteration 3: train_loss 5.789455413818359
Iteration 4: train_loss 5.6375555992126465
Iteration 5: train_loss 5.6255693435668945
Iteration 6: train_loss 5.559407711029053
Iteration 7: train_loss 5.6618499755859375
Iteration 8: train_loss 5.516351699829102
Iteration 9: train_loss 5.627169132232666
Iteration 10: train_loss 5.664588928222656
Iteration 11: train_loss 5.60274600982666
Iteration 12: train_loss 5.503454208374023
Iteration 13: train_loss 5.526324272155762
Iteration 14: train_loss 5.5903496742248535
Iteration 15: train_loss 5.500026702880859
Iteration 16: train_loss 5.516233444213867
Iteration 17: train_loss 5.456793785095215
Iteration 18: train_loss 5.432465076446533
Iteration 19: train_loss 5.489034175872803
Iteration 20: train_loss 5.4226393699646
Iteration 21: train_loss 5.347209930419922
Iteration 22: train_loss 5.494022369384766
Iteration 23: train_loss 5.338295936584473
Iteration 24: train_loss 5.431701183319092
Iteration 25: train_loss 5.360347270965576
Iteration 26: train_loss 5.356716632843018
Iteration 27: train_loss 5.275701522827148
Iteration 28: train_loss 5.294888973236084
Iteration 29: train_loss 5.293893337249756
Iteration 30: train_loss 5.318978786468506
Iteration 31: train_loss 5.439681053161621
Iteration 32: train_loss 5.231930732727051
Iteration 33: train_loss 5.198840141296387
Iteration 34: train_loss 5.393930435180664
Iteration 35: train_loss 5.192581653594971
Iteration 36: train_loss 5.218942165374756
Iteration 37: train_loss 5.221858978271484
Iteration 38: train_loss 5.223034858703613
Iteration 39: train_loss 5.25890588760376
Iteration 40: train_loss 5.431549549102783
Epoch 3: train_avg_loss 5.445665097236633 eval_avg_acc: 0.06952676551135076 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:41:34] [32mIntermediate result: 0.06952676551135076  (Index 2)[0m
================Epoch: 4================
Iteration 1: train_loss 5.094651699066162
Iteration 2: train_loss 5.115207195281982
Iteration 3: train_loss 5.132386684417725
Iteration 4: train_loss 5.086426734924316
Iteration 5: train_loss 5.056842803955078
Iteration 6: train_loss 5.083250045776367
Iteration 7: train_loss 5.069383144378662
Iteration 8: train_loss 4.985657691955566
Iteration 9: train_loss 4.9823198318481445
Iteration 10: train_loss 5.020808696746826
Iteration 11: train_loss 4.969832420349121
Iteration 12: train_loss 5.221312046051025
Iteration 13: train_loss 5.112087726593018
Iteration 14: train_loss 5.09480094909668
Iteration 15: train_loss 4.945630073547363
Iteration 16: train_loss 5.010556221008301
Iteration 17: train_loss 5.02822732925415
Iteration 18: train_loss 4.982222557067871
Iteration 19: train_loss 4.877432346343994
Iteration 20: train_loss 4.867166996002197
Iteration 21: train_loss 4.887254238128662
Iteration 22: train_loss 4.994245529174805
Iteration 23: train_loss 4.98568058013916
Iteration 24: train_loss 4.875260829925537
Iteration 25: train_loss 4.9604058265686035
Iteration 26: train_loss 4.915143966674805
Iteration 27: train_loss 4.797125816345215
Iteration 28: train_loss 4.837007522583008
Iteration 29: train_loss 4.86965799331665
Iteration 30: train_loss 4.917172908782959
Iteration 31: train_loss 4.798127174377441
Iteration 32: train_loss 4.844358921051025
Iteration 33: train_loss 4.851647853851318
Iteration 34: train_loss 4.814266204833984
Iteration 35: train_loss 4.777584552764893
Iteration 36: train_loss 4.735404968261719
Iteration 37: train_loss 4.829946517944336
Iteration 38: train_loss 4.740011215209961
Iteration 39: train_loss 4.770007133483887
Iteration 40: train_loss 4.463255882263184
Epoch 4: train_avg_loss 4.934994220733643 eval_avg_acc: 0.08568368488212541 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:41:46] [32mIntermediate result: 0.08568368488212541  (Index 3)[0m
================Epoch: 5================
Iteration 1: train_loss 4.692479610443115
Iteration 2: train_loss 4.635838031768799
Iteration 3: train_loss 4.686163902282715
Iteration 4: train_loss 4.573851108551025
Iteration 5: train_loss 4.628767967224121
Iteration 6: train_loss 4.7110595703125
Iteration 7: train_loss 4.668954372406006
Iteration 8: train_loss 4.635485649108887
Iteration 9: train_loss 4.64039945602417
Iteration 10: train_loss 4.623452186584473
Iteration 11: train_loss 4.645418167114258
Iteration 12: train_loss 4.63776159286499
Iteration 13: train_loss 4.676387310028076
Iteration 14: train_loss 4.541306018829346
Iteration 15: train_loss 4.578155040740967
Iteration 16: train_loss 4.632146835327148
Iteration 17: train_loss 4.625288486480713
Iteration 18: train_loss 4.603028774261475
Iteration 19: train_loss 4.445654392242432
Iteration 20: train_loss 4.568999290466309
Iteration 21: train_loss 4.5085554122924805
Iteration 22: train_loss 4.553818225860596
Iteration 23: train_loss 4.603267192840576
Iteration 24: train_loss 4.4951629638671875
Iteration 25: train_loss 4.514656066894531
Iteration 26: train_loss 4.4142351150512695
Iteration 27: train_loss 4.5195465087890625
Iteration 28: train_loss 4.412472724914551
Iteration 29: train_loss 4.568691730499268
Iteration 30: train_loss 4.614293575286865
Iteration 31: train_loss 4.456709861755371
Iteration 32: train_loss 4.497842788696289
Iteration 33: train_loss 4.369077205657959
Iteration 34: train_loss 4.397596836090088
Iteration 35: train_loss 4.4756550788879395
Iteration 36: train_loss 4.464723110198975
Iteration 37: train_loss 4.467312812805176
Iteration 38: train_loss 4.45515775680542
Iteration 39: train_loss 4.478766918182373
Iteration 40: train_loss 4.4340667724609375
Epoch 5: train_avg_loss 4.553805160522461 eval_avg_acc: 0.09283846925755362 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:41:58] [32mIntermediate result: 0.09283846925755362  (Index 4)[0m
================Epoch: 6================
Iteration 1: train_loss 4.475348472595215
Iteration 2: train_loss 4.284082412719727
Iteration 3: train_loss 4.3531365394592285
Iteration 4: train_loss 4.287685871124268
Iteration 5: train_loss 4.26585054397583
Iteration 6: train_loss 4.334285259246826
Iteration 7: train_loss 4.251474380493164
Iteration 8: train_loss 4.321517467498779
Iteration 9: train_loss 4.317296504974365
Iteration 10: train_loss 4.277593612670898
Iteration 11: train_loss 4.2396955490112305
Iteration 12: train_loss 4.260898590087891
Iteration 13: train_loss 4.394346714019775
Iteration 14: train_loss 4.423345565795898
Iteration 15: train_loss 4.284236431121826
Iteration 16: train_loss 4.278117656707764
Iteration 17: train_loss 4.323399543762207
Iteration 18: train_loss 4.287796497344971
Iteration 19: train_loss 4.248155117034912
Iteration 20: train_loss 4.172599792480469
Iteration 21: train_loss 4.228967189788818
Iteration 22: train_loss 4.267632961273193
Iteration 23: train_loss 4.215459823608398
Iteration 24: train_loss 4.275521755218506
Iteration 25: train_loss 4.253856658935547
Iteration 26: train_loss 4.189938068389893
Iteration 27: train_loss 4.413128852844238
Iteration 28: train_loss 4.338606357574463
Iteration 29: train_loss 4.276568412780762
Iteration 30: train_loss 4.181103229522705
Iteration 31: train_loss 4.196824073791504
Iteration 32: train_loss 4.29127311706543
Iteration 33: train_loss 4.233085632324219
Iteration 34: train_loss 4.085443496704102
Iteration 35: train_loss 4.167953014373779
Iteration 36: train_loss 4.07810640335083
Iteration 37: train_loss 4.050263404846191
Iteration 38: train_loss 4.1972737312316895
Iteration 39: train_loss 4.116748332977295
Iteration 40: train_loss 3.9223244190216064
Epoch 6: train_avg_loss 4.2515235364437105 eval_avg_acc: 0.11278135061387776 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:42:10] [32mIntermediate result: 0.11278135061387776  (Index 5)[0m
================Epoch: 7================
Iteration 1: train_loss 4.175161361694336
Iteration 2: train_loss 4.052771091461182
Iteration 3: train_loss 4.145269870758057
Iteration 4: train_loss 4.0511908531188965
Iteration 5: train_loss 3.9974844455718994
Iteration 6: train_loss 4.000612258911133
Iteration 7: train_loss 4.018026828765869
Iteration 8: train_loss 4.067219257354736
Iteration 9: train_loss 4.004730701446533
Iteration 10: train_loss 3.8806099891662598
Iteration 11: train_loss 4.0091094970703125
Iteration 12: train_loss 4.072233200073242
Iteration 13: train_loss 4.136105537414551
Iteration 14: train_loss 4.0520172119140625
Iteration 15: train_loss 4.019266605377197
Iteration 16: train_loss 3.9143197536468506
Iteration 17: train_loss 4.135740280151367
Iteration 18: train_loss 4.079667568206787
Iteration 19: train_loss 4.039252281188965
Iteration 20: train_loss 4.0529022216796875
Iteration 21: train_loss 4.0349273681640625
Iteration 22: train_loss 4.101711750030518
Iteration 23: train_loss 3.8850932121276855
Iteration 24: train_loss 4.08325719833374
Iteration 25: train_loss 4.106439113616943
Iteration 26: train_loss 4.102468967437744
Iteration 27: train_loss 3.9920835494995117
Iteration 28: train_loss 4.1326212882995605
Iteration 29: train_loss 3.9340245723724365
Iteration 30: train_loss 3.994062662124634
Iteration 31: train_loss 4.046278476715088
Iteration 32: train_loss 3.931096315383911
Iteration 33: train_loss 3.946101188659668
Iteration 34: train_loss 4.124138832092285
Iteration 35: train_loss 3.8783934116363525
Iteration 36: train_loss 4.010153293609619
Iteration 37: train_loss 3.930741548538208
Iteration 38: train_loss 3.963332176208496
Iteration 39: train_loss 3.9393138885498047
Iteration 40: train_loss 3.5790345668792725
Epoch 7: train_avg_loss 4.015474104881287 eval_avg_acc: 0.1417025568815076 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:42:23] [32mIntermediate result: 0.1417025568815076  (Index 6)[0m
================Epoch: 8================
Iteration 1: train_loss 3.8097431659698486
Iteration 2: train_loss 3.805577039718628
Iteration 3: train_loss 4.009873867034912
Iteration 4: train_loss 3.818279981613159
Iteration 5: train_loss 3.8568332195281982
Iteration 6: train_loss 3.8928184509277344
Iteration 7: train_loss 3.804112195968628
Iteration 8: train_loss 3.8093514442443848
Iteration 9: train_loss 3.988987922668457
Iteration 10: train_loss 3.7927024364471436
Iteration 11: train_loss 3.8622682094573975
Iteration 12: train_loss 3.760735273361206
Iteration 13: train_loss 3.817715883255005
Iteration 14: train_loss 3.8813040256500244
Iteration 15: train_loss 3.816868305206299
Iteration 16: train_loss 3.8831112384796143
Iteration 17: train_loss 3.811948299407959
Iteration 18: train_loss 3.774867296218872
Iteration 19: train_loss 3.942966938018799
Iteration 20: train_loss 3.7373809814453125
Iteration 21: train_loss 3.871617078781128
Iteration 22: train_loss 4.046004772186279
Iteration 23: train_loss 3.885840892791748
Iteration 24: train_loss 3.8370418548583984
Iteration 25: train_loss 3.911515951156616
Iteration 26: train_loss 3.869994640350342
Iteration 27: train_loss 3.889740467071533
Iteration 28: train_loss 3.711974620819092
Iteration 29: train_loss 3.8148162364959717
Iteration 30: train_loss 3.8820905685424805
Iteration 31: train_loss 3.824134111404419
Iteration 32: train_loss 3.764749765396118
Iteration 33: train_loss 3.771731376647949
Iteration 34: train_loss 3.8431077003479004
Iteration 35: train_loss 3.8089845180511475
Iteration 36: train_loss 3.8922295570373535
Iteration 37: train_loss 3.8792004585266113
Iteration 38: train_loss 3.8110108375549316
Iteration 39: train_loss 3.757098436355591
Iteration 40: train_loss 3.688939332962036
Epoch 8: train_avg_loss 3.840981733798981 eval_avg_acc: 0.10424821780832579 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:42:36] [32mIntermediate result: 0.10424821780832579  (Index 7)[0m
================Epoch: 9================
Iteration 1: train_loss 3.7792935371398926
Iteration 2: train_loss 3.8723251819610596
Iteration 3: train_loss 3.590263605117798
Iteration 4: train_loss 3.729763984680176
Iteration 5: train_loss 3.7410717010498047
Iteration 6: train_loss 3.735478639602661
Iteration 7: train_loss 3.750462770462036
Iteration 8: train_loss 3.7378041744232178
Iteration 9: train_loss 3.703486442565918
Iteration 10: train_loss 3.772303342819214
Iteration 11: train_loss 3.641911029815674
Iteration 12: train_loss 3.555800676345825
Iteration 13: train_loss 3.703108310699463
Iteration 14: train_loss 3.632728338241577
Iteration 15: train_loss 3.660947561264038
Iteration 16: train_loss 3.5784120559692383
Iteration 17: train_loss 3.7279999256134033
Iteration 18: train_loss 3.659048318862915
Iteration 19: train_loss 3.6932125091552734
Iteration 20: train_loss 3.811718225479126
Iteration 21: train_loss 3.634870767593384
Iteration 22: train_loss 3.631643533706665
Iteration 23: train_loss 3.7748732566833496
Iteration 24: train_loss 3.58439040184021
Iteration 25: train_loss 3.7060413360595703
Iteration 26: train_loss 3.661557674407959
Iteration 27: train_loss 3.7446694374084473
Iteration 28: train_loss 3.6126832962036133
Iteration 29: train_loss 3.605118989944458
Iteration 30: train_loss 3.593317985534668
Iteration 31: train_loss 3.7086071968078613
Iteration 32: train_loss 3.72106671333313
Iteration 33: train_loss 3.528297185897827
Iteration 34: train_loss 3.5959746837615967
Iteration 35: train_loss 3.6451897621154785
Iteration 36: train_loss 3.6147663593292236
Iteration 37: train_loss 3.625452756881714
Iteration 38: train_loss 3.7642695903778076
Iteration 39: train_loss 3.714043617248535
Iteration 40: train_loss 4.164304256439209
Epoch 9: train_avg_loss 3.6927069783210755 eval_avg_acc: 0.15329135833874857 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:42:48] [32mIntermediate result: 0.15329135833874857  (Index 8)[0m
================Epoch: 10================
Iteration 1: train_loss 3.4991238117218018
Iteration 2: train_loss 3.5365774631500244
Iteration 3: train_loss 3.5472798347473145
Iteration 4: train_loss 3.577977418899536
Iteration 5: train_loss 3.5876541137695312
Iteration 6: train_loss 3.484545946121216
Iteration 7: train_loss 3.598538637161255
Iteration 8: train_loss 3.5225989818573
Iteration 9: train_loss 3.598877429962158
Iteration 10: train_loss 3.67852520942688
Iteration 11: train_loss 3.4849560260772705
Iteration 12: train_loss 3.5747194290161133
Iteration 13: train_loss 3.584153413772583
Iteration 14: train_loss 3.6371140480041504
Iteration 15: train_loss 3.451826333999634
Iteration 16: train_loss 3.5516304969787598
Iteration 17: train_loss 3.4985945224761963
Iteration 18: train_loss 3.6620233058929443
Iteration 19: train_loss 3.486354351043701
Iteration 20: train_loss 3.5825796127319336
Iteration 21: train_loss 3.533149003982544
Iteration 22: train_loss 3.426313877105713
Iteration 23: train_loss 3.440669298171997
Iteration 24: train_loss 3.6009178161621094
Iteration 25: train_loss 3.453817129135132
Iteration 26: train_loss 3.5655384063720703
Iteration 27: train_loss 3.606984853744507
Iteration 28: train_loss 3.5172557830810547
Iteration 29: train_loss 3.4903714656829834
Iteration 30: train_loss 3.6478006839752197
Iteration 31: train_loss 3.4568963050842285
Iteration 32: train_loss 3.5589599609375
Iteration 33: train_loss 3.483736038208008
Iteration 34: train_loss 3.540393114089966
Iteration 35: train_loss 3.6028075218200684
Iteration 36: train_loss 3.4812912940979004
Iteration 37: train_loss 3.5455567836761475
Iteration 38: train_loss 3.501063823699951
Iteration 39: train_loss 3.5867364406585693
Iteration 40: train_loss 3.1441526412963867
Epoch 10: train_avg_loss 3.533251565694809 eval_avg_acc: 0.150197229659209 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:43:01] [32mIntermediate result: 0.150197229659209  (Index 9)[0m
================Epoch: 11================
Iteration 1: train_loss 3.4994029998779297
Iteration 2: train_loss 3.505455493927002
Iteration 3: train_loss 3.4814956188201904
Iteration 4: train_loss 3.567715644836426
Iteration 5: train_loss 3.525880813598633
Iteration 6: train_loss 3.4334466457366943
Iteration 7: train_loss 3.3542861938476562
Iteration 8: train_loss 3.506162405014038
Iteration 9: train_loss 3.3843834400177
Iteration 10: train_loss 3.361253261566162
Iteration 11: train_loss 3.432095766067505
Iteration 12: train_loss 3.506495475769043
Iteration 13: train_loss 3.535346746444702
Iteration 14: train_loss 3.4454617500305176
Iteration 15: train_loss 3.368185520172119
Iteration 16: train_loss 3.474741220474243
Iteration 17: train_loss 3.4037702083587646
Iteration 18: train_loss 3.450788974761963
Iteration 19: train_loss 3.340625047683716
Iteration 20: train_loss 3.379730224609375
Iteration 21: train_loss 3.3889479637145996
Iteration 22: train_loss 3.4181129932403564
Iteration 23: train_loss 3.3963565826416016
Iteration 24: train_loss 3.4936704635620117
Iteration 25: train_loss 3.399139165878296
Iteration 26: train_loss 3.327051877975464
Iteration 27: train_loss 3.4758880138397217
Iteration 28: train_loss 3.3836863040924072
Iteration 29: train_loss 3.3775908946990967
Iteration 30: train_loss 3.486909866333008
Iteration 31: train_loss 3.4624431133270264
Iteration 32: train_loss 3.353869915008545
Iteration 33: train_loss 3.4411113262176514
Iteration 34: train_loss 3.4434056282043457
Iteration 35: train_loss 3.389967441558838
Iteration 36: train_loss 3.377185583114624
Iteration 37: train_loss 3.3162200450897217
Iteration 38: train_loss 3.2739925384521484
Iteration 39: train_loss 3.3967771530151367
Iteration 40: train_loss 3.6254031658172607
Epoch 11: train_avg_loss 3.429611337184906 eval_avg_acc: 0.13543524707309693 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:43:13] [32mIntermediate result: 0.13543524707309693  (Index 10)[0m
================Epoch: 12================
Iteration 1: train_loss 3.351102828979492
Iteration 2: train_loss 3.402627944946289
Iteration 3: train_loss 3.283648729324341
Iteration 4: train_loss 3.253821611404419
Iteration 5: train_loss 3.3785581588745117
Iteration 6: train_loss 3.36624813079834
Iteration 7: train_loss 3.3769357204437256
Iteration 8: train_loss 3.303025245666504
Iteration 9: train_loss 3.1945950984954834
Iteration 10: train_loss 3.2954113483428955
Iteration 11: train_loss 3.338287353515625
Iteration 12: train_loss 3.322923421859741
Iteration 13: train_loss 3.2573728561401367
Iteration 14: train_loss 3.3122243881225586
Iteration 15: train_loss 3.240171194076538
Iteration 16: train_loss 3.3147268295288086
Iteration 17: train_loss 3.3163838386535645
Iteration 18: train_loss 3.3940329551696777
Iteration 19: train_loss 3.373567581176758
Iteration 20: train_loss 3.384627103805542
Iteration 21: train_loss 3.358447313308716
Iteration 22: train_loss 3.2309088706970215
Iteration 23: train_loss 3.3784680366516113
Iteration 24: train_loss 3.2843916416168213
Iteration 25: train_loss 3.4403469562530518
Iteration 26: train_loss 3.291224718093872
Iteration 27: train_loss 3.433112859725952
Iteration 28: train_loss 3.373065948486328
Iteration 29: train_loss 3.317323923110962
Iteration 30: train_loss 3.2515487670898438
Iteration 31: train_loss 3.266594171524048
Iteration 32: train_loss 3.3730785846710205
Iteration 33: train_loss 3.317068576812744
Iteration 34: train_loss 3.3243703842163086
Iteration 35: train_loss 3.2656502723693848
Iteration 36: train_loss 3.2253575325012207
Iteration 37: train_loss 3.3433117866516113
Iteration 38: train_loss 3.3135688304901123
Iteration 39: train_loss 3.302926778793335
Iteration 40: train_loss 3.526608467102051
Epoch 12: train_avg_loss 3.326941668987274 eval_avg_acc: 0.17035628465103858 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:43:25] [32mIntermediate result: 0.17035628465103858  (Index 11)[0m
================Epoch: 13================
Iteration 1: train_loss 3.253544807434082
Iteration 2: train_loss 3.165499687194824
Iteration 3: train_loss 3.1358861923217773
Iteration 4: train_loss 3.3024303913116455
Iteration 5: train_loss 3.2084972858428955
Iteration 6: train_loss 3.2346200942993164
Iteration 7: train_loss 3.220679759979248
Iteration 8: train_loss 3.2303359508514404
Iteration 9: train_loss 3.20919132232666
Iteration 10: train_loss 3.1095802783966064
Iteration 11: train_loss 3.2657008171081543
Iteration 12: train_loss 3.3431711196899414
Iteration 13: train_loss 3.2724978923797607
Iteration 14: train_loss 3.2648513317108154
Iteration 15: train_loss 3.2678778171539307
Iteration 16: train_loss 3.124656915664673
Iteration 17: train_loss 3.235867500305176
Iteration 18: train_loss 3.196533203125
Iteration 19: train_loss 3.1965157985687256
Iteration 20: train_loss 3.2747371196746826
Iteration 21: train_loss 3.128361940383911
Iteration 22: train_loss 3.2547292709350586
Iteration 23: train_loss 3.190110921859741
Iteration 24: train_loss 3.167732000350952
Iteration 25: train_loss 3.231325149536133
Iteration 26: train_loss 3.2673611640930176
Iteration 27: train_loss 3.1285202503204346
Iteration 28: train_loss 3.194037914276123
Iteration 29: train_loss 3.258972406387329
Iteration 30: train_loss 3.196601390838623
Iteration 31: train_loss 3.185126781463623
Iteration 32: train_loss 3.2047407627105713
Iteration 33: train_loss 3.163484811782837
Iteration 34: train_loss 3.2300705909729004
Iteration 35: train_loss 3.283418893814087
Iteration 36: train_loss 3.160187244415283
Iteration 37: train_loss 3.1905510425567627
Iteration 38: train_loss 3.2173593044281006
Iteration 39: train_loss 3.2610538005828857
Iteration 40: train_loss 3.52351450920105
Epoch 13: train_avg_loss 3.2237483859062195 eval_avg_acc: 0.1678267913359196 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:43:37] [32mIntermediate result: 0.1678267913359196  (Index 12)[0m
================Epoch: 14================
Iteration 1: train_loss 3.235933303833008
Iteration 2: train_loss 3.1609513759613037
Iteration 3: train_loss 3.163249969482422
Iteration 4: train_loss 3.17755389213562
Iteration 5: train_loss 3.1364269256591797
Iteration 6: train_loss 3.1169471740722656
Iteration 7: train_loss 3.248781442642212
Iteration 8: train_loss 3.267066478729248
Iteration 9: train_loss 3.115516424179077
Iteration 10: train_loss 3.1408779621124268
Iteration 11: train_loss 3.088984251022339
Iteration 12: train_loss 3.0796356201171875
Iteration 13: train_loss 3.09144926071167
Iteration 14: train_loss 3.1690051555633545
Iteration 15: train_loss 3.201883316040039
Iteration 16: train_loss 3.1924941539764404
Iteration 17: train_loss 3.083413600921631
Iteration 18: train_loss 3.1886274814605713
Iteration 19: train_loss 3.0478482246398926
Iteration 20: train_loss 3.0579442977905273
Iteration 21: train_loss 3.1133368015289307
Iteration 22: train_loss 3.131836414337158
Iteration 23: train_loss 3.1377806663513184
Iteration 24: train_loss 3.1308343410491943
Iteration 25: train_loss 3.168976068496704
Iteration 26: train_loss 3.1770548820495605
Iteration 27: train_loss 3.037456750869751
Iteration 28: train_loss 3.1071670055389404
Iteration 29: train_loss 3.225869655609131
Iteration 30: train_loss 3.1830291748046875
Iteration 31: train_loss 3.094526529312134
Iteration 32: train_loss 3.0733509063720703
Iteration 33: train_loss 3.2090747356414795
Iteration 34: train_loss 3.2212154865264893
Iteration 35: train_loss 3.060152292251587
Iteration 36: train_loss 3.0618324279785156
Iteration 37: train_loss 3.149374008178711
Iteration 38: train_loss 3.1050286293029785
Iteration 39: train_loss 2.9510562419891357
Iteration 40: train_loss 3.0197155475616455
Epoch 14: train_avg_loss 3.1330814719200135 eval_avg_acc: 0.14703532597747004 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:43:49] [32mIntermediate result: 0.14703532597747004  (Index 13)[0m
================Epoch: 15================
Iteration 1: train_loss 3.117234468460083
Iteration 2: train_loss 3.0698189735412598
Iteration 3: train_loss 3.112226963043213
Iteration 4: train_loss 3.110766649246216
Iteration 5: train_loss 3.1436734199523926
Iteration 6: train_loss 2.9497132301330566
Iteration 7: train_loss 3.062782049179077
Iteration 8: train_loss 3.08099627494812
Iteration 9: train_loss 3.0090675354003906
Iteration 10: train_loss 3.035771608352661
Iteration 11: train_loss 2.9559061527252197
Iteration 12: train_loss 2.946586847305298
Iteration 13: train_loss 3.017944812774658
Iteration 14: train_loss 3.1266019344329834
Iteration 15: train_loss 3.023010730743408
Iteration 16: train_loss 3.0622806549072266
Iteration 17: train_loss 3.0066535472869873
Iteration 18: train_loss 3.079730987548828
Iteration 19: train_loss 3.106843948364258
Iteration 20: train_loss 2.9791014194488525
Iteration 21: train_loss 2.982491970062256
Iteration 22: train_loss 2.9834725856781006
Iteration 23: train_loss 3.091064929962158
Iteration 24: train_loss 3.03482985496521
Iteration 25: train_loss 3.1286137104034424
Iteration 26: train_loss 3.023270606994629
Iteration 27: train_loss 3.064579963684082
Iteration 28: train_loss 3.0878095626831055
Iteration 29: train_loss 3.135138988494873
Iteration 30: train_loss 3.111392021179199
Iteration 31: train_loss 3.070847511291504
Iteration 32: train_loss 3.042433500289917
Iteration 33: train_loss 3.0041871070861816
Iteration 34: train_loss 3.010537624359131
Iteration 35: train_loss 3.0366523265838623
Iteration 36: train_loss 3.0640852451324463
Iteration 37: train_loss 3.0292294025421143
Iteration 38: train_loss 3.1162326335906982
Iteration 39: train_loss 3.1741583347320557
Iteration 40: train_loss 3.192262887954712
Epoch 15: train_avg_loss 3.0595000743865968 eval_avg_acc: 0.1867886875046504 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:44:00] [32mIntermediate result: 0.1867886875046504  (Index 14)[0m
================Epoch: 16================
Iteration 1: train_loss 3.023479700088501
Iteration 2: train_loss 2.907764196395874
Iteration 3: train_loss 2.9660584926605225
Iteration 4: train_loss 2.931671380996704
Iteration 5: train_loss 2.986851692199707
Iteration 6: train_loss 3.0212531089782715
Iteration 7: train_loss 3.058494806289673
Iteration 8: train_loss 3.0138068199157715
Iteration 9: train_loss 3.0383503437042236
Iteration 10: train_loss 2.960766553878784
Iteration 11: train_loss 3.0564138889312744
Iteration 12: train_loss 3.0224454402923584
Iteration 13: train_loss 2.975989580154419
Iteration 14: train_loss 2.97304105758667
Iteration 15: train_loss 3.0256707668304443
Iteration 16: train_loss 3.0016987323760986
Iteration 17: train_loss 3.0104525089263916
Iteration 18: train_loss 2.9631683826446533
Iteration 19: train_loss 2.9271485805511475
Iteration 20: train_loss 3.016911268234253
Iteration 21: train_loss 3.012493848800659
Iteration 22: train_loss 2.9896600246429443
Iteration 23: train_loss 3.0841047763824463
Iteration 24: train_loss 2.998588800430298
Iteration 25: train_loss 3.003587484359741
Iteration 26: train_loss 3.0422303676605225
Iteration 27: train_loss 3.107880115509033
Iteration 28: train_loss 3.0850183963775635
Iteration 29: train_loss 2.9348151683807373
Iteration 30: train_loss 3.0060617923736572
Iteration 31: train_loss 2.9339168071746826
Iteration 32: train_loss 3.022459030151367
Iteration 33: train_loss 2.9426448345184326
Iteration 34: train_loss 2.940088987350464
Iteration 35: train_loss 3.00006103515625
Iteration 36: train_loss 2.9307687282562256
Iteration 37: train_loss 2.869558095932007
Iteration 38: train_loss 3.035271167755127
Iteration 39: train_loss 3.1090102195739746
Iteration 40: train_loss 3.0108938217163086
Epoch 16: train_avg_loss 2.9985137701034548 eval_avg_acc: 0.18076926413880046 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:44:12] [32mIntermediate result: 0.18076926413880046  (Index 15)[0m
================Epoch: 17================
Iteration 1: train_loss 2.904405117034912
Iteration 2: train_loss 2.9490163326263428
Iteration 3: train_loss 2.921349287033081
Iteration 4: train_loss 2.8526229858398438
Iteration 5: train_loss 2.9076743125915527
Iteration 6: train_loss 2.904083013534546
Iteration 7: train_loss 2.917847156524658
Iteration 8: train_loss 2.9853503704071045
Iteration 9: train_loss 2.90175199508667
Iteration 10: train_loss 2.9237096309661865
Iteration 11: train_loss 2.9910387992858887
Iteration 12: train_loss 2.933272361755371
Iteration 13: train_loss 2.955781936645508
Iteration 14: train_loss 2.9998416900634766
Iteration 15: train_loss 3.0472664833068848
Iteration 16: train_loss 2.963956356048584
Iteration 17: train_loss 2.9577085971832275
Iteration 18: train_loss 2.909010171890259
Iteration 19: train_loss 2.9224538803100586
Iteration 20: train_loss 2.968416929244995
Iteration 21: train_loss 2.9807817935943604
Iteration 22: train_loss 3.0094339847564697
Iteration 23: train_loss 2.9936654567718506
Iteration 24: train_loss 2.932361602783203
Iteration 25: train_loss 2.9223294258117676
Iteration 26: train_loss 2.985534906387329
Iteration 27: train_loss 2.8970632553100586
Iteration 28: train_loss 2.8767709732055664
Iteration 29: train_loss 2.903607130050659
Iteration 30: train_loss 2.8784196376800537
Iteration 31: train_loss 2.8500659465789795
Iteration 32: train_loss 2.9185802936553955
Iteration 33: train_loss 2.879195213317871
Iteration 34: train_loss 3.097846031188965
Iteration 35: train_loss 3.0648763179779053
Iteration 36: train_loss 2.939006805419922
Iteration 37: train_loss 2.872164487838745
Iteration 38: train_loss 2.894219160079956
Iteration 39: train_loss 2.9889252185821533
Iteration 40: train_loss 2.9491333961486816
Epoch 17: train_avg_loss 2.941263461112976 eval_avg_acc: 0.193596944939534 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:44:26] [32mIntermediate result: 0.193596944939534  (Index 16)[0m
================Epoch: 18================
Iteration 1: train_loss 2.9259440898895264
Iteration 2: train_loss 2.951958656311035
Iteration 3: train_loss 2.936995029449463
Iteration 4: train_loss 2.877462387084961
Iteration 5: train_loss 2.835062026977539
Iteration 6: train_loss 2.8694257736206055
Iteration 7: train_loss 2.89919376373291
Iteration 8: train_loss 2.798604726791382
Iteration 9: train_loss 2.8035523891448975
Iteration 10: train_loss 2.8538713455200195
Iteration 11: train_loss 2.85502552986145
Iteration 12: train_loss 2.9743213653564453
Iteration 13: train_loss 2.8415887355804443
Iteration 14: train_loss 2.8931667804718018
Iteration 15: train_loss 2.8289828300476074
Iteration 16: train_loss 2.928828239440918
Iteration 17: train_loss 2.8310418128967285
Iteration 18: train_loss 2.8409223556518555
Iteration 19: train_loss 2.911942958831787
Iteration 20: train_loss 2.8494842052459717
Iteration 21: train_loss 2.9323267936706543
Iteration 22: train_loss 2.8946962356567383
Iteration 23: train_loss 2.888767719268799
Iteration 24: train_loss 2.9344265460968018
Iteration 25: train_loss 2.8701517581939697
Iteration 26: train_loss 2.9131031036376953
Iteration 27: train_loss 2.882462739944458
Iteration 28: train_loss 2.806169271469116
Iteration 29: train_loss 2.9078404903411865
Iteration 30: train_loss 2.901216745376587
Iteration 31: train_loss 2.8680241107940674
Iteration 32: train_loss 2.838805913925171
Iteration 33: train_loss 2.833940029144287
Iteration 34: train_loss 2.9204578399658203
Iteration 35: train_loss 2.852229356765747
Iteration 36: train_loss 2.969766616821289
Iteration 37: train_loss 2.7931723594665527
Iteration 38: train_loss 2.8698971271514893
Iteration 39: train_loss 2.8268237113952637
Iteration 40: train_loss 2.557624578475952
Epoch 18: train_avg_loss 2.869231951236725 eval_avg_acc: 0.204717288974003 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:44:39] [32mIntermediate result: 0.204717288974003  (Index 17)[0m
================Epoch: 19================
Iteration 1: train_loss 2.8238048553466797
Iteration 2: train_loss 2.8541259765625
Iteration 3: train_loss 2.749270439147949
Iteration 4: train_loss 2.8213489055633545
Iteration 5: train_loss 2.844467878341675
Iteration 6: train_loss 2.86344575881958
Iteration 7: train_loss 2.8745532035827637
Iteration 8: train_loss 2.8327651023864746
Iteration 9: train_loss 2.761256456375122
Iteration 10: train_loss 2.8228397369384766
Iteration 11: train_loss 2.8020071983337402
Iteration 12: train_loss 2.750781536102295
Iteration 13: train_loss 2.835462808609009
Iteration 14: train_loss 2.866093873977661
Iteration 15: train_loss 2.745303153991699
Iteration 16: train_loss 2.8822624683380127
Iteration 17: train_loss 2.837212324142456
Iteration 18: train_loss 2.8147473335266113
Iteration 19: train_loss 2.8453004360198975
Iteration 20: train_loss 2.782705783843994
Iteration 21: train_loss 2.794701099395752
Iteration 22: train_loss 2.906475782394409
Iteration 23: train_loss 2.8203866481781006
Iteration 24: train_loss 2.791368007659912
Iteration 25: train_loss 2.9326441287994385
Iteration 26: train_loss 2.791980028152466
Iteration 27: train_loss 2.8152971267700195
Iteration 28: train_loss 2.8063461780548096
Iteration 29: train_loss 2.773254871368408
Iteration 30: train_loss 2.9188711643218994
Iteration 31: train_loss 2.909311056137085
Iteration 32: train_loss 2.926835060119629
Iteration 33: train_loss 2.9290683269500732
Iteration 34: train_loss 2.8952112197875977
Iteration 35: train_loss 2.757749080657959
Iteration 36: train_loss 2.8671064376831055
Iteration 37: train_loss 2.8298354148864746
Iteration 38: train_loss 2.772803783416748
Iteration 39: train_loss 2.8281049728393555
Iteration 40: train_loss 3.000640630722046
Epoch 19: train_avg_loss 2.836943656206131 eval_avg_acc: 0.19837695440405595 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:44:53] [32mIntermediate result: 0.19837695440405595  (Index 18)[0m
================Epoch: 20================
Iteration 1: train_loss 2.869598388671875
Iteration 2: train_loss 2.7403032779693604
Iteration 3: train_loss 2.7951793670654297
Iteration 4: train_loss 2.6810457706451416
Iteration 5: train_loss 2.749920129776001
Iteration 6: train_loss 2.801356077194214
Iteration 7: train_loss 2.940542459487915
Iteration 8: train_loss 2.828026056289673
Iteration 9: train_loss 2.7923858165740967
Iteration 10: train_loss 2.8789236545562744
Iteration 11: train_loss 2.731464147567749
Iteration 12: train_loss 2.7014381885528564
Iteration 13: train_loss 2.8222362995147705
Iteration 14: train_loss 2.879183530807495
Iteration 15: train_loss 2.7290163040161133
Iteration 16: train_loss 2.8381776809692383
Iteration 17: train_loss 2.789382219314575
Iteration 18: train_loss 2.7091519832611084
Iteration 19: train_loss 2.740194082260132
Iteration 20: train_loss 2.74052357673645
Iteration 21: train_loss 2.7424447536468506
Iteration 22: train_loss 2.8151707649230957
Iteration 23: train_loss 2.783076286315918
Iteration 24: train_loss 2.7109618186950684
Iteration 25: train_loss 2.73228120803833
Iteration 26: train_loss 2.7256436347961426
Iteration 27: train_loss 2.790647029876709
Iteration 28: train_loss 2.7786788940429688
Iteration 29: train_loss 2.8064768314361572
Iteration 30: train_loss 2.7928085327148438
Iteration 31: train_loss 2.8702545166015625
Iteration 32: train_loss 2.8155863285064697
Iteration 33: train_loss 2.800335168838501
Iteration 34: train_loss 2.8203492164611816
Iteration 35: train_loss 2.8009207248687744
Iteration 36: train_loss 2.8215672969818115
Iteration 37: train_loss 2.6716291904449463
Iteration 38: train_loss 2.738186836242676
Iteration 39: train_loss 2.717015027999878
Iteration 40: train_loss 2.6377625465393066
Epoch 20: train_avg_loss 2.7782461404800416 eval_avg_acc: 0.19486752701175564 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:45:06] [32mIntermediate result: 0.19486752701175564  (Index 19)[0m
================Epoch: 21================
Iteration 1: train_loss 2.828392744064331
Iteration 2: train_loss 2.806187868118286
Iteration 3: train_loss 2.7203357219696045
Iteration 4: train_loss 2.7770097255706787
Iteration 5: train_loss 2.690670967102051
Iteration 6: train_loss 2.7503414154052734
Iteration 7: train_loss 2.7592291831970215
Iteration 8: train_loss 2.672776222229004
Iteration 9: train_loss 2.717129945755005
Iteration 10: train_loss 2.7190961837768555
Iteration 11: train_loss 2.620741128921509
Iteration 12: train_loss 2.7455391883850098
Iteration 13: train_loss 2.7365317344665527
Iteration 14: train_loss 2.823387384414673
Iteration 15: train_loss 2.70896053314209
Iteration 16: train_loss 2.6672866344451904
Iteration 17: train_loss 2.710172414779663
Iteration 18: train_loss 2.7246482372283936
Iteration 19: train_loss 2.7413179874420166
Iteration 20: train_loss 2.6601178646087646
Iteration 21: train_loss 2.625746726989746
Iteration 22: train_loss 2.7216737270355225
Iteration 23: train_loss 2.8932716846466064
Iteration 24: train_loss 2.7218146324157715
Iteration 25: train_loss 2.782116174697876
Iteration 26: train_loss 2.659905195236206
Iteration 27: train_loss 2.694049835205078
Iteration 28: train_loss 2.7462682723999023
Iteration 29: train_loss 2.72158145904541
Iteration 30: train_loss 2.731271982192993
Iteration 31: train_loss 2.674278974533081
Iteration 32: train_loss 2.6241533756256104
Iteration 33: train_loss 2.729977607727051
Iteration 34: train_loss 2.6085081100463867
Iteration 35: train_loss 2.7893106937408447
Iteration 36: train_loss 2.8189780712127686
Iteration 37: train_loss 2.7797915935516357
Iteration 38: train_loss 2.725083351135254
Iteration 39: train_loss 2.7042248249053955
Iteration 40: train_loss 2.724763870239258
Epoch 21: train_avg_loss 2.7264160811901093 eval_avg_acc: 0.19925603555889165 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:45:20] [32mIntermediate result: 0.19925603555889165  (Index 20)[0m
================Epoch: 22================
Iteration 1: train_loss 2.644993305206299
Iteration 2: train_loss 2.7556469440460205
Iteration 3: train_loss 2.6393582820892334
Iteration 4: train_loss 2.6710824966430664
Iteration 5: train_loss 2.719057559967041
Iteration 6: train_loss 2.7046570777893066
Iteration 7: train_loss 2.640967607498169
Iteration 8: train_loss 2.617365598678589
Iteration 9: train_loss 2.778045177459717
Iteration 10: train_loss 2.638791084289551
Iteration 11: train_loss 2.706129550933838
Iteration 12: train_loss 2.6972813606262207
Iteration 13: train_loss 2.62978458404541
Iteration 14: train_loss 2.6311159133911133
Iteration 15: train_loss 2.673375368118286
Iteration 16: train_loss 2.71724271774292
Iteration 17: train_loss 2.6322808265686035
Iteration 18: train_loss 2.6675827503204346
Iteration 19: train_loss 2.650678873062134
Iteration 20: train_loss 2.6527600288391113
Iteration 21: train_loss 2.7291338443756104
Iteration 22: train_loss 2.6654212474823
Iteration 23: train_loss 2.681950569152832
Iteration 24: train_loss 2.639625310897827
Iteration 25: train_loss 2.7337045669555664
Iteration 26: train_loss 2.8570196628570557
Iteration 27: train_loss 2.6651828289031982
Iteration 28: train_loss 2.709251880645752
Iteration 29: train_loss 2.6893694400787354
Iteration 30: train_loss 2.7257907390594482
Iteration 31: train_loss 2.7517457008361816
Iteration 32: train_loss 2.682257652282715
Iteration 33: train_loss 2.6894235610961914
Iteration 34: train_loss 2.7430851459503174
Iteration 35: train_loss 2.6442461013793945
Iteration 36: train_loss 2.7363953590393066
Iteration 37: train_loss 2.593552827835083
Iteration 38: train_loss 2.690829277038574
Iteration 39: train_loss 2.6805896759033203
Iteration 40: train_loss 2.34454345703125
Epoch 22: train_avg_loss 2.678032898902893 eval_avg_acc: 0.2228274049286186 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:45:34] [32mIntermediate result: 0.2228274049286186  (Index 21)[0m
================Epoch: 23================
Iteration 1: train_loss 2.543100118637085
Iteration 2: train_loss 2.5479812622070312
Iteration 3: train_loss 2.697474241256714
Iteration 4: train_loss 2.6987948417663574
Iteration 5: train_loss 2.5865938663482666
Iteration 6: train_loss 2.7204060554504395
Iteration 7: train_loss 2.6021337509155273
Iteration 8: train_loss 2.6816375255584717
Iteration 9: train_loss 2.6773173809051514
Iteration 10: train_loss 2.661010980606079
Iteration 11: train_loss 2.6076650619506836
Iteration 12: train_loss 2.5779404640197754
Iteration 13: train_loss 2.611619710922241
Iteration 14: train_loss 2.5784640312194824
Iteration 15: train_loss 2.699772357940674
Iteration 16: train_loss 2.6164119243621826
Iteration 17: train_loss 2.562645435333252
Iteration 18: train_loss 2.623093843460083
Iteration 19: train_loss 2.605151414871216
Iteration 20: train_loss 2.5721020698547363
Iteration 21: train_loss 2.675072193145752
Iteration 22: train_loss 2.62119197845459
Iteration 23: train_loss 2.606614589691162
Iteration 24: train_loss 2.590644121170044
Iteration 25: train_loss 2.6588077545166016
Iteration 26: train_loss 2.6043574810028076
Iteration 27: train_loss 2.697927951812744
Iteration 28: train_loss 2.708052635192871
Iteration 29: train_loss 2.6212496757507324
Iteration 30: train_loss 2.6945960521698
