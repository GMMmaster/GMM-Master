{'batch_size': 256, 'epochs': 200, 'lr': 0.0005, 'parent_path': '../data_for_GMM-Master/', 'loc_dim': 64, 'layer': 4, 'beam_size': 5, 'wd': 0.0, 'dev_id': 1, 'use_gcn': 1, 'atten_flag': 1, 'tf_ratio': 0.5}
Loading Dataset Done!!!
get graph extra data finished!
Loading model Done!!!
================Epoch: 1================
Iteration 1: train_loss 19.45168685913086
Iteration 2: train_loss 19.92210578918457
Iteration 3: train_loss 18.061429977416992
Iteration 4: train_loss 16.76482582092285
Iteration 5: train_loss 12.062400817871094
Iteration 6: train_loss 12.509099006652832
Iteration 7: train_loss 12.364270210266113
Iteration 8: train_loss 10.839985847473145
Iteration 9: train_loss 10.510054588317871
Iteration 10: train_loss 12.543315887451172
Iteration 11: train_loss 10.058016777038574
Iteration 12: train_loss 10.351940155029297
Iteration 13: train_loss 10.309553146362305
Iteration 14: train_loss 9.720337867736816
Iteration 15: train_loss 9.261235237121582
Iteration 16: train_loss 9.21903133392334
Iteration 17: train_loss 9.48934555053711
Iteration 18: train_loss 9.22723388671875
Iteration 19: train_loss 8.91897964477539
Iteration 20: train_loss 8.640131950378418
Iteration 21: train_loss 8.525378227233887
Iteration 22: train_loss 8.467950820922852
Iteration 23: train_loss 8.58308219909668
Iteration 24: train_loss 8.274907112121582
Iteration 25: train_loss 8.40703010559082
Iteration 26: train_loss 8.30374526977539
Iteration 27: train_loss 8.243558883666992
Iteration 28: train_loss 8.363076210021973
Iteration 29: train_loss 7.991475582122803
Iteration 30: train_loss 7.951684474945068
Iteration 31: train_loss 7.969189643859863
Iteration 32: train_loss 8.031506538391113
Iteration 33: train_loss 7.99268913269043
Iteration 34: train_loss 8.278939247131348
Iteration 35: train_loss 7.822831630706787
Iteration 36: train_loss 7.926520347595215
Iteration 37: train_loss 8.005293846130371
Iteration 38: train_loss 7.7145771980285645
Iteration 39: train_loss 7.608383655548096
Iteration 40: train_loss 7.587104320526123
Epoch 1: train_avg_loss 10.056847620010377 eval_avg_acc: 0.006325281802388162 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:53:27] [32mIntermediate result: 0.006325281802388162  (Index 0)[0m
================Epoch: 2================
Iteration 1: train_loss 7.9918036460876465
Iteration 2: train_loss 7.659846782684326
Iteration 3: train_loss 7.613541603088379
Iteration 4: train_loss 7.903039932250977
Iteration 5: train_loss 7.707380294799805
Iteration 6: train_loss 7.65341854095459
Iteration 7: train_loss 7.584493637084961
Iteration 8: train_loss 7.397132396697998
Iteration 9: train_loss 7.63777494430542
Iteration 10: train_loss 7.43783712387085
Iteration 11: train_loss 7.539912223815918
Iteration 12: train_loss 7.427063941955566
Iteration 13: train_loss 7.370582580566406
Iteration 14: train_loss 7.455827713012695
Iteration 15: train_loss 7.483160018920898
Iteration 16: train_loss 7.205902099609375
Iteration 17: train_loss 7.394044399261475
Iteration 18: train_loss 7.485767841339111
Iteration 19: train_loss 7.3818745613098145
Iteration 20: train_loss 7.346460819244385
Iteration 21: train_loss 7.255238056182861
Iteration 22: train_loss 7.260358810424805
Iteration 23: train_loss 7.047906398773193
Iteration 24: train_loss 7.224128246307373
Iteration 25: train_loss 7.169761657714844
Iteration 26: train_loss 7.179770469665527
Iteration 27: train_loss 7.199431419372559
Iteration 28: train_loss 7.193174362182617
Iteration 29: train_loss 7.041993141174316
Iteration 30: train_loss 7.0071940422058105
Iteration 31: train_loss 7.0889058113098145
Iteration 32: train_loss 7.006391525268555
Iteration 33: train_loss 6.973355293273926
Iteration 34: train_loss 6.96664571762085
Iteration 35: train_loss 6.875624179840088
Iteration 36: train_loss 7.010001182556152
Iteration 37: train_loss 7.03763484954834
Iteration 38: train_loss 7.053894996643066
Iteration 39: train_loss 6.874235153198242
Iteration 40: train_loss 7.263284206390381
Epoch 2: train_avg_loss 7.310144865512848 eval_avg_acc: 0.012183474531541134 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:53:37] [32mIntermediate result: 0.012183474531541134  (Index 1)[0m
================Epoch: 3================
Iteration 1: train_loss 6.99920129776001
Iteration 2: train_loss 6.8710503578186035
Iteration 3: train_loss 7.03657341003418
Iteration 4: train_loss 6.862943649291992
Iteration 5: train_loss 6.923565864562988
Iteration 6: train_loss 6.969162940979004
Iteration 7: train_loss 6.838693618774414
Iteration 8: train_loss 6.913622856140137
Iteration 9: train_loss 6.937384128570557
Iteration 10: train_loss 6.654823303222656
Iteration 11: train_loss 6.822785377502441
Iteration 12: train_loss 6.912664413452148
Iteration 13: train_loss 6.788260459899902
Iteration 14: train_loss 6.682905197143555
Iteration 15: train_loss 6.704298973083496
Iteration 16: train_loss 6.650948524475098
Iteration 17: train_loss 6.748990058898926
Iteration 18: train_loss 6.781982898712158
Iteration 19: train_loss 6.648782253265381
Iteration 20: train_loss 6.663415431976318
Iteration 21: train_loss 6.609713077545166
Iteration 22: train_loss 6.646050453186035
Iteration 23: train_loss 6.717589855194092
Iteration 24: train_loss 6.621801853179932
Iteration 25: train_loss 6.552576065063477
Iteration 26: train_loss 6.740008354187012
Iteration 27: train_loss 6.693473815917969
Iteration 28: train_loss 6.3811774253845215
Iteration 29: train_loss 6.641653537750244
Iteration 30: train_loss 6.653454780578613
Iteration 31: train_loss 6.5259690284729
Iteration 32: train_loss 6.414810657501221
Iteration 33: train_loss 6.544912338256836
Iteration 34: train_loss 6.57596492767334
Iteration 35: train_loss 6.4577155113220215
Iteration 36: train_loss 6.611125469207764
Iteration 37: train_loss 6.524829387664795
Iteration 38: train_loss 6.513788223266602
Iteration 39: train_loss 6.480681896209717
Iteration 40: train_loss 6.209900856018066
Epoch 3: train_avg_loss 6.6882313132286075 eval_avg_acc: 0.015715217559911933 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:53:46] [32mIntermediate result: 0.015715217559911933  (Index 2)[0m
================Epoch: 4================
Iteration 1: train_loss 6.513415336608887
Iteration 2: train_loss 6.422905445098877
Iteration 3: train_loss 6.559172630310059
Iteration 4: train_loss 6.577165126800537
Iteration 5: train_loss 6.597757339477539
Iteration 6: train_loss 6.660056114196777
Iteration 7: train_loss 6.701101779937744
Iteration 8: train_loss 6.680589199066162
Iteration 9: train_loss 6.631084442138672
Iteration 10: train_loss 6.554972171783447
Iteration 11: train_loss 6.7049665451049805
Iteration 12: train_loss 6.682939529418945
Iteration 13: train_loss 6.860095024108887
Iteration 14: train_loss 6.715399742126465
Iteration 15: train_loss 6.362016201019287
Iteration 16: train_loss 6.615289211273193
Iteration 17: train_loss 6.441164970397949
Iteration 18: train_loss 6.52938175201416
Iteration 19: train_loss 6.322478294372559
Iteration 20: train_loss 6.332232475280762
Iteration 21: train_loss 6.509171485900879
Iteration 22: train_loss 6.400487422943115
Iteration 23: train_loss 6.37520694732666
Iteration 24: train_loss 6.355453014373779
Iteration 25: train_loss 6.520139217376709
Iteration 26: train_loss 6.396712303161621
Iteration 27: train_loss 6.672528266906738
Iteration 28: train_loss 6.454465389251709
Iteration 29: train_loss 6.349579334259033
Iteration 30: train_loss 6.192943096160889
Iteration 31: train_loss 6.265371322631836
Iteration 32: train_loss 6.026846885681152
Iteration 33: train_loss 6.210788726806641
Iteration 34: train_loss 6.193807125091553
Iteration 35: train_loss 6.308549880981445
Iteration 36: train_loss 6.131839752197266
Iteration 37: train_loss 6.432747840881348
Iteration 38: train_loss 6.11018180847168
Iteration 39: train_loss 6.281444072723389
Iteration 40: train_loss 6.5814104080200195
Epoch 4: train_avg_loss 6.455846440792084 eval_avg_acc: 0.018611521914697088 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:53:56] [32mIntermediate result: 0.018611521914697088  (Index 3)[0m
================Epoch: 5================
Iteration 1: train_loss 6.448379993438721
Iteration 2: train_loss 6.271000862121582
Iteration 3: train_loss 6.138678073883057
Iteration 4: train_loss 6.184499740600586
Iteration 5: train_loss 6.473297595977783
Iteration 6: train_loss 6.3130645751953125
Iteration 7: train_loss 6.304140567779541
Iteration 8: train_loss 6.201505661010742
Iteration 9: train_loss 6.1171369552612305
Iteration 10: train_loss 6.373199939727783
Iteration 11: train_loss 6.093489170074463
Iteration 12: train_loss 6.177470684051514
Iteration 13: train_loss 6.2885026931762695
Iteration 14: train_loss 6.330041408538818
Iteration 15: train_loss 6.219686031341553
Iteration 16: train_loss 6.084566116333008
Iteration 17: train_loss 6.207024097442627
Iteration 18: train_loss 6.191956520080566
Iteration 19: train_loss 6.171566486358643
Iteration 20: train_loss 6.272294998168945
Iteration 21: train_loss 6.233737945556641
Iteration 22: train_loss 5.978471755981445
Iteration 23: train_loss 6.047972679138184
Iteration 24: train_loss 6.257857322692871
Iteration 25: train_loss 6.2849273681640625
Iteration 26: train_loss 6.097681045532227
Iteration 27: train_loss 6.149869441986084
Iteration 28: train_loss 5.921560287475586
Iteration 29: train_loss 6.197722911834717
Iteration 30: train_loss 6.167802333831787
Iteration 31: train_loss 6.142928600311279
Iteration 32: train_loss 6.245872974395752
Iteration 33: train_loss 6.299984455108643
Iteration 34: train_loss 6.14459753036499
Iteration 35: train_loss 6.133411407470703
Iteration 36: train_loss 6.307853698730469
Iteration 37: train_loss 6.273401260375977
Iteration 38: train_loss 6.281012058258057
Iteration 39: train_loss 6.280779838562012
Iteration 40: train_loss 6.663060665130615
Epoch 5: train_avg_loss 6.224300193786621 eval_avg_acc: 0.0168797121896017 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:54:05] [32mIntermediate result: 0.0168797121896017  (Index 4)[0m
================Epoch: 6================
Iteration 1: train_loss 6.2713398933410645
Iteration 2: train_loss 6.365805149078369
Iteration 3: train_loss 6.264830112457275
Iteration 4: train_loss 6.384787082672119
Iteration 5: train_loss 6.330802917480469
Iteration 6: train_loss 6.157226085662842
Iteration 7: train_loss 6.3765788078308105
Iteration 8: train_loss 6.273151874542236
Iteration 9: train_loss 6.171367645263672
Iteration 10: train_loss 6.238451957702637
Iteration 11: train_loss 6.212480068206787
Iteration 12: train_loss 6.238018035888672
Iteration 13: train_loss 6.387429237365723
Iteration 14: train_loss 6.0479583740234375
Iteration 15: train_loss 6.3389153480529785
Iteration 16: train_loss 6.018764972686768
Iteration 17: train_loss 6.051874160766602
Iteration 18: train_loss 6.242027282714844
Iteration 19: train_loss 5.960303783416748
Iteration 20: train_loss 6.0147480964660645
Iteration 21: train_loss 6.076944828033447
Iteration 22: train_loss 5.924830913543701
Iteration 23: train_loss 5.987695693969727
Iteration 24: train_loss 6.032064914703369
Iteration 25: train_loss 6.0103960037231445
Iteration 26: train_loss 5.934894561767578
Iteration 27: train_loss 5.905054569244385
Iteration 28: train_loss 6.064936637878418
Iteration 29: train_loss 5.876489639282227
Iteration 30: train_loss 6.02803897857666
Iteration 31: train_loss 6.14578914642334
Iteration 32: train_loss 5.873671531677246
Iteration 33: train_loss 5.852359771728516
Iteration 34: train_loss 5.913821697235107
Iteration 35: train_loss 6.0579423904418945
Iteration 36: train_loss 5.954340934753418
Iteration 37: train_loss 5.969910144805908
Iteration 38: train_loss 5.8122968673706055
Iteration 39: train_loss 5.898043155670166
Iteration 40: train_loss 5.979581356048584
Epoch 6: train_avg_loss 6.091149115562439 eval_avg_acc: 0.021947044956504368 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:54:15] [32mIntermediate result: 0.021947044956504368  (Index 5)[0m
================Epoch: 7================
Iteration 1: train_loss 6.291498184204102
Iteration 2: train_loss 6.022050857543945
Iteration 3: train_loss 6.056007385253906
Iteration 4: train_loss 5.9037299156188965
Iteration 5: train_loss 5.99831485748291
Iteration 6: train_loss 6.037571430206299
Iteration 7: train_loss 6.046123027801514
Iteration 8: train_loss 6.252959728240967
Iteration 9: train_loss 6.186896800994873
Iteration 10: train_loss 6.191155910491943
Iteration 11: train_loss 6.232218265533447
Iteration 12: train_loss 6.101169586181641
Iteration 13: train_loss 6.196917533874512
Iteration 14: train_loss 6.167102336883545
Iteration 15: train_loss 6.231808662414551
Iteration 16: train_loss 5.98760461807251
Iteration 17: train_loss 5.911161422729492
Iteration 18: train_loss 6.138301849365234
Iteration 19: train_loss 5.984547138214111
Iteration 20: train_loss 6.116009712219238
Iteration 21: train_loss 6.095735549926758
Iteration 22: train_loss 6.060751914978027
Iteration 23: train_loss 6.097829341888428
Iteration 24: train_loss 5.959699630737305
Iteration 25: train_loss 6.041031360626221
Iteration 26: train_loss 6.039420127868652
Iteration 27: train_loss 5.983170509338379
Iteration 28: train_loss 5.897364139556885
Iteration 29: train_loss 5.999824047088623
Iteration 30: train_loss 5.95170783996582
Iteration 31: train_loss 6.060667037963867
Iteration 32: train_loss 5.909668445587158
Iteration 33: train_loss 6.107390880584717
Iteration 34: train_loss 5.918397426605225
Iteration 35: train_loss 5.8033270835876465
Iteration 36: train_loss 5.69890022277832
Iteration 37: train_loss 5.8645148277282715
Iteration 38: train_loss 5.780982971191406
Iteration 39: train_loss 5.712655067443848
Iteration 40: train_loss 5.521862030029297
Epoch 7: train_avg_loss 6.013951241970062 eval_avg_acc: 0.029660541723138933 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:54:25] [32mIntermediate result: 0.029660541723138933  (Index 6)[0m
================Epoch: 8================
Iteration 1: train_loss 5.811999320983887
Iteration 2: train_loss 6.0652546882629395
Iteration 3: train_loss 5.77870512008667
Iteration 4: train_loss 5.817733287811279
Iteration 5: train_loss 5.778067588806152
Iteration 6: train_loss 5.851954460144043
Iteration 7: train_loss 5.978573322296143
Iteration 8: train_loss 6.001862525939941
Iteration 9: train_loss 5.704275131225586
Iteration 10: train_loss 5.830468654632568
Iteration 11: train_loss 5.83905029296875
Iteration 12: train_loss 5.911618232727051
Iteration 13: train_loss 5.7251386642456055
Iteration 14: train_loss 6.059977054595947
Iteration 15: train_loss 5.789782524108887
Iteration 16: train_loss 5.714694023132324
Iteration 17: train_loss 5.732909202575684
Iteration 18: train_loss 5.659895896911621
Iteration 19: train_loss 5.712788105010986
Iteration 20: train_loss 5.697786331176758
Iteration 21: train_loss 5.650672912597656
Iteration 22: train_loss 5.743584156036377
Iteration 23: train_loss 5.480747222900391
Iteration 24: train_loss 5.680874347686768
Iteration 25: train_loss 5.5832977294921875
Iteration 26: train_loss 5.765238285064697
Iteration 27: train_loss 5.595738887786865
Iteration 28: train_loss 5.670401096343994
Iteration 29: train_loss 5.824547290802002
Iteration 30: train_loss 5.706688404083252
Iteration 31: train_loss 5.677761077880859
Iteration 32: train_loss 5.627767562866211
Iteration 33: train_loss 5.808058738708496
Iteration 34: train_loss 5.697054862976074
Iteration 35: train_loss 5.791860103607178
Iteration 36: train_loss 5.661379814147949
Iteration 37: train_loss 5.6463494300842285
Iteration 38: train_loss 5.556064605712891
Iteration 39: train_loss 5.724905490875244
Iteration 40: train_loss 5.241415500640869
Epoch 8: train_avg_loss 5.7399235486984255 eval_avg_acc: 0.04283339888840551 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:54:34] [32mIntermediate result: 0.04283339888840551  (Index 7)[0m
================Epoch: 9================
Iteration 1: train_loss 5.637422561645508
Iteration 2: train_loss 5.519298076629639
Iteration 3: train_loss 5.586240291595459
Iteration 4: train_loss 5.7076873779296875
Iteration 5: train_loss 5.701206207275391
Iteration 6: train_loss 5.904305458068848
Iteration 7: train_loss 5.631958961486816
Iteration 8: train_loss 5.793656826019287
Iteration 9: train_loss 5.818327903747559
Iteration 10: train_loss 5.644485950469971
Iteration 11: train_loss 5.860342025756836
Iteration 12: train_loss 5.661748886108398
Iteration 13: train_loss 5.506760597229004
Iteration 14: train_loss 5.55462646484375
Iteration 15: train_loss 5.495689868927002
Iteration 16: train_loss 5.677244186401367
Iteration 17: train_loss 5.676975250244141
Iteration 18: train_loss 5.735435485839844
Iteration 19: train_loss 5.675273895263672
Iteration 20: train_loss 5.62933349609375
Iteration 21: train_loss 5.485764026641846
Iteration 22: train_loss 5.568230628967285
Iteration 23: train_loss 5.681459426879883
Iteration 24: train_loss 5.414667129516602
Iteration 25: train_loss 5.460592269897461
Iteration 26: train_loss 5.537247180938721
Iteration 27: train_loss 5.494744777679443
Iteration 28: train_loss 5.376796722412109
Iteration 29: train_loss 5.5995378494262695
Iteration 30: train_loss 5.5583720207214355
Iteration 31: train_loss 5.821285247802734
Iteration 32: train_loss 5.369088172912598
Iteration 33: train_loss 5.76475191116333
Iteration 34: train_loss 5.723020553588867
Iteration 35: train_loss 5.589996337890625
Iteration 36: train_loss 5.471385478973389
Iteration 37: train_loss 5.6002020835876465
Iteration 38: train_loss 5.406632900238037
Iteration 39: train_loss 5.402307033538818
Iteration 40: train_loss 5.865810871124268
Epoch 9: train_avg_loss 5.615247809886933 eval_avg_acc: 0.05272337456779621 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:54:44] [32mIntermediate result: 0.05272337456779621  (Index 8)[0m
================Epoch: 10================
Iteration 1: train_loss 5.290566921234131
Iteration 2: train_loss 5.459442615509033
Iteration 3: train_loss 5.348807334899902
Iteration 4: train_loss 5.331923961639404
Iteration 5: train_loss 5.364838123321533
Iteration 6: train_loss 5.524616241455078
Iteration 7: train_loss 5.530789852142334
Iteration 8: train_loss 5.2480082511901855
Iteration 9: train_loss 5.3652238845825195
Iteration 10: train_loss 5.404576778411865
Iteration 11: train_loss 5.531591415405273
Iteration 12: train_loss 5.5290303230285645
Iteration 13: train_loss 5.325567722320557
Iteration 14: train_loss 5.468484401702881
Iteration 15: train_loss 5.36567497253418
Iteration 16: train_loss 5.275057315826416
Iteration 17: train_loss 5.289166450500488
Iteration 18: train_loss 5.525647163391113
Iteration 19: train_loss 5.577923774719238
Iteration 20: train_loss 5.668274879455566
Iteration 21: train_loss 5.533156871795654
Iteration 22: train_loss 5.664953231811523
Iteration 23: train_loss 5.466151237487793
Iteration 24: train_loss 5.758677005767822
Iteration 25: train_loss 5.6255011558532715
Iteration 26: train_loss 5.600927829742432
Iteration 27: train_loss 5.413470268249512
Iteration 28: train_loss 5.419689655303955
Iteration 29: train_loss 5.470690727233887
Iteration 30: train_loss 5.570340633392334
Iteration 31: train_loss 5.449018478393555
Iteration 32: train_loss 5.502771377563477
Iteration 33: train_loss 5.4066901206970215
Iteration 34: train_loss 5.44004487991333
Iteration 35: train_loss 5.536224842071533
Iteration 36: train_loss 5.418997764587402
Iteration 37: train_loss 5.569582939147949
Iteration 38: train_loss 5.367454528808594
Iteration 39: train_loss 5.287098407745361
Iteration 40: train_loss 5.581119060516357
Epoch 10: train_avg_loss 5.4626943349838255 eval_avg_acc: 0.06497733348009908 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:54:54] [32mIntermediate result: 0.06497733348009908  (Index 9)[0m
================Epoch: 11================
Iteration 1: train_loss 5.574173450469971
Iteration 2: train_loss 5.238905906677246
Iteration 3: train_loss 5.407978057861328
Iteration 4: train_loss 5.404998302459717
Iteration 5: train_loss 4.9536967277526855
Iteration 6: train_loss 5.375677108764648
Iteration 7: train_loss 5.493885517120361
Iteration 8: train_loss 5.3538994789123535
Iteration 9: train_loss 5.330837249755859
Iteration 10: train_loss 5.332031726837158
Iteration 11: train_loss 5.543305397033691
Iteration 12: train_loss 5.260408878326416
Iteration 13: train_loss 5.196096420288086
Iteration 14: train_loss 5.223613262176514
Iteration 15: train_loss 5.308850288391113
Iteration 16: train_loss 5.309032440185547
Iteration 17: train_loss 5.503354549407959
Iteration 18: train_loss 5.207920551300049
Iteration 19: train_loss 5.3091230392456055
Iteration 20: train_loss 5.18789005279541
Iteration 21: train_loss 5.277775764465332
Iteration 22: train_loss 5.295622825622559
Iteration 23: train_loss 5.104652404785156
Iteration 24: train_loss 5.192870616912842
Iteration 25: train_loss 5.130772113800049
Iteration 26: train_loss 5.4249420166015625
Iteration 27: train_loss 5.129276752471924
Iteration 28: train_loss 5.460268974304199
Iteration 29: train_loss 5.5068583488464355
Iteration 30: train_loss 5.329830169677734
Iteration 31: train_loss 5.412045478820801
Iteration 32: train_loss 5.312682151794434
Iteration 33: train_loss 5.0139360427856445
Iteration 34: train_loss 5.220781326293945
Iteration 35: train_loss 5.21610689163208
Iteration 36: train_loss 5.309495449066162
Iteration 37: train_loss 5.135073184967041
Iteration 38: train_loss 5.314508438110352
Iteration 39: train_loss 5.256585597991943
Iteration 40: train_loss 5.376082420349121
Epoch 11: train_avg_loss 5.298396134376526 eval_avg_acc: 0.061661423500289046 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:55:04] [32mIntermediate result: 0.061661423500289046  (Index 10)[0m
================Epoch: 12================
Iteration 1: train_loss 5.22359037399292
Iteration 2: train_loss 5.367673397064209
Iteration 3: train_loss 5.354083061218262
Iteration 4: train_loss 5.054635524749756
Iteration 5: train_loss 5.194889068603516
Iteration 6: train_loss 5.325660705566406
Iteration 7: train_loss 5.15410852432251
Iteration 8: train_loss 5.128684043884277
Iteration 9: train_loss 5.223207950592041
Iteration 10: train_loss 5.280682563781738
Iteration 11: train_loss 5.261658668518066
Iteration 12: train_loss 5.384988307952881
Iteration 13: train_loss 5.15910530090332
Iteration 14: train_loss 5.185450077056885
Iteration 15: train_loss 5.074839115142822
Iteration 16: train_loss 5.059260368347168
Iteration 17: train_loss 5.2300214767456055
Iteration 18: train_loss 5.124751567840576
Iteration 19: train_loss 5.174117565155029
Iteration 20: train_loss 5.077154159545898
Iteration 21: train_loss 5.162567138671875
Iteration 22: train_loss 4.948603630065918
Iteration 23: train_loss 5.116781234741211
Iteration 24: train_loss 5.123899936676025
Iteration 25: train_loss 5.113497257232666
Iteration 26: train_loss 4.921546936035156
Iteration 27: train_loss 5.138519287109375
Iteration 28: train_loss 5.201128005981445
Iteration 29: train_loss 5.348645210266113
Iteration 30: train_loss 5.2977752685546875
Iteration 31: train_loss 5.086347579956055
Iteration 32: train_loss 5.139755725860596
Iteration 33: train_loss 5.3416361808776855
Iteration 34: train_loss 5.248204708099365
Iteration 35: train_loss 5.181628704071045
Iteration 36: train_loss 5.282039642333984
Iteration 37: train_loss 5.216282367706299
Iteration 38: train_loss 5.170522212982178
Iteration 39: train_loss 5.339425086975098
Iteration 40: train_loss 5.103275775909424
Epoch 12: train_avg_loss 5.188016092777252 eval_avg_acc: 0.0797197654492271 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:55:13] [32mIntermediate result: 0.0797197654492271  (Index 11)[0m
================Epoch: 13================
Iteration 1: train_loss 5.497670650482178
Iteration 2: train_loss 5.249965667724609
Iteration 3: train_loss 5.029717922210693
Iteration 4: train_loss 5.197389602661133
Iteration 5: train_loss 5.0554046630859375
Iteration 6: train_loss 5.073353290557861
Iteration 7: train_loss 5.125916957855225
Iteration 8: train_loss 5.0876240730285645
Iteration 9: train_loss 5.099620342254639
Iteration 10: train_loss 4.889467716217041
Iteration 11: train_loss 4.942437171936035
Iteration 12: train_loss 4.875933647155762
Iteration 13: train_loss 5.000290870666504
Iteration 14: train_loss 4.934937477111816
Iteration 15: train_loss 5.047348499298096
Iteration 16: train_loss 5.152566432952881
Iteration 17: train_loss 5.089079856872559
Iteration 18: train_loss 4.902167797088623
Iteration 19: train_loss 4.900704383850098
Iteration 20: train_loss 4.995120525360107
Iteration 21: train_loss 4.974053859710693
Iteration 22: train_loss 5.160287380218506
Iteration 23: train_loss 5.0443806648254395
Iteration 24: train_loss 4.890067100524902
Iteration 25: train_loss 4.806018829345703
Iteration 26: train_loss 4.86585807800293
Iteration 27: train_loss 4.930332660675049
Iteration 28: train_loss 5.058140754699707
Iteration 29: train_loss 5.067190647125244
Iteration 30: train_loss 5.066600322723389
Iteration 31: train_loss 5.0531907081604
Iteration 32: train_loss 5.079373359680176
Iteration 33: train_loss 4.917035102844238
Iteration 34: train_loss 4.914618015289307
Iteration 35: train_loss 4.952409267425537
Iteration 36: train_loss 5.060080528259277
Iteration 37: train_loss 5.004159927368164
Iteration 38: train_loss 4.937582015991211
Iteration 39: train_loss 5.00394344329834
Iteration 40: train_loss 5.263450622558594
Epoch 13: train_avg_loss 5.029887270927429 eval_avg_acc: 0.08488834429626932 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:55:23] [32mIntermediate result: 0.08488834429626932  (Index 12)[0m
================Epoch: 14================
Iteration 1: train_loss 5.087299823760986
Iteration 2: train_loss 4.848801612854004
Iteration 3: train_loss 5.034383773803711
Iteration 4: train_loss 5.019187927246094
Iteration 5: train_loss 4.850563049316406
Iteration 6: train_loss 5.130903720855713
Iteration 7: train_loss 4.894938945770264
Iteration 8: train_loss 5.163150310516357
Iteration 9: train_loss 4.934662342071533
Iteration 10: train_loss 5.001982688903809
Iteration 11: train_loss 5.113503456115723
Iteration 12: train_loss 4.820225238800049
Iteration 13: train_loss 4.865289211273193
Iteration 14: train_loss 5.003987789154053
Iteration 15: train_loss 4.852698802947998
Iteration 16: train_loss 4.9599456787109375
Iteration 17: train_loss 4.83644437789917
Iteration 18: train_loss 4.915990829467773
Iteration 19: train_loss 4.781160831451416
Iteration 20: train_loss 4.825192451477051
Iteration 21: train_loss 4.777681827545166
Iteration 22: train_loss 4.884164810180664
Iteration 23: train_loss 4.697965145111084
Iteration 24: train_loss 4.853710174560547
Iteration 25: train_loss 4.762310028076172
Iteration 26: train_loss 4.982002258300781
Iteration 27: train_loss 4.848846912384033
Iteration 28: train_loss 5.062968730926514
Iteration 29: train_loss 4.8613362312316895
Iteration 30: train_loss 4.793295860290527
Iteration 31: train_loss 4.79299783706665
Iteration 32: train_loss 4.919328689575195
Iteration 33: train_loss 4.792813777923584
Iteration 34: train_loss 4.799969673156738
Iteration 35: train_loss 4.760013103485107
Iteration 36: train_loss 4.80233907699585
Iteration 37: train_loss 4.985568046569824
Iteration 38: train_loss 4.662773132324219
Iteration 39: train_loss 4.853493690490723
Iteration 40: train_loss 4.599570274353027
Epoch 14: train_avg_loss 4.885836553573609 eval_avg_acc: 0.10893770034388069 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:55:33] [32mIntermediate result: 0.10893770034388069  (Index 13)[0m
================Epoch: 15================
Iteration 1: train_loss 4.995609283447266
Iteration 2: train_loss 5.002169609069824
Iteration 3: train_loss 4.583136558532715
Iteration 4: train_loss 4.7598066329956055
Iteration 5: train_loss 4.663739204406738
Iteration 6: train_loss 4.629260540008545
Iteration 7: train_loss 4.7834038734436035
Iteration 8: train_loss 4.864013671875
Iteration 9: train_loss 4.858303546905518
Iteration 10: train_loss 4.6831560134887695
Iteration 11: train_loss 4.736931324005127
Iteration 12: train_loss 4.882602691650391
Iteration 13: train_loss 4.703813076019287
Iteration 14: train_loss 4.764616012573242
Iteration 15: train_loss 4.767995357513428
Iteration 16: train_loss 4.699193954467773
Iteration 17: train_loss 4.834651947021484
Iteration 18: train_loss 4.62849760055542
Iteration 19: train_loss 4.681465148925781
Iteration 20: train_loss 4.801730155944824
Iteration 21: train_loss 4.602689743041992
Iteration 22: train_loss 4.693029880523682
Iteration 23: train_loss 4.844583511352539
Iteration 24: train_loss 4.940464973449707
Iteration 25: train_loss 4.669862747192383
Iteration 26: train_loss 4.772697925567627
Iteration 27: train_loss 4.772884368896484
Iteration 28: train_loss 4.729588985443115
Iteration 29: train_loss 4.867282390594482
Iteration 30: train_loss 4.688599109649658
Iteration 31: train_loss 4.844915866851807
Iteration 32: train_loss 4.843875885009766
Iteration 33: train_loss 4.774328708648682
Iteration 34: train_loss 4.987067222595215
Iteration 35: train_loss 4.730223655700684
Iteration 36: train_loss 4.697295665740967
Iteration 37: train_loss 4.835936069488525
Iteration 38: train_loss 4.7868218421936035
Iteration 39: train_loss 4.723780632019043
Iteration 40: train_loss 4.890625476837158
Epoch 15: train_avg_loss 4.7755162715911865 eval_avg_acc: 0.11004804170710034 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:55:42] [32mIntermediate result: 0.11004804170710034  (Index 14)[0m
================Epoch: 16================
Iteration 1: train_loss 4.698312759399414
Iteration 2: train_loss 4.747669696807861
Iteration 3: train_loss 4.513786315917969
Iteration 4: train_loss 4.692395210266113
Iteration 5: train_loss 5.023298263549805
Iteration 6: train_loss 4.60311222076416
Iteration 7: train_loss 4.849440097808838
Iteration 8: train_loss 4.591943740844727
Iteration 9: train_loss 4.749501705169678
Iteration 10: train_loss 4.776629447937012
Iteration 11: train_loss 4.505844593048096
Iteration 12: train_loss 4.824038505554199
Iteration 13: train_loss 4.526056289672852
Iteration 14: train_loss 4.4929304122924805
Iteration 15: train_loss 4.679470539093018
Iteration 16: train_loss 4.682470321655273
Iteration 17: train_loss 4.697505950927734
Iteration 18: train_loss 4.614434242248535
Iteration 19: train_loss 4.849331378936768
Iteration 20: train_loss 4.747120380401611
Iteration 21: train_loss 4.655032634735107
Iteration 22: train_loss 4.410706043243408
Iteration 23: train_loss 4.64644718170166
Iteration 24: train_loss 4.792410373687744
Iteration 25: train_loss 4.637448787689209
Iteration 26: train_loss 4.639510631561279
Iteration 27: train_loss 4.703451156616211
Iteration 28: train_loss 4.794933319091797
Iteration 29: train_loss 4.7876691818237305
Iteration 30: train_loss 4.775096416473389
Iteration 31: train_loss 4.651264667510986
Iteration 32: train_loss 4.83334493637085
Iteration 33: train_loss 4.591777324676514
Iteration 34: train_loss 5.020273685455322
Iteration 35: train_loss 4.607607364654541
Iteration 36: train_loss 4.698915004730225
Iteration 37: train_loss 4.622053623199463
Iteration 38: train_loss 4.77317476272583
Iteration 39: train_loss 4.688867568969727
Iteration 40: train_loss 5.05665922164917
Epoch 16: train_avg_loss 4.706298398971557 eval_avg_acc: 0.10151842107344995 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:55:52] [32mIntermediate result: 0.10151842107344995  (Index 15)[0m
================Epoch: 17================
Iteration 1: train_loss 4.469494342803955
Iteration 2: train_loss 5.004003047943115
Iteration 3: train_loss 4.709606170654297
Iteration 4: train_loss 4.7550368309021
Iteration 5: train_loss 4.740223407745361
Iteration 6: train_loss 4.751322269439697
Iteration 7: train_loss 4.668034076690674
Iteration 8: train_loss 4.703086853027344
Iteration 9: train_loss 4.710068702697754
Iteration 10: train_loss 4.551242351531982
Iteration 11: train_loss 4.604270935058594
Iteration 12: train_loss 4.563218116760254
Iteration 13: train_loss 4.737305164337158
Iteration 14: train_loss 4.569644927978516
Iteration 15: train_loss 4.807281017303467
Iteration 16: train_loss 4.670892238616943
Iteration 17: train_loss 4.685033321380615
Iteration 18: train_loss 4.480326175689697
Iteration 19: train_loss 4.696005344390869
Iteration 20: train_loss 4.714878559112549
Iteration 21: train_loss 4.885570049285889
Iteration 22: train_loss 4.673856258392334
Iteration 23: train_loss 4.593929290771484
Iteration 24: train_loss 4.441092491149902
Iteration 25: train_loss 4.520956039428711
Iteration 26: train_loss 4.566266059875488
Iteration 27: train_loss 4.461022853851318
Iteration 28: train_loss 4.463569164276123
Iteration 29: train_loss 4.628803730010986
Iteration 30: train_loss 4.6094560623168945
Iteration 31: train_loss 4.527348041534424
Iteration 32: train_loss 4.6943440437316895
Iteration 33: train_loss 4.681176662445068
Iteration 34: train_loss 4.540745735168457
Iteration 35: train_loss 4.505736351013184
Iteration 36: train_loss 4.789189338684082
Iteration 37: train_loss 4.8177032470703125
Iteration 38: train_loss 4.667529582977295
Iteration 39: train_loss 4.739407539367676
Iteration 40: train_loss 4.575648307800293
Epoch 17: train_avg_loss 4.649358117580414 eval_avg_acc: 0.11279624692072496 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 10:56:02] [32mIntermediate result: 0.11279624692072496  (Index 16)[0m
================Epoch: 18================
Iteration 1: train_loss 4.75771951675415
Iteration 2: train_loss 4.856527328491211
Iteration 3: train_loss 4.664505481719971
Iteration 4: train_loss 4.698852062225342
Iteration 5: train_loss 4.604892253875732
Iteration 6: train_loss 4.618657112121582
Iteration 7: train_loss 4.622503280639648
Iteration 8: train_loss 4.466125965118408
Iteration 9: train_loss 4.473298072814941
Iteration 10: train_loss 4.596446514129639
Iteration 11: train_loss 4.461976051330566
Iteration 12: train_loss 4.590346336364746
Iteration 13: train_loss 4.739752769470215
Iteration 14: train_loss 4.387876033782959
Iteration 15: train_loss 4.6772379875183105
Iteration 16: train_loss 4.666159152984619
Iteration 17: train_loss 4.618803977966309
Iteration 18: train_loss 4.6497015953063965
Iteration 19: train_loss 4.516287326812744
Iteration 20: train_loss 4.41505241394043
Iteration 21: train_loss 4.545055866241455
Iteration 22: train_loss 4.528874397277832
Iteration 23: train_loss 4.420140266418457
Iteration 24: train_loss 4.511310577392578
Iteration 25: train_loss 4.692897796630859
Iteration 26: train_loss 4.669855117797852
Iteration 27: train_loss 4.540466785430908
Iteration 28: train_loss 4.444438934326172
Iteration 29: train_loss 4.554763317108154
Iteration 30: train_loss 4.425198078155518
Iteration 31: train_loss 4.635547637939453
Iteration 32: train_loss 4.535123348236084
Iteration 33: train_loss 4.58133602142334
