{'batch_size': 256, 'epochs': 200, 'lr': 0.001, 'parent_path': '../data_for_GMM-Master/', 'loc_dim': 32, 'layer': 4, 'beam_size': 5, 'wd': 0.0, 'dev_id': 1, 'use_gcn': 1, 'atten_flag': 1, 'tf_ratio': 0.0}
Loading Dataset Done!!!
get graph extra data finished!
Loading model Done!!!
================Epoch: 1================
Iteration 1: train_loss 22.9570255279541
Iteration 2: train_loss 13.473206520080566
Iteration 3: train_loss 11.045230865478516
Iteration 4: train_loss 12.741716384887695
Iteration 5: train_loss 9.247998237609863
Iteration 6: train_loss 9.190539360046387
Iteration 7: train_loss 9.05581283569336
Iteration 8: train_loss 9.236029624938965
Iteration 9: train_loss 8.733545303344727
Iteration 10: train_loss 8.483016967773438
Iteration 11: train_loss 8.144325256347656
Iteration 12: train_loss 9.087931632995605
Iteration 13: train_loss 8.404922485351562
Iteration 14: train_loss 8.089924812316895
Iteration 15: train_loss 7.985415458679199
Iteration 16: train_loss 8.0929536819458
Iteration 17: train_loss 7.9281792640686035
Iteration 18: train_loss 7.725637912750244
Iteration 19: train_loss 7.617036819458008
Iteration 20: train_loss 7.73054838180542
Iteration 21: train_loss 7.71336030960083
Iteration 22: train_loss 7.543698310852051
Iteration 23: train_loss 7.626712799072266
Iteration 24: train_loss 7.494475841522217
Iteration 25: train_loss 7.496241092681885
Iteration 26: train_loss 7.5077223777771
Iteration 27: train_loss 7.3840789794921875
Iteration 28: train_loss 7.423013210296631
Iteration 29: train_loss 7.264803886413574
Iteration 30: train_loss 7.322986125946045
Iteration 31: train_loss 7.41237735748291
Iteration 32: train_loss 7.336094856262207
Iteration 33: train_loss 7.309301376342773
Iteration 34: train_loss 7.292884349822998
Iteration 35: train_loss 7.242486476898193
Iteration 36: train_loss 7.24964714050293
Iteration 37: train_loss 7.1215901374816895
Iteration 38: train_loss 7.076321601867676
Iteration 39: train_loss 7.084527492523193
Iteration 40: train_loss 7.152914047241211
Epoch 1: train_avg_loss 8.550655877590179 eval_avg_acc: 0.007621888679616366 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:06:13] [32mIntermediate result: 0.007621888679616366  (Index 0)[0m
================Epoch: 2================
Iteration 1: train_loss 7.131027698516846
Iteration 2: train_loss 6.981609344482422
Iteration 3: train_loss 7.115312099456787
Iteration 4: train_loss 7.111144542694092
Iteration 5: train_loss 7.029872894287109
Iteration 6: train_loss 7.014003753662109
Iteration 7: train_loss 6.948709011077881
Iteration 8: train_loss 6.95551872253418
Iteration 9: train_loss 6.930314064025879
Iteration 10: train_loss 6.967981338500977
Iteration 11: train_loss 7.019076824188232
Iteration 12: train_loss 6.968291759490967
Iteration 13: train_loss 6.976936340332031
Iteration 14: train_loss 7.0037665367126465
Iteration 15: train_loss 6.924439430236816
Iteration 16: train_loss 6.841556072235107
Iteration 17: train_loss 6.882996082305908
Iteration 18: train_loss 6.967977523803711
Iteration 19: train_loss 6.896917343139648
Iteration 20: train_loss 6.818796634674072
Iteration 21: train_loss 6.761361598968506
Iteration 22: train_loss 6.814136505126953
Iteration 23: train_loss 6.874327659606934
Iteration 24: train_loss 7.013906002044678
Iteration 25: train_loss 6.780517101287842
Iteration 26: train_loss 6.809364318847656
Iteration 27: train_loss 6.720989227294922
Iteration 28: train_loss 6.701948165893555
Iteration 29: train_loss 6.763217449188232
Iteration 30: train_loss 6.771831512451172
Iteration 31: train_loss 6.641815185546875
Iteration 32: train_loss 6.836277484893799
Iteration 33: train_loss 6.6687726974487305
Iteration 34: train_loss 6.6033782958984375
Iteration 35: train_loss 6.656304836273193
Iteration 36: train_loss 6.631141662597656
Iteration 37: train_loss 6.738412380218506
Iteration 38: train_loss 6.668978214263916
Iteration 39: train_loss 6.721870422363281
Iteration 40: train_loss 6.636786937713623
Epoch 2: train_avg_loss 6.857539641857147 eval_avg_acc: 0.012729513164769283 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:06:25] [32mIntermediate result: 0.012729513164769283  (Index 1)[0m
================Epoch: 3================
Iteration 1: train_loss 6.797256946563721
Iteration 2: train_loss 6.695079326629639
Iteration 3: train_loss 6.663913249969482
Iteration 4: train_loss 6.5629119873046875
Iteration 5: train_loss 6.608407974243164
Iteration 6: train_loss 6.650001525878906
Iteration 7: train_loss 6.552319526672363
Iteration 8: train_loss 6.564638137817383
Iteration 9: train_loss 6.557252407073975
Iteration 10: train_loss 6.595813751220703
Iteration 11: train_loss 6.564728736877441
Iteration 12: train_loss 6.456231117248535
Iteration 13: train_loss 6.549674034118652
Iteration 14: train_loss 6.525853157043457
Iteration 15: train_loss 6.462676048278809
Iteration 16: train_loss 6.448492527008057
Iteration 17: train_loss 6.468724727630615
Iteration 18: train_loss 6.468728542327881
Iteration 19: train_loss 6.495227336883545
Iteration 20: train_loss 6.584073543548584
Iteration 21: train_loss 6.543200492858887
Iteration 22: train_loss 6.619353294372559
Iteration 23: train_loss 6.4593658447265625
Iteration 24: train_loss 6.597587585449219
Iteration 25: train_loss 6.454387664794922
Iteration 26: train_loss 6.467929840087891
Iteration 27: train_loss 6.448549270629883
Iteration 28: train_loss 6.4378581047058105
Iteration 29: train_loss 6.505087852478027
Iteration 30: train_loss 6.494956016540527
Iteration 31: train_loss 6.532029151916504
Iteration 32: train_loss 6.4884867668151855
Iteration 33: train_loss 6.524087905883789
Iteration 34: train_loss 6.575870990753174
Iteration 35: train_loss 6.650359630584717
Iteration 36: train_loss 6.705211162567139
Iteration 37: train_loss 6.697201251983643
Iteration 38: train_loss 6.626010894775391
Iteration 39: train_loss 6.611112117767334
Iteration 40: train_loss 6.442677021026611
Epoch 3: train_avg_loss 6.553833186626434 eval_avg_acc: 0.012475124117522288 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:06:36] [32mIntermediate result: 0.012475124117522288  (Index 2)[0m
================Epoch: 4================
Iteration 1: train_loss 6.725812911987305
Iteration 2: train_loss 6.704788684844971
Iteration 3: train_loss 6.682516574859619
Iteration 4: train_loss 6.562283992767334
Iteration 5: train_loss 6.566590309143066
Iteration 6: train_loss 6.494946479797363
Iteration 7: train_loss 6.501668930053711
Iteration 8: train_loss 6.471306324005127
Iteration 9: train_loss 6.488145351409912
Iteration 10: train_loss 6.520836353302002
Iteration 11: train_loss 6.528113842010498
Iteration 12: train_loss 6.392923831939697
Iteration 13: train_loss 6.425356388092041
Iteration 14: train_loss 6.434713840484619
Iteration 15: train_loss 6.476729869842529
Iteration 16: train_loss 6.506763458251953
Iteration 17: train_loss 6.423040390014648
Iteration 18: train_loss 6.305335998535156
Iteration 19: train_loss 6.295475959777832
Iteration 20: train_loss 6.272454738616943
Iteration 21: train_loss 6.311444282531738
Iteration 22: train_loss 6.131589889526367
Iteration 23: train_loss 6.279004096984863
Iteration 24: train_loss 6.253208637237549
Iteration 25: train_loss 6.210752964019775
Iteration 26: train_loss 6.14763069152832
Iteration 27: train_loss 6.359787940979004
Iteration 28: train_loss 6.317203521728516
Iteration 29: train_loss 6.170818328857422
Iteration 30: train_loss 6.205549716949463
Iteration 31: train_loss 6.181542873382568
Iteration 32: train_loss 6.140891075134277
Iteration 33: train_loss 6.105022430419922
Iteration 34: train_loss 6.13875150680542
Iteration 35: train_loss 6.175168037414551
Iteration 36: train_loss 6.287331581115723
Iteration 37: train_loss 6.28350305557251
Iteration 38: train_loss 6.1696672439575195
Iteration 39: train_loss 6.245846748352051
Iteration 40: train_loss 6.577965259552002
Epoch 4: train_avg_loss 6.361812102794647 eval_avg_acc: 0.011804950794238623 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:06:47] [32mIntermediate result: 0.011804950794238623  (Index 3)[0m
================Epoch: 5================
Iteration 1: train_loss 6.344549179077148
Iteration 2: train_loss 6.40873908996582
Iteration 3: train_loss 6.398548126220703
Iteration 4: train_loss 6.49198579788208
Iteration 5: train_loss 6.45708703994751
Iteration 6: train_loss 6.327589511871338
Iteration 7: train_loss 6.3345441818237305
Iteration 8: train_loss 6.2668304443359375
Iteration 9: train_loss 6.271684646606445
Iteration 10: train_loss 6.35830020904541
Iteration 11: train_loss 6.305572986602783
Iteration 12: train_loss 6.252116680145264
Iteration 13: train_loss 6.291166305541992
Iteration 14: train_loss 6.2589826583862305
Iteration 15: train_loss 6.2316765785217285
Iteration 16: train_loss 6.318850040435791
Iteration 17: train_loss 6.3075852394104
Iteration 18: train_loss 6.275918483734131
Iteration 19: train_loss 6.183347225189209
Iteration 20: train_loss 6.32791805267334
Iteration 21: train_loss 6.114439487457275
Iteration 22: train_loss 6.163578033447266
Iteration 23: train_loss 6.203370094299316
Iteration 24: train_loss 6.113099575042725
Iteration 25: train_loss 6.094789505004883
Iteration 26: train_loss 6.1287994384765625
Iteration 27: train_loss 6.125272274017334
Iteration 28: train_loss 6.011429786682129
Iteration 29: train_loss 6.018131256103516
Iteration 30: train_loss 6.100919246673584
Iteration 31: train_loss 6.030486106872559
Iteration 32: train_loss 6.020174503326416
Iteration 33: train_loss 6.041438579559326
Iteration 34: train_loss 5.932080268859863
Iteration 35: train_loss 5.9977827072143555
Iteration 36: train_loss 6.004157543182373
Iteration 37: train_loss 5.93778657913208
Iteration 38: train_loss 6.0199761390686035
Iteration 39: train_loss 6.032729625701904
Iteration 40: train_loss 5.816050052642822
Epoch 5: train_avg_loss 6.182987082004547 eval_avg_acc: 0.019953284989685766 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:06:59] [32mIntermediate result: 0.019953284989685766  (Index 4)[0m
================Epoch: 6================
Iteration 1: train_loss 6.1858649253845215
Iteration 2: train_loss 6.172285079956055
Iteration 3: train_loss 6.0846734046936035
Iteration 4: train_loss 6.025787830352783
Iteration 5: train_loss 6.116407871246338
Iteration 6: train_loss 6.105619430541992
Iteration 7: train_loss 6.149147033691406
Iteration 8: train_loss 6.173662185668945
Iteration 9: train_loss 6.193418979644775
Iteration 10: train_loss 6.051963806152344
Iteration 11: train_loss 6.095775127410889
Iteration 12: train_loss 6.113563060760498
Iteration 13: train_loss 6.065920829772949
Iteration 14: train_loss 6.1704535484313965
Iteration 15: train_loss 6.066412448883057
Iteration 16: train_loss 6.101129055023193
Iteration 17: train_loss 6.131308555603027
Iteration 18: train_loss 6.0688700675964355
Iteration 19: train_loss 6.251530170440674
Iteration 20: train_loss 6.158169269561768
Iteration 21: train_loss 6.2411017417907715
Iteration 22: train_loss 6.2344465255737305
Iteration 23: train_loss 6.195587635040283
Iteration 24: train_loss 6.117043972015381
Iteration 25: train_loss 6.111661911010742
Iteration 26: train_loss 6.137885570526123
Iteration 27: train_loss 6.064291000366211
Iteration 28: train_loss 6.1661553382873535
Iteration 29: train_loss 6.174901485443115
Iteration 30: train_loss 6.203522205352783
Iteration 31: train_loss 6.223255157470703
Iteration 32: train_loss 6.147173881530762
Iteration 33: train_loss 6.118607997894287
Iteration 34: train_loss 6.111546516418457
Iteration 35: train_loss 6.045408248901367
Iteration 36: train_loss 6.166864395141602
Iteration 37: train_loss 6.131175994873047
Iteration 38: train_loss 6.222168922424316
Iteration 39: train_loss 6.117654800415039
Iteration 40: train_loss 6.2483415603637695
Epoch 6: train_avg_loss 6.141518938541412 eval_avg_acc: 0.020106061171691777 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:07:12] [32mIntermediate result: 0.020106061171691777  (Index 5)[0m
================Epoch: 7================
Iteration 1: train_loss 6.234539985656738
Iteration 2: train_loss 6.13382625579834
Iteration 3: train_loss 6.078271389007568
Iteration 4: train_loss 6.133420944213867
Iteration 5: train_loss 6.0406880378723145
Iteration 6: train_loss 6.120825290679932
Iteration 7: train_loss 6.043041229248047
Iteration 8: train_loss 6.033102989196777
Iteration 9: train_loss 6.062568187713623
Iteration 10: train_loss 6.17166805267334
Iteration 11: train_loss 6.065605163574219
Iteration 12: train_loss 6.018453121185303
Iteration 13: train_loss 5.961482048034668
Iteration 14: train_loss 5.974283695220947
Iteration 15: train_loss 6.0798821449279785
Iteration 16: train_loss 6.05517053604126
Iteration 17: train_loss 6.078958034515381
Iteration 18: train_loss 6.090648174285889
Iteration 19: train_loss 6.072883129119873
Iteration 20: train_loss 6.000954627990723
Iteration 21: train_loss 6.106410980224609
Iteration 22: train_loss 6.131335258483887
Iteration 23: train_loss 6.005207538604736
Iteration 24: train_loss 6.013095855712891
Iteration 25: train_loss 5.9981842041015625
Iteration 26: train_loss 5.900688648223877
Iteration 27: train_loss 5.9817891120910645
Iteration 28: train_loss 5.896875858306885
Iteration 29: train_loss 5.89478874206543
Iteration 30: train_loss 5.895915508270264
Iteration 31: train_loss 5.935487747192383
Iteration 32: train_loss 5.852414608001709
Iteration 33: train_loss 5.97515869140625
Iteration 34: train_loss 5.966769695281982
Iteration 35: train_loss 5.901051044464111
Iteration 36: train_loss 6.047861099243164
Iteration 37: train_loss 6.110045433044434
Iteration 38: train_loss 6.129900932312012
Iteration 39: train_loss 6.085870742797852
Iteration 40: train_loss 6.130153656005859
Epoch 7: train_avg_loss 6.035231959819794 eval_avg_acc: 0.029961770893743866 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:07:25] [32mIntermediate result: 0.029961770893743866  (Index 6)[0m
================Epoch: 8================
Iteration 1: train_loss 5.991672515869141
Iteration 2: train_loss 6.139112949371338
Iteration 3: train_loss 6.100857257843018
Iteration 4: train_loss 6.132258415222168
Iteration 5: train_loss 5.971375465393066
Iteration 6: train_loss 5.9754767417907715
Iteration 7: train_loss 6.008787155151367
Iteration 8: train_loss 5.878818035125732
Iteration 9: train_loss 5.92457389831543
Iteration 10: train_loss 5.974567890167236
Iteration 11: train_loss 5.905500888824463
Iteration 12: train_loss 5.867180347442627
Iteration 13: train_loss 5.898027420043945
Iteration 14: train_loss 6.015778541564941
Iteration 15: train_loss 5.83152437210083
Iteration 16: train_loss 5.928436756134033
Iteration 17: train_loss 5.8827338218688965
Iteration 18: train_loss 5.881856441497803
Iteration 19: train_loss 5.937586784362793
Iteration 20: train_loss 5.864161968231201
Iteration 21: train_loss 5.880113124847412
Iteration 22: train_loss 5.85141658782959
Iteration 23: train_loss 5.82403039932251
Iteration 24: train_loss 5.854363918304443
Iteration 25: train_loss 5.829715251922607
Iteration 26: train_loss 5.852163791656494
Iteration 27: train_loss 5.952187538146973
Iteration 28: train_loss 5.808013439178467
Iteration 29: train_loss 5.922874927520752
Iteration 30: train_loss 5.899372577667236
Iteration 31: train_loss 5.796379566192627
Iteration 32: train_loss 5.772590160369873
Iteration 33: train_loss 5.874922752380371
Iteration 34: train_loss 5.8952956199646
Iteration 35: train_loss 5.687394142150879
Iteration 36: train_loss 5.7012457847595215
Iteration 37: train_loss 5.85361385345459
Iteration 38: train_loss 5.71105432510376
Iteration 39: train_loss 5.76598596572876
Iteration 40: train_loss 5.626468658447266
Epoch 8: train_avg_loss 5.886737251281739 eval_avg_acc: 0.037209342501920324 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:07:37] [32mIntermediate result: 0.037209342501920324  (Index 7)[0m
================Epoch: 9================
Iteration 1: train_loss 5.831044673919678
Iteration 2: train_loss 5.898067951202393
Iteration 3: train_loss 5.92547607421875
Iteration 4: train_loss 5.822780132293701
Iteration 5: train_loss 5.952515125274658
Iteration 6: train_loss 5.971080303192139
Iteration 7: train_loss 5.986729145050049
Iteration 8: train_loss 5.998972415924072
Iteration 9: train_loss 6.060815334320068
Iteration 10: train_loss 6.0956501960754395
Iteration 11: train_loss 6.111112117767334
Iteration 12: train_loss 6.087526798248291
Iteration 13: train_loss 6.2003583908081055
Iteration 14: train_loss 6.217215538024902
Iteration 15: train_loss 6.143316745758057
Iteration 16: train_loss 6.114735126495361
Iteration 17: train_loss 6.166527271270752
Iteration 18: train_loss 6.058391571044922
Iteration 19: train_loss 6.052746772766113
Iteration 20: train_loss 6.187304973602295
Iteration 21: train_loss 6.139118671417236
Iteration 22: train_loss 6.019883155822754
Iteration 23: train_loss 6.004713535308838
Iteration 24: train_loss 5.976011276245117
Iteration 25: train_loss 6.051695823669434
Iteration 26: train_loss 6.1548237800598145
Iteration 27: train_loss 6.043133735656738
Iteration 28: train_loss 6.005946159362793
Iteration 29: train_loss 6.016418933868408
Iteration 30: train_loss 6.01537561416626
Iteration 31: train_loss 5.9029693603515625
Iteration 32: train_loss 5.884392261505127
Iteration 33: train_loss 5.794804096221924
Iteration 34: train_loss 5.91964864730835
Iteration 35: train_loss 5.90963077545166
Iteration 36: train_loss 5.903802394866943
Iteration 37: train_loss 5.8723907470703125
Iteration 38: train_loss 5.755288600921631
Iteration 39: train_loss 5.796413421630859
Iteration 40: train_loss 5.901119709014893
Epoch 9: train_avg_loss 5.998748683929444 eval_avg_acc: 0.02032458840138827 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:07:50] [32mIntermediate result: 0.02032458840138827  (Index 8)[0m
================Epoch: 10================
Iteration 1: train_loss 5.946420192718506
Iteration 2: train_loss 5.833205223083496
Iteration 3: train_loss 5.819845199584961
Iteration 4: train_loss 5.909134864807129
Iteration 5: train_loss 5.822732448577881
Iteration 6: train_loss 5.789649486541748
Iteration 7: train_loss 5.766463279724121
Iteration 8: train_loss 5.778597354888916
Iteration 9: train_loss 5.795140266418457
Iteration 10: train_loss 5.816077709197998
Iteration 11: train_loss 5.852878570556641
Iteration 12: train_loss 5.831019878387451
Iteration 13: train_loss 5.671682357788086
Iteration 14: train_loss 5.863615989685059
Iteration 15: train_loss 5.869504451751709
Iteration 16: train_loss 5.8517985343933105
Iteration 17: train_loss 5.926417827606201
Iteration 18: train_loss 5.981281757354736
Iteration 19: train_loss 6.006900787353516
Iteration 20: train_loss 6.030152797698975
Iteration 21: train_loss 5.8877973556518555
Iteration 22: train_loss 5.9366631507873535
Iteration 23: train_loss 6.043260097503662
Iteration 24: train_loss 5.947267532348633
Iteration 25: train_loss 5.967743396759033
Iteration 26: train_loss 6.068919658660889
Iteration 27: train_loss 5.977525234222412
Iteration 28: train_loss 5.925328731536865
Iteration 29: train_loss 6.076552867889404
Iteration 30: train_loss 5.991903781890869
Iteration 31: train_loss 5.786484718322754
Iteration 32: train_loss 5.899066925048828
Iteration 33: train_loss 5.906088352203369
Iteration 34: train_loss 5.860945701599121
Iteration 35: train_loss 5.843684673309326
Iteration 36: train_loss 5.834353446960449
Iteration 37: train_loss 5.76783561706543
Iteration 38: train_loss 5.878130912780762
Iteration 39: train_loss 5.771470069885254
Iteration 40: train_loss 5.702017307281494
Epoch 10: train_avg_loss 5.880888962745667 eval_avg_acc: 0.04161254472196928 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:08:03] [32mIntermediate result: 0.04161254472196928  (Index 9)[0m
================Epoch: 11================
Iteration 1: train_loss 5.955341339111328
Iteration 2: train_loss 5.816812515258789
Iteration 3: train_loss 5.802703380584717
Iteration 4: train_loss 5.710694313049316
Iteration 5: train_loss 5.639998435974121
Iteration 6: train_loss 5.800787448883057
Iteration 7: train_loss 5.700592517852783
Iteration 8: train_loss 5.673285007476807
Iteration 9: train_loss 5.738063812255859
Iteration 10: train_loss 5.687889099121094
Iteration 11: train_loss 5.763206958770752
Iteration 12: train_loss 5.675650119781494
Iteration 13: train_loss 5.74302339553833
Iteration 14: train_loss 5.811575412750244
Iteration 15: train_loss 5.872841835021973
Iteration 16: train_loss 5.808289051055908
Iteration 17: train_loss 5.797977447509766
Iteration 18: train_loss 5.796864032745361
Iteration 19: train_loss 5.789621829986572
Iteration 20: train_loss 5.782642841339111
Iteration 21: train_loss 5.729079246520996
Iteration 22: train_loss 5.697961330413818
Iteration 23: train_loss 5.640415191650391
Iteration 24: train_loss 5.7546892166137695
Iteration 25: train_loss 5.8167009353637695
Iteration 26: train_loss 5.904211044311523
Iteration 27: train_loss 5.851820468902588
Iteration 28: train_loss 5.806663990020752
Iteration 29: train_loss 5.802539348602295
Iteration 30: train_loss 5.84226131439209
Iteration 31: train_loss 5.892426490783691
Iteration 32: train_loss 5.828458309173584
Iteration 33: train_loss 5.807621002197266
Iteration 34: train_loss 5.747735023498535
Iteration 35: train_loss 5.792638778686523
Iteration 36: train_loss 5.936525821685791
Iteration 37: train_loss 5.736963748931885
Iteration 38: train_loss 5.7524733543396
Iteration 39: train_loss 5.802897930145264
Iteration 40: train_loss 5.8773112297058105
Epoch 11: train_avg_loss 5.784731364250183 eval_avg_acc: 0.039129966402586445 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:08:15] [32mIntermediate result: 0.039129966402586445  (Index 10)[0m
================Epoch: 12================
Iteration 1: train_loss 5.785882472991943
Iteration 2: train_loss 5.880505561828613
Iteration 3: train_loss 5.832718372344971
Iteration 4: train_loss 5.783243656158447
Iteration 5: train_loss 5.904792308807373
Iteration 6: train_loss 5.847400188446045
Iteration 7: train_loss 5.92973518371582
Iteration 8: train_loss 5.946797847747803
Iteration 9: train_loss 6.014305114746094
Iteration 10: train_loss 5.986390590667725
Iteration 11: train_loss 5.8919758796691895
Iteration 12: train_loss 5.817683696746826
Iteration 13: train_loss 5.808594703674316
Iteration 14: train_loss 5.910825252532959
Iteration 15: train_loss 5.80689001083374
Iteration 16: train_loss 5.958328723907471
Iteration 17: train_loss 5.875093936920166
Iteration 18: train_loss 5.847908020019531
Iteration 19: train_loss 5.832458972930908
Iteration 20: train_loss 5.810431003570557
Iteration 21: train_loss 5.783178806304932
Iteration 22: train_loss 5.859842777252197
Iteration 23: train_loss 5.836453914642334
Iteration 24: train_loss 5.872932434082031
Iteration 25: train_loss 5.828399181365967
Iteration 26: train_loss 5.778431415557861
Iteration 27: train_loss 5.830282211303711
Iteration 28: train_loss 5.855436325073242
Iteration 29: train_loss 5.86222505569458
Iteration 30: train_loss 5.9742751121521
Iteration 31: train_loss 5.7902374267578125
Iteration 32: train_loss 5.843320369720459
Iteration 33: train_loss 5.884596824645996
Iteration 34: train_loss 5.925314903259277
Iteration 35: train_loss 5.778975009918213
Iteration 36: train_loss 5.804540157318115
Iteration 37: train_loss 5.923034191131592
Iteration 38: train_loss 5.874472141265869
Iteration 39: train_loss 5.903881549835205
Iteration 40: train_loss 5.983614444732666
Epoch 12: train_avg_loss 5.8666351437568665 eval_avg_acc: 0.03890694365235472 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:08:28] [32mIntermediate result: 0.03890694365235472  (Index 11)[0m
================Epoch: 13================
Iteration 1: train_loss 5.8064680099487305
Iteration 2: train_loss 5.833090305328369
Iteration 3: train_loss 5.9145989418029785
Iteration 4: train_loss 5.892307281494141
Iteration 5: train_loss 5.855132102966309
Iteration 6: train_loss 5.900469779968262
Iteration 7: train_loss 5.762983322143555
Iteration 8: train_loss 5.835659027099609
Iteration 9: train_loss 5.898214817047119
Iteration 10: train_loss 5.778712272644043
Iteration 11: train_loss 5.796073913574219
Iteration 12: train_loss 5.839484214782715
Iteration 13: train_loss 5.8254499435424805
Iteration 14: train_loss 5.806914329528809
Iteration 15: train_loss 5.857807636260986
Iteration 16: train_loss 5.751428604125977
Iteration 17: train_loss 5.724912166595459
Iteration 18: train_loss 5.852114200592041
Iteration 19: train_loss 5.829929351806641
Iteration 20: train_loss 5.758293151855469
Iteration 21: train_loss 5.897929668426514
Iteration 22: train_loss 5.768850326538086
Iteration 23: train_loss 5.805283069610596
Iteration 24: train_loss 5.814895153045654
Iteration 25: train_loss 5.863392353057861
Iteration 26: train_loss 5.7436089515686035
Iteration 27: train_loss 5.745192527770996
Iteration 28: train_loss 5.7861857414245605
Iteration 29: train_loss 5.758364200592041
Iteration 30: train_loss 5.800326347351074
Iteration 31: train_loss 5.745748043060303
Iteration 32: train_loss 5.689294815063477
Iteration 33: train_loss 5.680431365966797
Iteration 34: train_loss 5.674919605255127
Iteration 35: train_loss 5.806195259094238
Iteration 36: train_loss 5.83076286315918
Iteration 37: train_loss 5.742106914520264
Iteration 38: train_loss 5.821866035461426
Iteration 39: train_loss 5.866171836853027
Iteration 40: train_loss 5.719158172607422
Epoch 13: train_avg_loss 5.802018165588379 eval_avg_acc: 0.05159668879831951 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:08:41] [32mIntermediate result: 0.05159668879831951  (Index 12)[0m
================Epoch: 14================
Iteration 1: train_loss 5.825044631958008
Iteration 2: train_loss 5.755550861358643
Iteration 3: train_loss 5.736502647399902
Iteration 4: train_loss 5.664676666259766
Iteration 5: train_loss 5.762373924255371
Iteration 6: train_loss 5.734116554260254
Iteration 7: train_loss 5.718665599822998
Iteration 8: train_loss 5.727692127227783
Iteration 9: train_loss 5.892686367034912
Iteration 10: train_loss 5.780550003051758
Iteration 11: train_loss 5.6751933097839355
Iteration 12: train_loss 5.663259983062744
Iteration 13: train_loss 5.728291988372803
Iteration 14: train_loss 5.657959461212158
Iteration 15: train_loss 5.673113822937012
Iteration 16: train_loss 5.652796268463135
Iteration 17: train_loss 5.7339701652526855
Iteration 18: train_loss 5.645110607147217
Iteration 19: train_loss 5.5588154792785645
Iteration 20: train_loss 5.682750701904297
Iteration 21: train_loss 5.568646430969238
Iteration 22: train_loss 5.742619037628174
Iteration 23: train_loss 5.630746841430664
Iteration 24: train_loss 5.5886125564575195
Iteration 25: train_loss 5.627420902252197
Iteration 26: train_loss 5.7191386222839355
Iteration 27: train_loss 5.7890119552612305
Iteration 28: train_loss 5.690830230712891
Iteration 29: train_loss 5.699159145355225
Iteration 30: train_loss 5.690646648406982
Iteration 31: train_loss 5.6873698234558105
Iteration 32: train_loss 5.622599124908447
Iteration 33: train_loss 5.579030513763428
Iteration 34: train_loss 5.627257347106934
Iteration 35: train_loss 5.668566703796387
Iteration 36: train_loss 5.710702896118164
Iteration 37: train_loss 5.6419572830200195
Iteration 38: train_loss 5.495471477508545
Iteration 39: train_loss 5.670793056488037
Iteration 40: train_loss 5.450369358062744
Epoch 14: train_avg_loss 5.679251778125763 eval_avg_acc: 0.06801372909479597 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:08:53] [32mIntermediate result: 0.06801372909479597  (Index 13)[0m
================Epoch: 15================
Iteration 1: train_loss 5.6129937171936035
Iteration 2: train_loss 5.461587905883789
Iteration 3: train_loss 5.684373378753662
Iteration 4: train_loss 5.594369888305664
Iteration 5: train_loss 5.660407066345215
Iteration 6: train_loss 5.643354892730713
Iteration 7: train_loss 5.675783634185791
Iteration 8: train_loss 5.639100551605225
Iteration 9: train_loss 5.583519458770752
Iteration 10: train_loss 5.646517276763916
Iteration 11: train_loss 5.764697551727295
Iteration 12: train_loss 5.717259883880615
Iteration 13: train_loss 5.723551273345947
Iteration 14: train_loss 5.630229473114014
Iteration 15: train_loss 5.699184417724609
Iteration 16: train_loss 5.708996772766113
Iteration 17: train_loss 5.648599147796631
Iteration 18: train_loss 5.617600440979004
Iteration 19: train_loss 5.594507694244385
Iteration 20: train_loss 5.7705397605896
Iteration 21: train_loss 5.760650634765625
Iteration 22: train_loss 5.648715019226074
Iteration 23: train_loss 5.635970115661621
Iteration 24: train_loss 5.674709320068359
Iteration 25: train_loss 5.585676193237305
Iteration 26: train_loss 5.654627323150635
Iteration 27: train_loss 5.779182434082031
Iteration 28: train_loss 5.734839916229248
Iteration 29: train_loss 5.692205429077148
Iteration 30: train_loss 5.65542459487915
Iteration 31: train_loss 5.54075288772583
Iteration 32: train_loss 5.798305988311768
Iteration 33: train_loss 5.627671718597412
Iteration 34: train_loss 5.627652168273926
Iteration 35: train_loss 5.61640739440918
Iteration 36: train_loss 5.703824520111084
Iteration 37: train_loss 5.574227333068848
Iteration 38: train_loss 5.521341800689697
Iteration 39: train_loss 5.640453815460205
Iteration 40: train_loss 4.853736877441406
Epoch 15: train_avg_loss 5.635088741779327 eval_avg_acc: 0.057241220260350036 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:09:05] [32mIntermediate result: 0.057241220260350036  (Index 14)[0m
================Epoch: 16================
Iteration 1: train_loss 5.645303249359131
Iteration 2: train_loss 5.463119983673096
Iteration 3: train_loss 5.564059734344482
Iteration 4: train_loss 5.582961082458496
Iteration 5: train_loss 5.695141315460205
Iteration 6: train_loss 5.652027606964111
Iteration 7: train_loss 5.549289226531982
Iteration 8: train_loss 5.508485794067383
Iteration 9: train_loss 5.543745994567871
Iteration 10: train_loss 5.613408088684082
Iteration 11: train_loss 5.543467998504639
Iteration 12: train_loss 5.644487380981445
Iteration 13: train_loss 5.5624098777771
Iteration 14: train_loss 5.602431774139404
Iteration 15: train_loss 5.62948751449585
Iteration 16: train_loss 5.581898212432861
Iteration 17: train_loss 5.6382880210876465
Iteration 18: train_loss 5.554891586303711
Iteration 19: train_loss 5.687983512878418
Iteration 20: train_loss 5.6885271072387695
Iteration 21: train_loss 5.473740577697754
Iteration 22: train_loss 5.5156378746032715
Iteration 23: train_loss 5.5904059410095215
Iteration 24: train_loss 5.572507381439209
Iteration 25: train_loss 5.6237616539001465
Iteration 26: train_loss 5.646631717681885
Iteration 27: train_loss 5.620414733886719
Iteration 28: train_loss 5.613094806671143
Iteration 29: train_loss 5.537664890289307
Iteration 30: train_loss 5.543498516082764
Iteration 31: train_loss 5.584634780883789
Iteration 32: train_loss 5.698293209075928
Iteration 33: train_loss 5.675391674041748
Iteration 34: train_loss 5.70379638671875
Iteration 35: train_loss 5.59374475479126
Iteration 36: train_loss 5.654311656951904
Iteration 37: train_loss 5.591710090637207
Iteration 38: train_loss 5.725113391876221
Iteration 39: train_loss 5.632101058959961
Iteration 40: train_loss 5.850661277770996
Epoch 16: train_avg_loss 5.609963285923004 eval_avg_acc: 0.05786125494658403 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:09:18] [32mIntermediate result: 0.05786125494658403  (Index 15)[0m
================Epoch: 17================
Iteration 1: train_loss 5.681948661804199
Iteration 2: train_loss 5.615983963012695
Iteration 3: train_loss 5.673877716064453
Iteration 4: train_loss 5.618201732635498
Iteration 5: train_loss 5.491225242614746
Iteration 6: train_loss 5.790517330169678
Iteration 7: train_loss 5.624722480773926
Iteration 8: train_loss 5.6771674156188965
Iteration 9: train_loss 5.6751532554626465
Iteration 10: train_loss 5.638724327087402
Iteration 11: train_loss 5.620093822479248
Iteration 12: train_loss 5.619390487670898
Iteration 13: train_loss 5.707113265991211
Iteration 14: train_loss 5.545041561126709
Iteration 15: train_loss 5.493203639984131
Iteration 16: train_loss 5.554111003875732
Iteration 17: train_loss 5.491630554199219
Iteration 18: train_loss 5.638970375061035
Iteration 19: train_loss 5.550545692443848
Iteration 20: train_loss 5.652177333831787
Iteration 21: train_loss 5.532983303070068
Iteration 22: train_loss 5.575430870056152
Iteration 23: train_loss 5.508845329284668
Iteration 24: train_loss 5.492948055267334
Iteration 25: train_loss 5.386137962341309
Iteration 26: train_loss 5.536158084869385
Iteration 27: train_loss 5.471043586730957
Iteration 28: train_loss 5.578872203826904
Iteration 29: train_loss 5.500097751617432
Iteration 30: train_loss 5.5512800216674805
Iteration 31: train_loss 5.625291347503662
Iteration 32: train_loss 5.587069511413574
Iteration 33: train_loss 5.617310047149658
Iteration 34: train_loss 5.502315998077393
Iteration 35: train_loss 5.471334934234619
Iteration 36: train_loss 5.5487871170043945
Iteration 37: train_loss 5.609598636627197
Iteration 38: train_loss 5.440390110015869
Iteration 39: train_loss 5.5378265380859375
Iteration 40: train_loss 5.354735851287842
Epoch 17: train_avg_loss 5.569706428050995 eval_avg_acc: 0.07206716149747816 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:09:31] [32mIntermediate result: 0.07206716149747816  (Index 16)[0m
================Epoch: 18================
Iteration 1: train_loss 5.612509250640869
Iteration 2: train_loss 5.57000207901001
Iteration 3: train_loss 5.575784206390381
Iteration 4: train_loss 5.389245510101318
Iteration 5: train_loss 5.475297927856445
Iteration 6: train_loss 5.498509883880615
Iteration 7: train_loss 5.547969341278076
Iteration 8: train_loss 5.581486225128174
Iteration 9: train_loss 5.531271934509277
Iteration 10: train_loss 5.501668453216553
Iteration 11: train_loss 5.549878120422363
Iteration 12: train_loss 5.504912853240967
Iteration 13: train_loss 5.600800037384033
Iteration 14: train_loss 5.5777812004089355
Iteration 15: train_loss 5.485856056213379
Iteration 16: train_loss 5.577197551727295
Iteration 17: train_loss 5.3754754066467285
Iteration 18: train_loss 5.430830478668213
Iteration 19: train_loss 5.483084678649902
Iteration 20: train_loss 5.417510032653809
Iteration 21: train_loss 5.608959197998047
Iteration 22: train_loss 5.452877044677734
Iteration 23: train_loss 5.360440254211426
Iteration 24: train_loss 5.4469733238220215
Iteration 25: train_loss 5.372767448425293
Iteration 26: train_loss 5.359779357910156
Iteration 27: train_loss 5.413156986236572
Iteration 28: train_loss 5.391373157501221
Iteration 29: train_loss 5.45259428024292
Iteration 30: train_loss 5.5713934898376465
Iteration 31: train_loss 5.484173774719238
Iteration 32: train_loss 5.464444160461426
Iteration 33: train_loss 5.526495933532715
Iteration 34: train_loss 5.31959342956543
Iteration 35: train_loss 5.465608596801758
Iteration 36: train_loss 5.461339950561523
Iteration 37: train_loss 5.44070291519165
Iteration 38: train_loss 5.447346210479736
Iteration 39: train_loss 5.330279350280762
Iteration 40: train_loss 5.037945747375488
Epoch 18: train_avg_loss 5.4673828959465025 eval_avg_acc: 0.08123959174627719 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:09:43] [32mIntermediate result: 0.08123959174627719  (Index 17)[0m
================Epoch: 19================
Iteration 1: train_loss 5.344481945037842
Iteration 2: train_loss 5.355461597442627
Iteration 3: train_loss 5.591679096221924
Iteration 4: train_loss 5.442831993103027
Iteration 5: train_loss 5.490468502044678
Iteration 6: train_loss 5.482961654663086
Iteration 7: train_loss 5.520215034484863
Iteration 8: train_loss 5.504101276397705
Iteration 9: train_loss 5.535037994384766
Iteration 10: train_loss 5.568399429321289
Iteration 11: train_loss 5.468564033508301
Iteration 12: train_loss 5.611408710479736
Iteration 13: train_loss 5.54573917388916
Iteration 14: train_loss 5.439005374908447
Iteration 15: train_loss 5.597573280334473
Iteration 16: train_loss 5.40492057800293
Iteration 17: train_loss 5.429285049438477
Iteration 18: train_loss 5.432181358337402
Iteration 19: train_loss 5.354283809661865
Iteration 20: train_loss 5.3639726638793945
Iteration 21: train_loss 5.377063751220703
Iteration 22: train_loss 5.462619304656982
Iteration 23: train_loss 5.504201889038086
Iteration 24: train_loss 5.40478515625
Iteration 25: train_loss 5.394662380218506
Iteration 26: train_loss 5.277487277984619
Iteration 27: train_loss 5.309731960296631
Iteration 28: train_loss 5.248498439788818
Iteration 29: train_loss 5.345418453216553
Iteration 30: train_loss 5.27341365814209
Iteration 31: train_loss 5.1938090324401855
Iteration 32: train_loss 5.172163486480713
Iteration 33: train_loss 5.369699478149414
Iteration 34: train_loss 5.341744899749756
Iteration 35: train_loss 5.354034900665283
Iteration 36: train_loss 5.305018424987793
Iteration 37: train_loss 5.500006198883057
Iteration 38: train_loss 5.319807529449463
Iteration 39: train_loss 5.332747936248779
Iteration 40: train_loss 5.643179893493652
Epoch 19: train_avg_loss 5.415316665172577 eval_avg_acc: 0.09251394766576801 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:09:55] [32mIntermediate result: 0.09251394766576801  (Index 18)[0m
================Epoch: 20================
Iteration 1: train_loss 5.318871021270752
Iteration 2: train_loss 5.342454433441162
Iteration 3: train_loss 5.355574131011963
Iteration 4: train_loss 5.423861026763916
Iteration 5: train_loss 5.459480285644531
Iteration 6: train_loss 5.246999263763428
Iteration 7: train_loss 5.4169769287109375
Iteration 8: train_loss 5.427773475646973
Iteration 9: train_loss 5.220495223999023
Iteration 10: train_loss 5.335813045501709
Iteration 11: train_loss 5.378698348999023
Iteration 12: train_loss 5.479392051696777
Iteration 13: train_loss 5.382519245147705
Iteration 14: train_loss 5.447323322296143
Iteration 15: train_loss 5.408061981201172
Iteration 16: train_loss 5.241899490356445
Iteration 17: train_loss 5.331854820251465
Iteration 18: train_loss 5.373648166656494
Iteration 19: train_loss 5.402346611022949
Iteration 20: train_loss 5.437217712402344
Iteration 21: train_loss 5.46657657623291
Iteration 22: train_loss 5.4129462242126465
Iteration 23: train_loss 5.506381988525391
Iteration 24: train_loss 5.478231906890869
Iteration 25: train_loss 5.427255630493164
Iteration 26: train_loss 5.290915012359619
Iteration 27: train_loss 5.255825996398926
Iteration 28: train_loss 5.442932605743408
Iteration 29: train_loss 5.413456916809082
Iteration 30: train_loss 5.372716426849365
Iteration 31: train_loss 5.3252458572387695
Iteration 32: train_loss 5.257135391235352
Iteration 33: train_loss 5.235848903656006
Iteration 34: train_loss 5.289999485015869
Iteration 35: train_loss 5.334407806396484
Iteration 36: train_loss 5.290668964385986
Iteration 37: train_loss 5.333952903747559
Iteration 38: train_loss 5.388773441314697
Iteration 39: train_loss 5.470284461975098
Iteration 40: train_loss 5.356880187988281
Epoch 20: train_avg_loss 5.36954243183136 eval_avg_acc: 0.09668428951510293 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:10:07] [32mIntermediate result: 0.09668428951510293  (Index 19)[0m
================Epoch: 21================
Iteration 1: train_loss 5.374902725219727
Iteration 2: train_loss 5.344829082489014
Iteration 3: train_loss 5.247067928314209
Iteration 4: train_loss 5.335108280181885
Iteration 5: train_loss 5.39502477645874
Iteration 6: train_loss 5.235990047454834
Iteration 7: train_loss 5.172435283660889
Iteration 8: train_loss 5.268605709075928
Iteration 9: train_loss 5.401242256164551
Iteration 10: train_loss 5.289793491363525
Iteration 11: train_loss 5.405391216278076
Iteration 12: train_loss 5.2981390953063965
Iteration 13: train_loss 5.12945556640625
Iteration 14: train_loss 5.173280715942383
Iteration 15: train_loss 5.3398823738098145
Iteration 16: train_loss 5.391181468963623
Iteration 17: train_loss 5.240321636199951
Iteration 18: train_loss 5.282907009124756
Iteration 19: train_loss 5.086826324462891
Iteration 20: train_loss 5.248485565185547
Iteration 21: train_loss 5.188746929168701
Iteration 22: train_loss 5.2416486740112305
Iteration 23: train_loss 5.090864181518555
Iteration 24: train_loss 5.3265790939331055
Iteration 25: train_loss 5.434401988983154
Iteration 26: train_loss 5.291973114013672
Iteration 27: train_loss 5.377374649047852
Iteration 28: train_loss 5.327502727508545
Iteration 29: train_loss 5.326807022094727
Iteration 30: train_loss 5.2644805908203125
Iteration 31: train_loss 5.390688419342041
Iteration 32: train_loss 5.185286045074463
Iteration 33: train_loss 5.285647869110107
Iteration 34: train_loss 5.4284257888793945
Iteration 35: train_loss 5.231262683868408
Iteration 36: train_loss 5.261641025543213
Iteration 37: train_loss 5.315332889556885
Iteration 38: train_loss 5.2976603507995605
Iteration 39: train_loss 5.170234203338623
Iteration 40: train_loss 5.095564842224121
Epoch 21: train_avg_loss 5.279824841022491 eval_avg_acc: 0.09428781845025522 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:10:20] [32mIntermediate result: 0.09428781845025522  (Index 20)[0m
================Epoch: 22================
Iteration 1: train_loss 5.358539581298828
Iteration 2: train_loss 5.471194267272949
Iteration 3: train_loss 5.446662425994873
Iteration 4: train_loss 5.345823287963867
Iteration 5: train_loss 5.20976448059082
Iteration 6: train_loss 5.281984806060791
Iteration 7: train_loss 5.381234169006348
Iteration 8: train_loss 5.402532577514648
Iteration 9: train_loss 5.298311710357666
Iteration 10: train_loss 5.2644829750061035
Iteration 11: train_loss 5.281881809234619
Iteration 12: train_loss 5.277905464172363
Iteration 13: train_loss 5.288492679595947
Iteration 14: train_loss 5.321902275085449
Iteration 15: train_loss 5.22646427154541
Iteration 16: train_loss 5.385356426239014
Iteration 17: train_loss 5.351953983306885
Iteration 18: train_loss 5.2501115798950195
Iteration 19: train_loss 5.327624320983887
Iteration 20: train_loss 5.403614521026611
Iteration 21: train_loss 5.295324802398682
Iteration 22: train_loss 5.156431198120117
Iteration 23: train_loss 5.38203763961792
Iteration 24: train_loss 5.31072998046875
Iteration 25: train_loss 5.205097675323486
Iteration 26: train_loss 5.221844673156738
Iteration 27: train_loss 5.104846954345703
Iteration 28: train_loss 5.192318439483643
Iteration 29: train_loss 5.277121543884277
Iteration 30: train_loss 5.300504207611084
Iteration 31: train_loss 5.17567777633667
Iteration 32: train_loss 5.117443561553955
Iteration 33: train_loss 5.133860111236572
Iteration 34: train_loss 5.312990188598633
Iteration 35: train_loss 5.255168914794922
Iteration 36: train_loss 5.20174503326416
Iteration 37: train_loss 5.32156229019165
Iteration 38: train_loss 5.183466911315918
Iteration 39: train_loss 5.110944747924805
Iteration 40: train_loss 4.8261308670043945
Epoch 22: train_avg_loss 5.266527128219605 eval_avg_acc: 0.1095244970749231 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:10:32] [32mIntermediate result: 0.1095244970749231  (Index 21)[0m
================Epoch: 23================
Iteration 1: train_loss 5.183933258056641
Iteration 2: train_loss 5.218573093414307
Iteration 3: train_loss 5.101783752441406
Iteration 4: train_loss 5.1896162033081055
Iteration 5: train_loss 5.142822265625
Iteration 6: train_loss 5.219937324523926
Iteration 7: train_loss 5.32678747177124
Iteration 8: train_loss 5.262699127197266
Iteration 9: train_loss 5.169295787811279
Iteration 10: train_loss 5.161368370056152
Iteration 11: train_loss 5.064414024353027
Iteration 12: train_loss 5.0951337814331055
Iteration 13: train_loss 5.166500091552734
Iteration 14: train_loss 5.182875633239746
Iteration 15: train_loss 5.260352611541748
Iteration 16: train_loss 5.247391700744629
Iteration 17: train_loss 5.065580368041992
Iteration 18: train_loss 5.288080215454102
Iteration 19: train_loss 5.206592559814453
Iteration 20: train_loss 5.0433502197265625
Iteration 21: train_loss 5.183291912078857
Iteration 22: train_loss 5.137375831604004
Iteration 23: train_loss 5.247260570526123
Iteration 24: train_loss 5.1833953857421875
Iteration 25: train_loss 5.23936128616333
Iteration 26: train_loss 5.155046463012695
Iteration 27: train_loss 5.201541423797607
Iteration 28: train_loss 5.15656852722168
Iteration 29: train_loss 5.245174407958984
Iteration 30: train_loss 5.064730644226074
Iteration 31: train_loss 5.22875452041626
Iteration 32: train_loss 5.183218479156494
Iteration 33: train_loss 5.102261066436768
Iteration 34: train_loss 5.118132591247559
Iteration 35: train_loss 5.241273403167725
Iteration 36: train_loss 5.108935832977295
Iteration 37: train_loss 5.103549003601074
Iteration 38: train_loss 5.121888160705566
Iteration 39: train_loss 5.351441860198975
Iteration 40: train_loss 5.194535255432129
Epoch 23: train_avg_loss 5.17912061214447 eval_avg_acc: 0.11361491825564621 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:10:44] [32mIntermediate result: 0.11361491825564621  (Index 22)[0m
================Epoch: 24================
Iteration 1: train_loss 5.142786026000977
Iteration 2: train_loss 5.0971550941467285
Iteration 3: train_loss 4.9725165367126465
Iteration 4: train_loss 5.057498455047607
Iteration 5: train_loss 5.09731912612915
Iteration 6: train_loss 5.0194854736328125
Iteration 7: train_loss 5.087582588195801
Iteration 8: train_loss 5.078835487365723
Iteration 9: train_loss 5.019317626953125
Iteration 10: train_loss 5.109925746917725
Iteration 11: train_loss 5.197103023529053
Iteration 12: train_loss 5.113866329193115
Iteration 13: train_loss 5.340578079223633
Iteration 14: train_loss 5.101565361022949
Iteration 15: train_loss 5.187353610992432
Iteration 16: train_loss 5.1697869300842285
Iteration 17: train_loss 5.158631801605225
Iteration 18: train_loss 5.11375093460083
Iteration 19: train_loss 5.098108291625977
Iteration 20: train_loss 5.39070463180542
Iteration 21: train_loss 4.989940643310547
Iteration 22: train_loss 5.138237476348877
Iteration 23: train_loss 5.069609642028809
Iteration 24: train_loss 5.177245140075684
Iteration 25: train_loss 5.2472124099731445
Iteration 26: train_loss 5.175443172454834
Iteration 27: train_loss 5.257946491241455
Iteration 28: train_loss 5.082016468048096
Iteration 29: train_loss 5.052962303161621
Iteration 30: train_loss 5.017697811126709
Iteration 31: train_loss 5.1165852546691895
Iteration 32: train_loss 5.0183424949646
Iteration 33: train_loss 5.042661190032959
Iteration 34: train_loss 5.152072906494141
Iteration 35: train_loss 5.115100383758545
Iteration 36: train_loss 5.141044616699219
Iteration 37: train_loss 5.182258129119873
Iteration 38: train_loss 5.044734001159668
Iteration 39: train_loss 5.112999439239502
Iteration 40: train_loss 5.653868675231934
Epoch 24: train_avg_loss 5.133546245098114 eval_avg_acc: 0.11860954329872608 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:10:57] [32mIntermediate result: 0.11860954329872608  (Index 23)[0m
================Epoch: 25================
Iteration 1: train_loss 5.072200298309326
Iteration 2: train_loss 5.20829439163208
Iteration 3: train_loss 5.155420780181885
Iteration 4: train_loss 5.095083713531494
Iteration 5: train_loss 5.012241840362549
Iteration 6: train_loss 5.219122886657715
Iteration 7: train_loss 5.145081520080566
Iteration 8: train_loss 5.078682899475098
Iteration 9: train_loss 5.0008111000061035
Iteration 10: train_loss 4.986947536468506
Iteration 11: train_loss 4.999261379241943
Iteration 12: train_loss 4.967965602874756
Iteration 13: train_loss 5.0472412109375
Iteration 14: train_loss 5.075387477874756
Iteration 15: train_loss 5.145817279815674
Iteration 16: train_loss 4.9872260093688965
Iteration 17: train_loss 5.110462188720703
Iteration 18: train_loss 5.070094585418701
Iteration 19: train_loss 5.067897319793701
Iteration 20: train_loss 5.069474697113037
Iteration 21: train_loss 5.116189479827881
Iteration 22: train_loss 5.178679943084717
Iteration 23: train_loss 4.920464992523193
Iteration 24: train_loss 5.009010314941406
Iteration 25: train_loss 5.07636833190918
Iteration 26: train_loss 5.0138678550720215
Iteration 27: train_loss 5.000151634216309
Iteration 28: train_loss 4.948635101318359
Iteration 29: train_loss 5.133520603179932
Iteration 30: train_loss 5.142930507659912
Iteration 31: train_loss 4.940802574157715
Iteration 32: train_loss 5.001769065856934
Iteration 33: train_loss 5.10049295425415
Iteration 34: train_loss 5.092343330383301
Iteration 35: train_loss 5.059176921844482
Iteration 36: train_loss 4.8636088371276855
Iteration 37: train_loss 5.130921840667725
Iteration 38: train_loss 5.195251941680908
Iteration 39: train_loss 4.919445514678955
Iteration 40: train_loss 4.713059902191162
Epoch 25: train_avg_loss 5.051785159111023 eval_avg_acc: 0.11539889107508612 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:11:09] [32mIntermediate result: 0.11539889107508612  (Index 24)[0m
================Epoch: 26================
Iteration 1: train_loss 5.291004657745361
Iteration 2: train_loss 5.128850936889648
Iteration 3: train_loss 5.159121990203857
Iteration 4: train_loss 5.182259559631348
Iteration 5: train_loss 5.000604629516602
Iteration 6: train_loss 5.0482072830200195
Iteration 7: train_loss 4.936506748199463
Iteration 8: train_loss 5.039631366729736
Iteration 9: train_loss 4.942293643951416
Iteration 10: train_loss 5.0921502113342285
Iteration 11: train_loss 5.047680377960205
Iteration 12: train_loss 4.918490409851074
Iteration 13: train_loss 5.108530521392822
Iteration 14: train_loss 5.0647711753845215
Iteration 15: train_loss 4.992294788360596
Iteration 16: train_loss 5.007843971252441
Iteration 17: train_loss 5.117320537567139
Iteration 18: train_loss 5.045541763305664
Iteration 19: train_loss 4.9782795906066895
Iteration 20: train_loss 5.123311519622803
Iteration 21: train_loss 5.011058330535889
Iteration 22: train_loss 5.064469814300537
Iteration 23: train_loss 4.969122886657715
Iteration 24: train_loss 4.955131530761719
Iteration 25: train_loss 4.963066101074219
Iteration 26: train_loss 5.046942710876465
Iteration 27: train_loss 5.042884826660156
Iteration 28: train_loss 4.905628204345703
Iteration 29: train_loss 5.011787414550781
Iteration 30: train_loss 4.852750778198242
Iteration 31: train_loss 4.9650654792785645
Iteration 32: train_loss 4.901532173156738
Iteration 33: train_loss 5.01487398147583
Iteration 34: train_loss 5.08521842956543
Iteration 35: train_loss 4.947444915771484
Iteration 36: train_loss 5.101900100708008
Iteration 37: train_loss 5.16672420501709
Iteration 38: train_loss 5.073540210723877
Iteration 39: train_loss 4.967543601989746
Iteration 40: train_loss 5.490884304046631
Epoch 26: train_avg_loss 5.044056642055511 eval_avg_acc: 0.12470699135485139 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:11:22] [32mIntermediate result: 0.12470699135485139  (Index 25)[0m
================Epoch: 27================
Iteration 1: train_loss 4.92728853225708
Iteration 2: train_loss 4.9228835105896
Iteration 3: train_loss 5.109382629394531
Iteration 4: train_loss 4.911780834197998
Iteration 5: train_loss 5.009773254394531
Iteration 6: train_loss 4.874344825744629
Iteration 7: train_loss 4.924139499664307
Iteration 8: train_loss 4.866179466247559
Iteration 9: train_loss 4.736408233642578
Iteration 10: train_loss 5.030052661895752
Iteration 11: train_loss 4.887869834899902
Iteration 12: train_loss 4.914336204528809
Iteration 13: train_loss 4.962184429168701
Iteration 14: train_loss 5.015953063964844
Iteration 15: train_loss 5.042220115661621
Iteration 16: train_loss 4.980184555053711
Iteration 17: train_loss 4.884475231170654
Iteration 18: train_loss 4.9159345626831055
Iteration 19: train_loss 4.788611888885498
Iteration 20: train_loss 4.918616771697998
Iteration 21: train_loss 4.959110260009766
Iteration 22: train_loss 4.960921764373779
Iteration 23: train_loss 5.049800872802734
Iteration 24: train_loss 4.897091865539551
Iteration 25: train_loss 4.992255210876465
Iteration 26: train_loss 4.991354942321777
Iteration 27: train_loss 4.844316005706787
Iteration 28: train_loss 4.94602632522583
Iteration 29: train_loss 4.962310314178467
Iteration 30: train_loss 4.962114334106445
Iteration 31: train_loss 4.915805816650391
Iteration 32: train_loss 5.123725414276123
Iteration 33: train_loss 5.06355619430542
Iteration 34: train_loss 4.945218086242676
Iteration 35: train_loss 4.972774505615234
Iteration 36: train_loss 5.024199962615967
Iteration 37: train_loss 5.002713203430176
Iteration 38: train_loss 5.033400535583496
Iteration 39: train_loss 4.848480701446533
Iteration 40: train_loss 5.116057395935059
Epoch 27: train_avg_loss 4.955846345424652 eval_avg_acc: 0.12706451382753606 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:11:34] [32mIntermediate result: 0.12706451382753606  (Index 26)[0m
================Epoch: 28================
Iteration 1: train_loss 5.031301498413086
Iteration 2: train_loss 4.9933762550354
Iteration 3: train_loss 4.931847095489502
Iteration 4: train_loss 4.936981678009033
Iteration 5: train_loss 5.006036281585693
Iteration 6: train_loss 4.846536636352539
Iteration 7: train_loss 4.956844806671143
Iteration 8: train_loss 4.93416166305542
Iteration 9: train_loss 5.088252544403076
Iteration 10: train_loss 5.103796482086182
Iteration 11: train_loss 4.909811973571777
Iteration 12: train_loss 4.890273094177246
Iteration 13: train_loss 4.919436454772949
Iteration 14: train_loss 4.945566177368164
Iteration 15: train_loss 4.898108005523682
Iteration 16: train_loss 4.982352256774902
Iteration 17: train_loss 4.756155014038086
Iteration 18: train_loss 5.105033874511719
Iteration 19: train_loss 5.089835166931152
Iteration 20: train_loss 4.9293084144592285
Iteration 21: train_loss 5.047477722167969
Iteration 22: train_loss 4.743339538574219
Iteration 23: train_loss 4.826411724090576
Iteration 24: train_loss 4.766091346740723
Iteration 25: train_loss 4.941126346588135
Iteration 26: train_loss 4.931315898895264
Iteration 27: train_loss 5.081393241882324
Iteration 28: train_loss 4.6642746925354
Iteration 29: train_loss 4.967532157897949
Iteration 30: train_loss 4.7682037353515625
Iteration 31: train_loss 5.0899224281311035
Iteration 32: train_loss 4.869073867797852
Iteration 33: train_loss 4.783533573150635
Iteration 34: train_loss 4.732621192932129
Iteration 35: train_loss 4.974330902099609
Iteration 36: train_loss 5.010895252227783
Iteration 37: train_loss 4.949540138244629
Iteration 38: train_loss 4.89479398727417
Iteration 39: train_loss 4.976273059844971
Iteration 40: train_loss 4.990202903747559
Epoch 28: train_avg_loss 4.931584227085113 eval_avg_acc: 0.1364908469180039 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:11:46] [32mIntermediate result: 0.1364908469180039  (Index 27)[0m
================Epoch: 29================
Iteration 1: train_loss 4.806231498718262
Iteration 2: train_loss 5.005679130554199
Iteration 3: train_loss 4.910770416259766
Iteration 4: train_loss 4.818246841430664
Iteration 5: train_loss 4.818436145782471
Iteration 6: train_loss 4.7961225509643555
Iteration 7: train_loss 4.8395891189575195
Iteration 8: train_loss 4.8441386222839355
Iteration 9: train_loss 5.135130405426025
Iteration 10: train_loss 4.753404140472412
Iteration 11: train_loss 4.841345310211182
Iteration 12: train_loss 5.002659797668457
Iteration 13: train_loss 4.859700679779053
Iteration 14: train_loss 4.9391255378723145
Iteration 15: train_loss 4.822633743286133
Iteration 16: train_loss 4.770833969116211
Iteration 17: train_loss 4.806495189666748
Iteration 18: train_loss 4.655495643615723
Iteration 19: train_loss 4.710796356201172
Iteration 20: train_loss 4.786838054656982
Iteration 21: train_loss 4.811576843261719
Iteration 22: train_loss 4.907678604125977
Iteration 23: train_loss 4.83024263381958
Iteration 24: train_loss 4.93644380569458
Iteration 25: train_loss 4.857953071594238
Iteration 26: train_loss 4.811339855194092
Iteration 27: train_loss 4.920591831207275
Iteration 28: train_loss 4.7748494148254395
Iteration 29: train_loss 4.897952079772949
Iteration 30: train_loss 4.834014415740967
Iteration 31: train_loss 4.859957695007324
Iteration 32: train_loss 5.015262603759766
Iteration 33: train_loss 4.696844100952148
Iteration 34: train_loss 4.7634172439575195
Iteration 35: train_loss 4.850773811340332
Iteration 36: train_loss 4.801611423492432
Iteration 37: train_loss 4.788290977478027
Iteration 38: train_loss 4.906484127044678
Iteration 39: train_loss 4.764329433441162
Iteration 40: train_loss 5.132651329040527
Epoch 29: train_avg_loss 4.852148461341858 eval_avg_acc: 0.1293460404617835 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:11:59] [32mIntermediate result: 0.1293460404617835  (Index 28)[0m
================Epoch: 30================
Iteration 1: train_loss 4.875736236572266
Iteration 2: train_loss 4.978895664215088
Iteration 3: train_loss 4.798683166503906
Iteration 4: train_loss 4.710287094116211
Iteration 5: train_loss 4.852137088775635
Iteration 6: train_loss 4.894032955169678
Iteration 7: train_loss 4.952400207519531
Iteration 8: train_loss 4.861896991729736
Iteration 9: train_loss 4.573617458343506
Iteration 10: train_loss 4.7737298011779785
Iteration 11: train_loss 4.837841510772705
Iteration 12: train_loss 4.837224960327148
Iteration 13: train_loss 4.9398651123046875
Iteration 14: train_loss 4.707300186157227
Iteration 15: train_loss 4.955898761749268
Iteration 16: train_loss 4.704381465911865
Iteration 17: train_loss 4.830833911895752
Iteration 18: train_loss 4.871515274047852
Iteration 19: train_loss 4.838990211486816
Iteration 20: train_loss 4.7788848876953125
Iteration 21: train_loss 4.8597731590271
Iteration 22: train_loss 4.744134426116943
Iteration 23: train_loss 4.790335655212402
Iteration 24: train_loss 4.762321949005127
Iteration 25: train_loss 4.8327250480651855
Iteration 26: train_loss 4.765503883361816
Iteration 27: train_loss 4.711366653442383
Iteration 28: train_loss 4.856427192687988
Iteration 29: train_loss 4.802942752838135
Iteration 30: train_loss 4.760113716125488
Iteration 31: train_loss 4.842284679412842
Iteration 32: train_loss 4.8711042404174805
Iteration 33: train_loss 4.895095348358154
Iteration 34: train_loss 4.824023246765137
Iteration 35: train_loss 4.832101345062256
Iteration 36: train_loss 4.822530269622803
Iteration 37: train_loss 4.79039192199707
Iteration 38: train_loss 4.842133522033691
Iteration 39: train_loss 4.732682704925537
Iteration 40: train_loss 5.220810890197754
Epoch 30: train_avg_loss 4.828323888778686 eval_avg_acc: 0.1482437316893242 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:12:11] [32mIntermediate result: 0.1482437316893242  (Index 29)[0m
================Epoch: 31================
Iteration 1: train_loss 4.852074146270752
Iteration 2: train_loss 4.8556904792785645
Iteration 3: train_loss 4.755336284637451
Iteration 4: train_loss 4.806982517242432
Iteration 5: train_loss 4.653078556060791
Iteration 6: train_loss 4.825900077819824
Iteration 7: train_loss 4.68088436126709
Iteration 8: train_loss 4.673384666442871
Iteration 9: train_loss 4.808498859405518
Iteration 10: train_loss 4.736878871917725
Iteration 11: train_loss 4.7995476722717285
Iteration 12: train_loss 4.869166374206543
Iteration 13: train_loss 4.64512825012207
Iteration 14: train_loss 4.663442134857178
Iteration 15: train_loss 4.678811550140381
Iteration 16: train_loss 4.734628677368164
Iteration 17: train_loss 4.727336406707764
Iteration 18: train_loss 4.665309906005859
Iteration 19: train_loss 4.673898696899414
Iteration 20: train_loss 4.80164098739624
Iteration 21: train_loss 4.86680269241333
Iteration 22: train_loss 4.787856101989746
Iteration 23: train_loss 4.777019500732422
Iteration 24: train_loss 4.734116077423096
Iteration 25: train_loss 4.597588062286377
Iteration 26: train_loss 4.898207187652588
Iteration 27: train_loss 4.704223155975342
Iteration 28: train_loss 4.826805114746094
Iteration 29: train_loss 4.650290012359619
Iteration 30: train_loss 4.865336894989014
Iteration 31: train_loss 4.904120922088623
Iteration 32: train_loss 4.766538143157959
Iteration 33: train_loss 4.748502254486084
Iteration 34: train_loss 4.770676612854004
Iteration 35: train_loss 4.7871785163879395
Iteration 36: train_loss 4.902555465698242
Iteration 37: train_loss 4.551308631896973
Iteration 38: train_loss 4.660635948181152
Iteration 39: train_loss 4.688053607940674
Iteration 40: train_loss 4.220988750457764
Epoch 31: train_avg_loss 4.740410578250885 eval_avg_acc: 0.1461016951516011 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:12:23] [32mIntermediate result: 0.1461016951516011  (Index 30)[0m
================Epoch: 32================
Iteration 1: train_loss 4.893466949462891
Iteration 2: train_loss 4.731160640716553
Iteration 3: train_loss 4.724313735961914
Iteration 4: train_loss 4.812376976013184
Iteration 5: train_loss 4.847223281860352
Iteration 6: train_loss 4.752257823944092
Iteration 7: train_loss 4.615941524505615
Iteration 8: train_loss 4.716429710388184
Iteration 9: train_loss 4.5504679679870605
Iteration 10: train_loss 4.820131301879883
Iteration 11: train_loss 4.73530912399292
Iteration 12: train_loss 4.736690521240234
Iteration 13: train_loss 4.752886772155762
Iteration 14: train_loss 4.699329853057861
Iteration 15: train_loss 4.713287353515625
Iteration 16: train_loss 4.731832504272461
Iteration 17: train_loss 4.591543674468994
Iteration 18: train_loss 4.736117839813232
Iteration 19: train_loss 4.481069087982178
Iteration 20: train_loss 4.5815629959106445
Iteration 21: train_loss 4.6854705810546875
Iteration 22: train_loss 4.584165573120117
Iteration 23: train_loss 4.6809186935424805
Iteration 24: train_loss 4.671180725097656
Iteration 25: train_loss 4.652299404144287
Iteration 26: train_loss 4.559365749359131
Iteration 27: train_loss 4.610174179077148
Iteration 28: train_loss 4.638573169708252
Iteration 29: train_loss 4.772247791290283
Iteration 30: train_loss 4.582879543304443
Iteration 31: train_loss 4.677952289581299
Iteration 32: train_loss 4.73502779006958
Iteration 33: train_loss 4.486048221588135
Iteration 34: train_loss 4.680093765258789
Iteration 35: train_loss 4.692169189453125
Iteration 36: train_loss 4.660898685455322
Iteration 37: train_loss 4.656950950622559
Iteration 38: train_loss 4.740254878997803
Iteration 39: train_loss 4.645690441131592
Iteration 40: train_loss 4.724949359893799
Epoch 32: train_avg_loss 4.684017765522003 eval_avg_acc: 0.1487548436237063 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:12:36] [32mIntermediate result: 0.1487548436237063  (Index 31)[0m
================Epoch: 33================
Iteration 1: train_loss 4.821340560913086
Iteration 2: train_loss 4.817583084106445
Iteration 3: train_loss 4.825142860412598
Iteration 4: train_loss 4.726714134216309
Iteration 5: train_loss 4.603305339813232
Iteration 6: train_loss 4.718832969665527
Iteration 7: train_loss 4.6203155517578125
Iteration 8: train_loss 4.628659248352051
Iteration 9: train_loss 4.731652736663818
Iteration 10: train_loss 4.800656318664551
Iteration 11: train_loss 4.683774471282959
Iteration 12: train_loss 4.580719947814941
Iteration 13: train_loss 4.73223352432251
Iteration 14: train_loss 4.704963207244873
Iteration 15: train_loss 4.7554731369018555
Iteration 16: train_loss 4.664150714874268
Iteration 17: train_loss 4.58488655090332
Iteration 18: train_loss 4.675802707672119
Iteration 19: train_loss 4.569242477416992
Iteration 20: train_loss 4.579153060913086
Iteration 21: train_loss 4.751152038574219
Iteration 22: train_loss 4.69843864440918
Iteration 23: train_loss 4.661836624145508
Iteration 24: train_loss 4.798157691955566
Iteration 25: train_loss 4.608360290527344
Iteration 26: train_loss 4.788586616516113
Iteration 27: train_loss 4.65179443359375
Iteration 28: train_loss 4.740732669830322
Iteration 29: train_loss 4.568589687347412
Iteration 30: train_loss 4.674840927124023
Iteration 31: train_loss 4.652966499328613
Iteration 32: train_loss 4.499760150909424
Iteration 33: train_loss 4.630273342132568
Iteration 34: train_loss 4.581778526306152
Iteration 35: train_loss 4.557382583618164
Iteration 36: train_loss 4.5801239013671875
Iteration 37: train_loss 4.689094543457031
Iteration 38: train_loss 4.550172805786133
Iteration 39: train_loss 4.667120456695557
Iteration 40: train_loss 4.557948112487793
Epoch 33: train_avg_loss 4.66834282875061 eval_avg_acc: 0.1525202280222426 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:12:49] [32mIntermediate result: 0.1525202280222426  (Index 32)[0m
================Epoch: 34================
Iteration 1: train_loss 4.704298973083496
Iteration 2: train_loss 4.599991321563721
Iteration 3: train_loss 4.755860328674316
Iteration 4: train_loss 4.613770961761475
Iteration 5: train_loss 4.738276481628418
Iteration 6: train_loss 4.630315780639648
Iteration 7: train_loss 4.39060640335083
Iteration 8: train_loss 4.53517484664917
Iteration 9: train_loss 4.741669178009033
Iteration 10: train_loss 4.612430572509766
Iteration 11: train_loss 4.6377997398376465
Iteration 12: train_loss 4.6330156326293945
Iteration 13: train_loss 4.672942638397217
Iteration 14: train_loss 4.6166582107543945
Iteration 15: train_loss 4.6683669090271
Iteration 16: train_loss 4.717583656311035
Iteration 17: train_loss 4.506798267364502
Iteration 18: train_loss 4.630980491638184
Iteration 19: train_loss 4.4604668617248535
Iteration 20: train_loss 4.817417144775391
Iteration 21: train_loss 4.564315319061279
Iteration 22: train_loss 4.737685203552246
Iteration 23: train_loss 4.689260959625244
Iteration 24: train_loss 4.583720684051514
Iteration 25: train_loss 4.557556629180908
Iteration 26: train_loss 4.662814617156982
Iteration 27: train_loss 4.588312149047852
Iteration 28: train_loss 4.6091132164001465
Iteration 29: train_loss 4.568370819091797
Iteration 30: train_loss 4.553236484527588
Iteration 31: train_loss 4.593210697174072
Iteration 32: train_loss 4.537067413330078
Iteration 33: train_loss 4.71211576461792
Iteration 34: train_loss 4.694018840789795
Iteration 35: train_loss 4.552804946899414
Iteration 36: train_loss 4.600333213806152
Iteration 37: train_loss 4.604804039001465
Iteration 38: train_loss 4.530154705047607
Iteration 39: train_loss 4.777672290802002
Iteration 40: train_loss 4.628332614898682
Epoch 34: train_avg_loss 4.625733125209808 eval_avg_acc: 0.15693046924223966 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:13:02] [32mIntermediate result: 0.15693046924223966  (Index 33)[0m
================Epoch: 35================
Iteration 1: train_loss 4.442365646362305
Iteration 2: train_loss 4.566957473754883
Iteration 3: train_loss 4.466104984283447
Iteration 4: train_loss 4.650265216827393
Iteration 5: train_loss 4.649515628814697
Iteration 6: train_loss 4.672370433807373
Iteration 7: train_loss 4.4191508293151855
Iteration 8: train_loss 4.582325458526611
Iteration 9: train_loss 4.521911144256592
Iteration 10: train_loss 4.623500823974609
Iteration 11: train_loss 4.696082592010498
Iteration 12: train_loss 4.520669937133789
Iteration 13: train_loss 4.56425666809082
Iteration 14: train_loss 4.567845821380615
Iteration 15: train_loss 4.6146345138549805
Iteration 16: train_loss 4.531455993652344
Iteration 17: train_loss 4.3158040046691895
Iteration 18: train_loss 4.500772953033447
Iteration 19: train_loss 4.459401607513428
Iteration 20: train_loss 4.51492166519165
Iteration 21: train_loss 4.4958648681640625
Iteration 22: train_loss 4.528133869171143
Iteration 23: train_loss 4.49981689453125
Iteration 24: train_loss 4.630165100097656
Iteration 25: train_loss 4.449181079864502
Iteration 26: train_loss 4.648777008056641
Iteration 27: train_loss 4.412041664123535
Iteration 28: train_loss 4.585358142852783
Iteration 29: train_loss 4.574026584625244
Iteration 30: train_loss 4.531134605407715
Iteration 31: train_loss 4.413604736328125
Iteration 32: train_loss 4.552097797393799
Iteration 33: train_loss 4.482020378112793
Iteration 34: train_loss 4.546544551849365
Iteration 35: train_loss 4.465640544891357
Iteration 36: train_loss 4.5432233810424805
Iteration 37: train_loss 4.603143215179443
Iteration 38: train_loss 4.530735015869141
Iteration 39: train_loss 4.583707809448242
Iteration 40: train_loss 4.389400005340576
Epoch 35: train_avg_loss 4.533623266220093 eval_avg_acc: 0.15937675636138915 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:13:14] [32mIntermediate result: 0.15937675636138915  (Index 34)[0m
================Epoch: 36================
Iteration 1: train_loss 4.642693996429443
Iteration 2: train_loss 4.709756374359131
Iteration 3: train_loss 4.558953762054443
Iteration 4: train_loss 4.624526500701904
Iteration 5: train_loss 4.521564483642578
Iteration 6: train_loss 4.46135950088501
Iteration 7: train_loss 4.441409111022949
Iteration 8: train_loss 4.545863628387451
Iteration 9: train_loss 4.735957622528076
Iteration 10: train_loss 4.372406959533691
Iteration 11: train_loss 4.53108549118042
Iteration 12: train_loss 4.563645362854004
Iteration 13: train_loss 4.543501377105713
Iteration 14: train_loss 4.495733737945557
Iteration 15: train_loss 4.485223770141602
Iteration 16: train_loss 4.5555739402771
Iteration 17: train_loss 4.407768249511719
Iteration 18: train_loss 4.581485271453857
Iteration 19: train_loss 4.454926013946533
Iteration 20: train_loss 4.609855651855469
Iteration 21: train_loss 4.519148826599121
Iteration 22: train_loss 4.393160343170166
Iteration 23: train_loss 4.529605865478516
Iteration 24: train_loss 4.616708278656006
Iteration 25: train_loss 4.461459636688232
Iteration 26: train_loss 4.432793140411377
Iteration 27: train_loss 4.365869522094727
Iteration 28: train_loss 4.400923252105713
Iteration 29: train_loss 4.400240898132324
Iteration 30: train_loss 4.53881311416626
Iteration 31: train_loss 4.47724723815918
Iteration 32: train_loss 4.461180210113525
Iteration 33: train_loss 4.515738487243652
Iteration 34: train_loss 4.524131774902344
Iteration 35: train_loss 4.51559591293335
Iteration 36: train_loss 4.629931449890137
Iteration 37: train_loss 4.5434250831604
Iteration 38: train_loss 4.545882701873779
Iteration 39: train_loss 4.564004898071289
Iteration 40: train_loss 4.403763294219971
Epoch 36: train_avg_loss 4.517072868347168 eval_avg_acc: 0.16447755319082372 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:13:26] [32mIntermediate result: 0.16447755319082372  (Index 35)[0m
================Epoch: 37================
Iteration 1: train_loss 4.394983768463135
Iteration 2: train_loss 4.499227046966553
Iteration 3: train_loss 4.512298583984375
Iteration 4: train_loss 4.471235752105713
Iteration 5: train_loss 4.468050956726074
Iteration 6: train_loss 4.361109733581543
Iteration 7: train_loss 4.485084056854248
Iteration 8: train_loss 4.540404796600342
Iteration 9: train_loss 4.494291305541992
Iteration 10: train_loss 4.268196105957031
Iteration 11: train_loss 4.380146503448486
Iteration 12: train_loss 4.389556407928467
Iteration 13: train_loss 4.506820201873779
Iteration 14: train_loss 4.439542293548584
Iteration 15: train_loss 4.424892902374268
Iteration 16: train_loss 4.439639568328857
Iteration 17: train_loss 4.486215591430664
Iteration 18: train_loss 4.4824347496032715
Iteration 19: train_loss 4.57038688659668
Iteration 20: train_loss 4.34942102432251
Iteration 21: train_loss 4.508818626403809
Iteration 22: train_loss 4.391099452972412
Iteration 23: train_loss 4.313701152801514
Iteration 24: train_loss 4.590612411499023
Iteration 25: train_loss 4.331085681915283
Iteration 26: train_loss 4.391726970672607
Iteration 27: train_loss 4.49260139465332
Iteration 28: train_loss 4.502641677856445
Iteration 29: train_loss 4.444135665893555
Iteration 30: train_loss 4.371886730194092
Iteration 31: train_loss 4.405582427978516
Iteration 32: train_loss 4.505267143249512
Iteration 33: train_loss 4.523480415344238
Iteration 34: train_loss 4.470973491668701
Iteration 35: train_loss 4.3667521476745605
Iteration 36: train_loss 4.453173637390137
Iteration 37: train_loss 4.500783920288086
Iteration 38: train_loss 4.3799333572387695
Iteration 39: train_loss 4.495357036590576
Iteration 40: train_loss 4.039957523345947
Epoch 37: train_avg_loss 4.436087727546692 eval_avg_acc: 0.1699315957362625 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:13:39] [32mIntermediate result: 0.1699315957362625  (Index 36)[0m
================Epoch: 38================
Iteration 1: train_loss 4.494582176208496
Iteration 2: train_loss 4.52302360534668
Iteration 3: train_loss 4.465103626251221
Iteration 4: train_loss 4.551042079925537
Iteration 5: train_loss 4.527092456817627
Iteration 6: train_loss 4.499197959899902
Iteration 7: train_loss 4.482641220092773
Iteration 8: train_loss 4.309541702270508
Iteration 9: train_loss 4.398622989654541
Iteration 10: train_loss 4.344531536102295
Iteration 11: train_loss 4.500896453857422
Iteration 12: train_loss 4.312166690826416
Iteration 13: train_loss 4.387249946594238
Iteration 14: train_loss 4.461239337921143
Iteration 15: train_loss 4.377769470214844
Iteration 16: train_loss 4.321418285369873
Iteration 17: train_loss 4.406830310821533
Iteration 18: train_loss 4.342766284942627
Iteration 19: train_loss 4.261195659637451
Iteration 20: train_loss 4.363725662231445
Iteration 21: train_loss 4.425736904144287
Iteration 22: train_loss 4.446110725402832
Iteration 23: train_loss 4.312742710113525
Iteration 24: train_loss 4.5142412185668945
Iteration 25: train_loss 4.304760932922363
Iteration 26: train_loss 4.587798595428467
Iteration 27: train_loss 4.416545391082764
Iteration 28: train_loss 4.563085556030273
Iteration 29: train_loss 4.344869136810303
Iteration 30: train_loss 4.346966743469238
Iteration 31: train_loss 4.381552219390869
Iteration 32: train_loss 4.406803607940674
Iteration 33: train_loss 4.4736199378967285
Iteration 34: train_loss 4.6237711906433105
Iteration 35: train_loss 4.530167579650879
Iteration 36: train_loss 4.395079135894775
Iteration 37: train_loss 4.487308025360107
Iteration 38: train_loss 4.330814361572266
Iteration 39: train_loss 4.47883415222168
Iteration 40: train_loss 4.272546291351318
Epoch 38: train_avg_loss 4.424349796772003 eval_avg_acc: 0.16735566522305076 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:13:51] [32mIntermediate result: 0.16735566522305076  (Index 37)[0m
================Epoch: 39================
Iteration 1: train_loss 4.36221170425415
Iteration 2: train_loss 4.470423221588135
Iteration 3: train_loss 4.45146369934082
Iteration 4: train_loss 4.607513904571533
Iteration 5: train_loss 4.5888352394104
Iteration 6: train_loss 4.463686466217041
Iteration 7: train_loss 4.285520553588867
Iteration 8: train_loss 4.292699337005615
Iteration 9: train_loss 4.403187274932861
Iteration 10: train_loss 4.459195613861084
Iteration 11: train_loss 4.380608081817627
Iteration 12: train_loss 4.369394302368164
Iteration 13: train_loss 4.422378063201904
Iteration 14: train_loss 4.278739929199219
Iteration 15: train_loss 4.43541955947876
Iteration 16: train_loss 4.403081893920898
Iteration 17: train_loss 4.315471172332764
Iteration 18: train_loss 4.445995330810547
Iteration 19: train_loss 4.315433979034424
Iteration 20: train_loss 4.339502811431885
Iteration 21: train_loss 4.3581743240356445
Iteration 22: train_loss 4.349428653717041
Iteration 23: train_loss 4.288384437561035
Iteration 24: train_loss 4.401042461395264
Iteration 25: train_loss 4.210947513580322
Iteration 26: train_loss 4.375178337097168
Iteration 27: train_loss 4.333100318908691
Iteration 28: train_loss 4.352221488952637
Iteration 29: train_loss 4.2159295082092285
Iteration 30: train_loss 4.420250415802002
Iteration 31: train_loss 4.358802795410156
Iteration 32: train_loss 4.3050007820129395
Iteration 33: train_loss 4.483848571777344
Iteration 34: train_loss 4.311765193939209
Iteration 35: train_loss 4.526729583740234
Iteration 36: train_loss 4.453173637390137
Iteration 37: train_loss 4.379985332489014
Iteration 38: train_loss 4.293221473693848
Iteration 39: train_loss 4.535966396331787
Iteration 40: train_loss 4.271312713623047
Epoch 39: train_avg_loss 4.3828806519508365 eval_avg_acc: 0.17520720883305138 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:14:04] [32mIntermediate result: 0.17520720883305138  (Index 38)[0m
================Epoch: 40================
Iteration 1: train_loss 4.357515811920166
Iteration 2: train_loss 4.417482852935791
Iteration 3: train_loss 4.466042995452881
Iteration 4: train_loss 4.367707252502441
Iteration 5: train_loss 4.462220191955566
Iteration 6: train_loss 4.335021495819092
Iteration 7: train_loss 4.422842025756836
Iteration 8: train_loss 4.433587551116943
Iteration 9: train_loss 4.427374839782715
Iteration 10: train_loss 4.461183547973633
Iteration 11: train_loss 4.395707607269287
Iteration 12: train_loss 4.343797206878662
Iteration 13: train_loss 4.26748514175415
Iteration 14: train_loss 4.339611053466797
Iteration 15: train_loss 4.311298370361328
Iteration 16: train_loss 4.293288230895996
Iteration 17: train_loss 4.277251720428467
Iteration 18: train_loss 4.372410297393799
Iteration 19: train_loss 4.4045562744140625
Iteration 20: train_loss 4.230797290802002
Iteration 21: train_loss 4.194745063781738
Iteration 22: train_loss 4.3543219566345215
Iteration 23: train_loss 4.128050804138184
Iteration 24: train_loss 4.476708889007568
Iteration 25: train_loss 4.29965877532959
Iteration 26: train_loss 4.330513000488281
Iteration 27: train_loss 4.342434406280518
Iteration 28: train_loss 4.381824970245361
Iteration 29: train_loss 4.537907600402832
Iteration 30: train_loss 4.434643745422363
Iteration 31: train_loss 4.340941429138184
Iteration 32: train_loss 4.381834983825684
Iteration 33: train_loss 4.295872688293457
Iteration 34: train_loss 4.351688385009766
Iteration 35: train_loss 4.299527168273926
Iteration 36: train_loss 4.508339881896973
Iteration 37: train_loss 4.307987689971924
Iteration 38: train_loss 4.339221477508545
Iteration 39: train_loss 4.527445316314697
Iteration 40: train_loss 4.448197841644287
Epoch 40: train_avg_loss 4.366726195812225 eval_avg_acc: 0.17165820352414546 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:14:16] [32mIntermediate result: 0.17165820352414546  (Index 39)[0m
================Epoch: 41================
Iteration 1: train_loss 4.469594955444336
Iteration 2: train_loss 4.3615498542785645
Iteration 3: train_loss 4.379446983337402
Iteration 4: train_loss 4.41879940032959
Iteration 5: train_loss 4.480232238769531
Iteration 6: train_loss 4.349818229675293
Iteration 7: train_loss 4.430863857269287
Iteration 8: train_loss 4.168454170227051
Iteration 9: train_loss 4.350419521331787
Iteration 10: train_loss 4.304869174957275
Iteration 11: train_loss 4.46738338470459
Iteration 12: train_loss 4.2819294929504395
Iteration 13: train_loss 4.346022605895996
Iteration 14: train_loss 4.339408874511719
Iteration 15: train_loss 4.386612415313721
Iteration 16: train_loss 4.3604416847229
Iteration 17: train_loss 4.304038047790527
Iteration 18: train_loss 4.3150105476379395
Iteration 19: train_loss 4.217759132385254
Iteration 20: train_loss 4.332409381866455
Iteration 21: train_loss 4.265257358551025
Iteration 22: train_loss 4.182688236236572
Iteration 23: train_loss 4.240916728973389
Iteration 24: train_loss 4.379358768463135
Iteration 25: train_loss 4.253631114959717
Iteration 26: train_loss 4.400187015533447
Iteration 27: train_loss 4.297952651977539
Iteration 28: train_loss 4.3159027099609375
Iteration 29: train_loss 4.140856742858887
Iteration 30: train_loss 4.530590057373047
Iteration 31: train_loss 4.242862224578857
Iteration 32: train_loss 4.347527980804443
Iteration 33: train_loss 4.368832111358643
Iteration 34: train_loss 4.492209434509277
Iteration 35: train_loss 4.276864051818848
Iteration 36: train_loss 4.534310340881348
Iteration 37: train_loss 4.300076961517334
Iteration 38: train_loss 4.523937225341797
Iteration 39: train_loss 4.310915946960449
Iteration 40: train_loss 4.620511531829834
Epoch 41: train_avg_loss 4.352261328697205 eval_avg_acc: 0.17844230243668135 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:14:29] [32mIntermediate result: 0.17844230243668135  (Index 40)[0m
================Epoch: 42================
Iteration 1: train_loss 4.292765140533447
Iteration 2: train_loss 4.256009101867676
Iteration 3: train_loss 4.313919544219971
Iteration 4: train_loss 4.273351192474365
Iteration 5: train_loss 4.216498851776123
Iteration 6: train_loss 4.327875137329102
Iteration 7: train_loss 4.238613128662109
Iteration 8: train_loss 4.309896469116211
Iteration 9: train_loss 4.300129413604736
Iteration 10: train_loss 4.268444538116455
Iteration 11: train_loss 4.209255695343018
Iteration 12: train_loss 4.238440036773682
Iteration 13: train_loss 4.209580898284912
Iteration 14: train_loss 4.302159786224365
Iteration 15: train_loss 4.081018924713135
Iteration 16: train_loss 4.39788818359375
Iteration 17: train_loss 4.2528533935546875
Iteration 18: train_loss 4.347987174987793
Iteration 19: train_loss 4.3277459144592285
Iteration 20: train_loss 4.381105422973633
Iteration 21: train_loss 4.39984655380249
Iteration 22: train_loss 4.258387565612793
Iteration 23: train_loss 4.358587741851807
Iteration 24: train_loss 4.182205677032471
Iteration 25: train_loss 4.440003395080566
Iteration 26: train_loss 4.464328765869141
Iteration 27: train_loss 4.437070369720459
Iteration 28: train_loss 4.21605110168457
Iteration 29: train_loss 4.3046464920043945
Iteration 30: train_loss 4.140481948852539
Iteration 31: train_loss 4.117852687835693
Iteration 32: train_loss 4.24019193649292
Iteration 33: train_loss 4.352465629577637
Iteration 34: train_loss 4.28890323638916
Iteration 35: train_loss 4.447295665740967
Iteration 36: train_loss 4.367916107177734
Iteration 37: train_loss 4.356119155883789
Iteration 38: train_loss 4.245494365692139
Iteration 39: train_loss 4.32520055770874
Iteration 40: train_loss 3.947026491165161
Epoch 42: train_avg_loss 4.28589033484459 eval_avg_acc: 0.18327511700018237 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:14:42] [32mIntermediate result: 0.18327511700018237  (Index 41)[0m
================Epoch: 43================
Iteration 1: train_loss 4.283745288848877
Iteration 2: train_loss 4.25860595703125
Iteration 3: train_loss 4.214882850646973
Iteration 4: train_loss 4.308800220489502
Iteration 5: train_loss 3.960068464279175
Iteration 6: train_loss 4.192824840545654
Iteration 7: train_loss 4.225339889526367
Iteration 8: train_loss 4.288431644439697
Iteration 9: train_loss 4.278077602386475
Iteration 10: train_loss 4.152323246002197
Iteration 11: train_loss 4.130477428436279
Iteration 12: train_loss 4.248492240905762
Iteration 13: train_loss 4.254273891448975
Iteration 14: train_loss 4.169640064239502
Iteration 15: train_loss 4.114256858825684
Iteration 16: train_loss 4.280432224273682
Iteration 17: train_loss 4.122568607330322
Iteration 18: train_loss 4.167263507843018
Iteration 19: train_loss 4.232674598693848
Iteration 20: train_loss 4.202784538269043
Iteration 21: train_loss 4.335470199584961
Iteration 22: train_loss 4.270481586456299
Iteration 23: train_loss 4.215540409088135
Iteration 24: train_loss 4.353039741516113
Iteration 25: train_loss 4.248533248901367
Iteration 26: train_loss 4.309113025665283
Iteration 27: train_loss 4.2315826416015625
Iteration 28: train_loss 4.235208988189697
Iteration 29: train_loss 4.293747901916504
Iteration 30: train_loss 4.163409233093262
Iteration 31: train_loss 4.352841377258301
Iteration 32: train_loss 4.316653251647949
Iteration 33: train_loss 4.232701301574707
Iteration 34: train_loss 4.371363639831543
Iteration 35: train_loss 4.23543643951416
Iteration 36: train_loss 4.073312759399414
Iteration 37: train_loss 4.357397556304932
Iteration 38: train_loss 4.226800918579102
Iteration 39: train_loss 4.462958335876465
Iteration 40: train_loss 3.8380608558654785
Epoch 43: train_avg_loss 4.230240434408188 eval_avg_acc: 0.1729159128480845 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:14:54] [32mIntermediate result: 0.1729159128480845  (Index 42)[0m
================Epoch: 44================
Iteration 1: train_loss 4.274169921875
Iteration 2: train_loss 4.209146499633789
Iteration 3: train_loss 4.148529052734375
Iteration 4: train_loss 4.329322814941406
Iteration 5: train_loss 4.220506191253662
Iteration 6: train_loss 4.195040702819824
Iteration 7: train_loss 4.153071880340576
Iteration 8: train_loss 4.338212013244629
Iteration 9: train_loss 4.2453203201293945
Iteration 10: train_loss 4.26529598236084
Iteration 11: train_loss 4.12152624130249
Iteration 12: train_loss 4.269278526306152
Iteration 13: train_loss 4.2250566482543945
Iteration 14: train_loss 4.1474738121032715
Iteration 15: train_loss 4.158954620361328
Iteration 16: train_loss 4.295026779174805
Iteration 17: train_loss 4.2000250816345215
Iteration 18: train_loss 4.247233867645264
Iteration 19: train_loss 4.225096225738525
Iteration 20: train_loss 4.230520725250244
Iteration 21: train_loss 4.14918851852417
Iteration 22: train_loss 4.341545581817627
Iteration 23: train_loss 4.151411056518555
Iteration 24: train_loss 4.115817546844482
Iteration 25: train_loss 4.054327964782715
Iteration 26: train_loss 4.323736190795898
Iteration 27: train_loss 4.330223083496094
Iteration 28: train_loss 4.238145351409912
Iteration 29: train_loss 3.9422242641448975
Iteration 30: train_loss 4.223937511444092
Iteration 31: train_loss 4.173968315124512
Iteration 32: train_loss 4.178244590759277
Iteration 33: train_loss 4.302511692047119
Iteration 34: train_loss 4.232485771179199
Iteration 35: train_loss 4.217422008514404
Iteration 36: train_loss 4.300665378570557
Iteration 37: train_loss 4.288044452667236
Iteration 38: train_loss 4.196193218231201
Iteration 39: train_loss 4.336357116699219
Iteration 40: train_loss 4.267743110656738
Epoch 44: train_avg_loss 4.22157501578331 eval_avg_acc: 0.18702546286251803 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:15:07] [32mIntermediate result: 0.18702546286251803  (Index 43)[0m
================Epoch: 45================
Iteration 1: train_loss 4.333554267883301
Iteration 2: train_loss 4.267731189727783
Iteration 3: train_loss 4.0415778160095215
Iteration 4: train_loss 4.260590553283691
Iteration 5: train_loss 4.172651767730713
Iteration 6: train_loss 4.0406999588012695
Iteration 7: train_loss 4.258877277374268
Iteration 8: train_loss 4.2182230949401855
Iteration 9: train_loss 4.379589557647705
Iteration 10: train_loss 4.191501140594482
Iteration 11: train_loss 4.261014938354492
Iteration 12: train_loss 4.128870964050293
Iteration 13: train_loss 4.075506210327148
Iteration 14: train_loss 4.062269687652588
Iteration 15: train_loss 4.121705532073975
Iteration 16: train_loss 4.220976829528809
Iteration 17: train_loss 4.220696926116943
Iteration 18: train_loss 4.078009605407715
Iteration 19: train_loss 4.229222297668457
Iteration 20: train_loss 4.259223461151123
Iteration 21: train_loss 4.267500877380371
Iteration 22: train_loss 4.239340782165527
Iteration 23: train_loss 4.180010795593262
Iteration 24: train_loss 4.194064140319824
Iteration 25: train_loss 4.147099494934082
Iteration 26: train_loss 4.033007621765137
Iteration 27: train_loss 4.173328399658203
Iteration 28: train_loss 4.113372325897217
Iteration 29: train_loss 4.220382213592529
Iteration 30: train_loss 4.188477039337158
Iteration 31: train_loss 4.280858516693115
Iteration 32: train_loss 4.3257155418396
Iteration 33: train_loss 4.309665203094482
Iteration 34: train_loss 4.211576461791992
Iteration 35: train_loss 4.277769565582275
Iteration 36: train_loss 4.225632667541504
Iteration 37: train_loss 4.080178260803223
Iteration 38: train_loss 4.157227039337158
Iteration 39: train_loss 4.361970901489258
Iteration 40: train_loss 4.519032955169678
Epoch 45: train_avg_loss 4.208217597007751 eval_avg_acc: 0.16907145713298583 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:15:19] [32mIntermediate result: 0.16907145713298583  (Index 44)[0m
================Epoch: 46================
Iteration 1: train_loss 4.338965892791748
Iteration 2: train_loss 4.258121490478516
Iteration 3: train_loss 4.161131858825684
Iteration 4: train_loss 4.117741584777832
Iteration 5: train_loss 4.419457912445068
Iteration 6: train_loss 4.126965045928955
Iteration 7: train_loss 4.231928825378418
Iteration 8: train_loss 4.1285810470581055
Iteration 9: train_loss 4.092658042907715
Iteration 10: train_loss 4.229261875152588
Iteration 11: train_loss 4.312418460845947
Iteration 12: train_loss 4.152711391448975
Iteration 13: train_loss 4.232053279876709
Iteration 14: train_loss 3.9460811614990234
Iteration 15: train_loss 4.233316421508789
Iteration 16: train_loss 4.034062385559082
Iteration 17: train_loss 4.172667980194092
Iteration 18: train_loss 4.217952728271484
Iteration 19: train_loss 4.2756195068359375
Iteration 20: train_loss 4.3342437744140625
Iteration 21: train_loss 4.1277971267700195
Iteration 22: train_loss 4.192893028259277
Iteration 23: train_loss 4.265334606170654
Iteration 24: train_loss 4.208402633666992
Iteration 25: train_loss 4.205679893493652
Iteration 26: train_loss 4.264439582824707
Iteration 27: train_loss 4.073143005371094
Iteration 28: train_loss 4.256619453430176
Iteration 29: train_loss 4.173795700073242
Iteration 30: train_loss 4.123551845550537
Iteration 31: train_loss 4.0136942863464355
Iteration 32: train_loss 4.134494304656982
Iteration 33: train_loss 4.142986297607422
Iteration 34: train_loss 4.218424320220947
Iteration 35: train_loss 4.281656265258789
Iteration 36: train_loss 4.167852401733398
Iteration 37: train_loss 4.131505966186523
Iteration 38: train_loss 4.138034820556641
Iteration 39: train_loss 4.134164810180664
Iteration 40: train_loss 4.094525337219238
Epoch 46: train_avg_loss 4.184123408794403 eval_avg_acc: 0.19037797476999824 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:15:32] [32mIntermediate result: 0.19037797476999824  (Index 45)[0m
================Epoch: 47================
Iteration 1: train_loss 4.103143692016602
Iteration 2: train_loss 4.191328048706055
Iteration 3: train_loss 4.214267253875732
Iteration 4: train_loss 4.154138088226318
Iteration 5: train_loss 4.174010753631592
Iteration 6: train_loss 3.9238388538360596
Iteration 7: train_loss 4.193198204040527
Iteration 8: train_loss 4.097883701324463
Iteration 9: train_loss 4.176576137542725
Iteration 10: train_loss 4.207190036773682
Iteration 11: train_loss 4.18638277053833
Iteration 12: train_loss 3.9928770065307617
Iteration 13: train_loss 3.9750077724456787
Iteration 14: train_loss 3.935236930847168
Iteration 15: train_loss 4.256728172302246
Iteration 16: train_loss 4.162608623504639
Iteration 17: train_loss 4.10452938079834
Iteration 18: train_loss 4.102624416351318
Iteration 19: train_loss 4.140050888061523
Iteration 20: train_loss 4.107067108154297
Iteration 21: train_loss 4.0140461921691895
Iteration 22: train_loss 4.027429103851318
Iteration 23: train_loss 4.0941033363342285
Iteration 24: train_loss 4.033637046813965
Iteration 25: train_loss 4.096996784210205
Iteration 26: train_loss 3.961167812347412
Iteration 27: train_loss 4.189437389373779
Iteration 28: train_loss 4.284522533416748
Iteration 29: train_loss 4.27380895614624
Iteration 30: train_loss 4.074663162231445
Iteration 31: train_loss 4.070958614349365
Iteration 32: train_loss 4.076995372772217
Iteration 33: train_loss 4.261186599731445
Iteration 34: train_loss 4.079859256744385
Iteration 35: train_loss 4.272012710571289
Iteration 36: train_loss 4.147975921630859
Iteration 37: train_loss 4.159102916717529
Iteration 38: train_loss 4.06290864944458
Iteration 39: train_loss 4.32275390625
Iteration 40: train_loss 4.437158107757568
Epoch 47: train_avg_loss 4.1334853053092955 eval_avg_acc: 0.1904658378870568 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:15:44] [32mIntermediate result: 0.1904658378870568  (Index 46)[0m
================Epoch: 48================
Iteration 1: train_loss 4.128617763519287
Iteration 2: train_loss 4.1971564292907715
Iteration 3: train_loss 3.947094678878784
Iteration 4: train_loss 4.147653102874756
Iteration 5: train_loss 4.065459728240967
Iteration 6: train_loss 4.087265968322754
Iteration 7: train_loss 4.304831504821777
Iteration 8: train_loss 3.9637081623077393
Iteration 9: train_loss 3.97816801071167
Iteration 10: train_loss 4.160719394683838
Iteration 11: train_loss 4.042603492736816
Iteration 12: train_loss 4.05793571472168
Iteration 13: train_loss 4.121012210845947
Iteration 14: train_loss 4.067584991455078
Iteration 15: train_loss 4.013418197631836
Iteration 16: train_loss 3.9861271381378174
Iteration 17: train_loss 4.164522171020508
Iteration 18: train_loss 3.9805986881256104
Iteration 19: train_loss 4.1546549797058105
Iteration 20: train_loss 4.084110260009766
Iteration 21: train_loss 4.0291666984558105
Iteration 22: train_loss 3.9817557334899902
Iteration 23: train_loss 4.165180206298828
Iteration 24: train_loss 4.0912065505981445
Iteration 25: train_loss 4.10574197769165
Iteration 26: train_loss 4.253556728363037
Iteration 27: train_loss 4.224310874938965
Iteration 28: train_loss 4.165031433105469
Iteration 29: train_loss 4.104186058044434
Iteration 30: train_loss 4.149521350860596
Iteration 31: train_loss 4.165213108062744
Iteration 32: train_loss 4.239732265472412
Iteration 33: train_loss 4.169804096221924
Iteration 34: train_loss 4.204891204833984
Iteration 35: train_loss 4.150722980499268
Iteration 36: train_loss 4.092801570892334
Iteration 37: train_loss 4.0993804931640625
Iteration 38: train_loss 4.137284278869629
Iteration 39: train_loss 4.385981559753418
Iteration 40: train_loss 4.550518035888672
Epoch 48: train_avg_loss 4.127980744838714 eval_avg_acc: 0.1813509938538634 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:15:57] [32mIntermediate result: 0.1813509938538634  (Index 47)[0m
================Epoch: 49================
Iteration 1: train_loss 4.291626930236816
Iteration 2: train_loss 4.11300802230835
Iteration 3: train_loss 4.119972229003906
Iteration 4: train_loss 4.12660026550293
Iteration 5: train_loss 4.078901767730713
Iteration 6: train_loss 4.0566253662109375
Iteration 7: train_loss 4.189694404602051
Iteration 8: train_loss 4.224954128265381
Iteration 9: train_loss 4.063798427581787
Iteration 10: train_loss 4.107890605926514
Iteration 11: train_loss 3.9736626148223877
Iteration 12: train_loss 3.8842527866363525
Iteration 13: train_loss 4.082062721252441
Iteration 14: train_loss 4.091707229614258
Iteration 15: train_loss 3.997462749481201
Iteration 16: train_loss 4.049587726593018
Iteration 17: train_loss 4.099612236022949
Iteration 18: train_loss 4.063012599945068
Iteration 19: train_loss 4.021603107452393
Iteration 20: train_loss 3.900648832321167
Iteration 21: train_loss 4.021834850311279
Iteration 22: train_loss 3.854437828063965
Iteration 23: train_loss 4.142395973205566
Iteration 24: train_loss 4.161668300628662
Iteration 25: train_loss 4.231888771057129
Iteration 26: train_loss 4.228823661804199
Iteration 27: train_loss 4.031520843505859
Iteration 28: train_loss 4.122490882873535
Iteration 29: train_loss 3.979363441467285
Iteration 30: train_loss 4.088289260864258
Iteration 31: train_loss 4.018161296844482
Iteration 32: train_loss 4.042967319488525
Iteration 33: train_loss 4.141845226287842
Iteration 34: train_loss 3.991551637649536
Iteration 35: train_loss 4.019879341125488
Iteration 36: train_loss 3.9974656105041504
Iteration 37: train_loss 4.0602521896362305
Iteration 38: train_loss 4.077613830566406
Iteration 39: train_loss 4.08620548248291
Iteration 40: train_loss 4.300398826599121
Epoch 49: train_avg_loss 4.078393483161927 eval_avg_acc: 0.19347776684761214 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:16:10] [32mIntermediate result: 0.19347776684761214  (Index 48)[0m
================Epoch: 50================
Iteration 1: train_loss 4.059478759765625
Iteration 2: train_loss 3.939718008041382
Iteration 3: train_loss 3.9472148418426514
Iteration 4: train_loss 3.923907518386841
Iteration 5: train_loss 4.053339004516602
Iteration 6: train_loss 4.121303558349609
Iteration 7: train_loss 4.0072126388549805
Iteration 8: train_loss 4.06251335144043
Iteration 9: train_loss 4.015639781951904
Iteration 10: train_loss 3.9520888328552246
Iteration 11: train_loss 3.8335187435150146
Iteration 12: train_loss 4.084107398986816
Iteration 13: train_loss 4.114616870880127
Iteration 14: train_loss 4.111747741699219
Iteration 15: train_loss 3.8890702724456787
Iteration 16: train_loss 4.043923854827881
Iteration 17: train_loss 4.0071821212768555
Iteration 18: train_loss 4.017054080963135
Iteration 19: train_loss 4.186917304992676
Iteration 20: train_loss 4.162868499755859
Iteration 21: train_loss 3.9971184730529785
Iteration 22: train_loss 3.964094400405884
Iteration 23: train_loss 4.089107513427734
Iteration 24: train_loss 4.122808933258057
Iteration 25: train_loss 4.12296199798584
Iteration 26: train_loss 4.038504600524902
Iteration 27: train_loss 4.220308303833008
Iteration 28: train_loss 4.243384838104248
Iteration 29: train_loss 3.9836344718933105
Iteration 30: train_loss 3.9768292903900146
Iteration 31: train_loss 4.140078544616699
Iteration 32: train_loss 3.9337847232818604
Iteration 33: train_loss 3.841986894607544
Iteration 34: train_loss 3.986410140991211
Iteration 35: train_loss 4.068304538726807
Iteration 36: train_loss 4.092893123626709
Iteration 37: train_loss 4.078178405761719
Iteration 38: train_loss 4.122611045837402
Iteration 39: train_loss 4.1888275146484375
Iteration 40: train_loss 4.1112470626831055
Epoch 50: train_avg_loss 4.04641245007515 eval_avg_acc: 0.19675140120133808 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:16:23] [32mIntermediate result: 0.19675140120133808  (Index 49)[0m
================Epoch: 51================
Iteration 1: train_loss 4.054370880126953
Iteration 2: train_loss 3.9151062965393066
Iteration 3: train_loss 4.139657020568848
Iteration 4: train_loss 4.036736488342285
Iteration 5: train_loss 4.020512104034424
Iteration 6: train_loss 3.8636906147003174
Iteration 7: train_loss 3.993617057800293
Iteration 8: train_loss 4.116440773010254
Iteration 9: train_loss 4.092094421386719
Iteration 10: train_loss 4.009382247924805
Iteration 11: train_loss 3.998157262802124
Iteration 12: train_loss 4.024281024932861
Iteration 13: train_loss 3.9407694339752197
Iteration 14: train_loss 4.059401035308838
Iteration 15: train_loss 4.10045862197876
Iteration 16: train_loss 4.084697246551514
Iteration 17: train_loss 4.0955352783203125
Iteration 18: train_loss 3.945794105529785
Iteration 19: train_loss 4.052584648132324
Iteration 20: train_loss 4.06765604019165
Iteration 21: train_loss 3.977348566055298
Iteration 22: train_loss 4.161279678344727
Iteration 23: train_loss 3.855386972427368
Iteration 24: train_loss 4.188338279724121
Iteration 25: train_loss 3.96208119392395
Iteration 26: train_loss 4.195940017700195
Iteration 27: train_loss 4.145630836486816
Iteration 28: train_loss 3.971503496170044
Iteration 29: train_loss 3.856708288192749
Iteration 30: train_loss 4.235630035400391
Iteration 31: train_loss 3.835134983062744
Iteration 32: train_loss 4.067989349365234
Iteration 33: train_loss 3.8753416538238525
Iteration 34: train_loss 4.025606632232666
Iteration 35: train_loss 4.0518035888671875
Iteration 36: train_loss 3.9010396003723145
Iteration 37: train_loss 3.9396629333496094
Iteration 38: train_loss 3.971827507019043
Iteration 39: train_loss 3.9521729946136475
Iteration 40: train_loss 3.8183109760284424
Epoch 51: train_avg_loss 4.0149920046329495 eval_avg_acc: 0.19477650179416628 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:16:35] [32mIntermediate result: 0.19477650179416628  (Index 50)[0m
================Epoch: 52================
Iteration 1: train_loss 3.997736692428589
Iteration 2: train_loss 3.96050763130188
Iteration 3: train_loss 3.845085859298706
Iteration 4: train_loss 3.9665396213531494
Iteration 5: train_loss 3.9598805904388428
Iteration 6: train_loss 4.046165466308594
Iteration 7: train_loss 3.8716888427734375
Iteration 8: train_loss 4.003450870513916
Iteration 9: train_loss 3.853100538253784
Iteration 10: train_loss 3.8179850578308105
Iteration 11: train_loss 3.9507737159729004
Iteration 12: train_loss 3.9946646690368652
Iteration 13: train_loss 3.8447701930999756
Iteration 14: train_loss 4.206533908843994
Iteration 15: train_loss 3.8886001110076904
Iteration 16: train_loss 3.8479201793670654
Iteration 17: train_loss 3.9284651279449463
Iteration 18: train_loss 4.0019001960754395
Iteration 19: train_loss 4.101968765258789
Iteration 20: train_loss 4.039931774139404
Iteration 21: train_loss 4.048946857452393
Iteration 22: train_loss 3.996487617492676
Iteration 23: train_loss 4.004115581512451
Iteration 24: train_loss 3.994385004043579
Iteration 25: train_loss 3.878436326980591
Iteration 26: train_loss 4.102820873260498
Iteration 27: train_loss 4.059938430786133
Iteration 28: train_loss 3.951038122177124
Iteration 29: train_loss 4.061582088470459
Iteration 30: train_loss 3.9523046016693115
Iteration 31: train_loss 4.11312198638916
Iteration 32: train_loss 3.8626439571380615
Iteration 33: train_loss 3.9642081260681152
Iteration 34: train_loss 3.967456817626953
Iteration 35: train_loss 3.9263482093811035
Iteration 36: train_loss 3.963073968887329
Iteration 37: train_loss 4.011181831359863
Iteration 38: train_loss 4.048998832702637
Iteration 39: train_loss 4.024702072143555
Iteration 40: train_loss 4.252807140350342
Epoch 52: train_avg_loss 3.9828067064285277 eval_avg_acc: 0.18826145807853828 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:16:47] [32mIntermediate result: 0.18826145807853828  (Index 51)[0m
================Epoch: 53================
Iteration 1: train_loss 3.790910243988037
Iteration 2: train_loss 4.035633087158203
Iteration 3: train_loss 3.9609901905059814
Iteration 4: train_loss 3.9309258460998535
Iteration 5: train_loss 3.949827194213867
Iteration 6: train_loss 3.8672659397125244
Iteration 7: train_loss 3.9830853939056396
Iteration 8: train_loss 4.013583183288574
Iteration 9: train_loss 3.894949436187744
Iteration 10: train_loss 3.968252182006836
Iteration 11: train_loss 3.9523422718048096
Iteration 12: train_loss 4.056402683258057
Iteration 13: train_loss 3.94563627243042
Iteration 14: train_loss 3.9742226600646973
Iteration 15: train_loss 4.022322177886963
Iteration 16: train_loss 4.015214443206787
Iteration 17: train_loss 4.101504325866699
Iteration 18: train_loss 4.1060285568237305
Iteration 19: train_loss 4.07048225402832
Iteration 20: train_loss 3.988731861114502
Iteration 21: train_loss 3.92521595954895
Iteration 22: train_loss 3.9619140625
Iteration 23: train_loss 3.8290743827819824
Iteration 24: train_loss 4.028990268707275
Iteration 25: train_loss 4.133473873138428
Iteration 26: train_loss 3.9808359146118164
Iteration 27: train_loss 3.953939437866211
Iteration 28: train_loss 4.030867576599121
Iteration 29: train_loss 3.932689666748047
Iteration 30: train_loss 3.9049124717712402
Iteration 31: train_loss 3.8679428100585938
Iteration 32: train_loss 4.033947467803955
Iteration 33: train_loss 3.8827884197235107
Iteration 34: train_loss 3.984571933746338
Iteration 35: train_loss 3.941969633102417
Iteration 36: train_loss 3.990035057067871
Iteration 37: train_loss 3.93245792388916
Iteration 38: train_loss 3.986124038696289
Iteration 39: train_loss 3.8755292892456055
Iteration 40: train_loss 4.379188060760498
Epoch 53: train_avg_loss 3.9796194612979887 eval_avg_acc: 0.1967194279309886 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:16:59] [32mIntermediate result: 0.1967194279309886  (Index 52)[0m
================Epoch: 54================
Iteration 1: train_loss 4.019892692565918
Iteration 2: train_loss 4.17573356628418
Iteration 3: train_loss 3.9752562046051025
Iteration 4: train_loss 3.9751768112182617
Iteration 5: train_loss 3.9248762130737305
Iteration 6: train_loss 3.944380760192871
Iteration 7: train_loss 4.074635028839111
Iteration 8: train_loss 3.9865212440490723
Iteration 9: train_loss 3.9857895374298096
Iteration 10: train_loss 4.020958423614502
Iteration 11: train_loss 4.007517337799072
Iteration 12: train_loss 3.998291254043579
Iteration 13: train_loss 4.018614292144775
Iteration 14: train_loss 4.107925891876221
Iteration 15: train_loss 3.8957557678222656
Iteration 16: train_loss 3.8221676349639893
Iteration 17: train_loss 3.854325532913208
Iteration 18: train_loss 3.8187429904937744
Iteration 19: train_loss 3.8425915241241455
Iteration 20: train_loss 3.7020862102508545
Iteration 21: train_loss 3.9116008281707764
Iteration 22: train_loss 4.026951789855957
Iteration 23: train_loss 3.893603801727295
Iteration 24: train_loss 3.9640676975250244
Iteration 25: train_loss 3.8867828845977783
Iteration 26: train_loss 3.873587131500244
Iteration 27: train_loss 3.87919282913208
Iteration 28: train_loss 3.840456008911133
Iteration 29: train_loss 4.062361717224121
Iteration 30: train_loss 3.8209409713745117
Iteration 31: train_loss 3.836890697479248
Iteration 32: train_loss 3.8920724391937256
Iteration 33: train_loss 3.8791351318359375
Iteration 34: train_loss 3.815199375152588
Iteration 35: train_loss 4.034029483795166
Iteration 36: train_loss 3.8894519805908203
Iteration 37: train_loss 4.062207221984863
Iteration 38: train_loss 3.9412214756011963
Iteration 39: train_loss 3.916856050491333
Iteration 40: train_loss 3.545750617980957
Epoch 54: train_avg_loss 3.92808997631073 eval_avg_acc: 0.19398271203614054 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:17:11] [32mIntermediate result: 0.19398271203614054  (Index 53)[0m
================Epoch: 55================
Iteration 1: train_loss 3.8926615715026855
Iteration 2: train_loss 3.9954893589019775
Iteration 3: train_loss 4.084509372711182
Iteration 4: train_loss 3.8113996982574463
Iteration 5: train_loss 3.956362724304199
Iteration 6: train_loss 3.824500560760498
Iteration 7: train_loss 3.888589859008789
Iteration 8: train_loss 3.869399070739746
Iteration 9: train_loss 3.8876216411590576
Iteration 10: train_loss 3.8811917304992676
Iteration 11: train_loss 3.8520617485046387
Iteration 12: train_loss 3.766634464263916
Iteration 13: train_loss 3.942186117172241
Iteration 14: train_loss 3.8617630004882812
Iteration 15: train_loss 3.813721179962158
Iteration 16: train_loss 3.9113988876342773
Iteration 17: train_loss 3.9578182697296143
Iteration 18: train_loss 3.8952765464782715
Iteration 19: train_loss 3.9042341709136963
Iteration 20: train_loss 3.976654052734375
Iteration 21: train_loss 3.978318929672241
Iteration 22: train_loss 3.8245601654052734
Iteration 23: train_loss 3.904902935028076
Iteration 24: train_loss 3.8701119422912598
Iteration 25: train_loss 3.7747697830200195
Iteration 26: train_loss 3.7566027641296387
Iteration 27: train_loss 3.960103988647461
Iteration 28: train_loss 3.9104115962982178
Iteration 29: train_loss 3.9731338024139404
Iteration 30: train_loss 3.9345812797546387
Iteration 31: train_loss 3.8984246253967285
Iteration 32: train_loss 3.8341221809387207
Iteration 33: train_loss 4.039764404296875
Iteration 34: train_loss 3.898271083831787
Iteration 35: train_loss 4.133857250213623
Iteration 36: train_loss 4.0236287117004395
Iteration 37: train_loss 4.008812427520752
Iteration 38: train_loss 4.145021438598633
Iteration 39: train_loss 3.973310947418213
Iteration 40: train_loss 4.142168045043945
Epoch 55: train_avg_loss 3.92395880818367 eval_avg_acc: 0.19543137254738385 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:17:23] [32mIntermediate result: 0.19543137254738385  (Index 54)[0m
================Epoch: 56================
Iteration 1: train_loss 3.9530978202819824
Iteration 2: train_loss 4.002991676330566
Iteration 3: train_loss 3.8633017539978027
Iteration 4: train_loss 3.8967270851135254
Iteration 5: train_loss 3.9254841804504395
Iteration 6: train_loss 3.9730470180511475
Iteration 7: train_loss 3.7776167392730713
Iteration 8: train_loss 3.993065118789673
Iteration 9: train_loss 3.8646485805511475
Iteration 10: train_loss 3.9954347610473633
Iteration 11: train_loss 3.939181327819824
Iteration 12: train_loss 3.6777241230010986
Iteration 13: train_loss 3.903791666030884
Iteration 14: train_loss 3.801344394683838
Iteration 15: train_loss 3.8347859382629395
Iteration 16: train_loss 3.8376517295837402
Iteration 17: train_loss 3.9146294593811035
Iteration 18: train_loss 3.9002039432525635
Iteration 19: train_loss 3.8439390659332275
Iteration 20: train_loss 3.8982839584350586
Iteration 21: train_loss 3.8209228515625
Iteration 22: train_loss 3.699348211288452
Iteration 23: train_loss 3.915492296218872
Iteration 24: train_loss 3.898818016052246
Iteration 25: train_loss 3.8613972663879395
Iteration 26: train_loss 3.8398048877716064
Iteration 27: train_loss 3.8310112953186035
Iteration 28: train_loss 3.900603771209717
Iteration 29: train_loss 3.7266159057617188
Iteration 30: train_loss 3.8666534423828125
Iteration 31: train_loss 4.010920524597168
Iteration 32: train_loss 3.9365596771240234
Iteration 33: train_loss 3.7814719676971436
Iteration 34: train_loss 4.085883617401123
Iteration 35: train_loss 3.8182153701782227
Iteration 36: train_loss 3.8583567142486572
Iteration 37: train_loss 3.958876132965088
Iteration 38: train_loss 3.8014302253723145
Iteration 39: train_loss 3.90088152885437
Iteration 40: train_loss 3.465928077697754
Epoch 56: train_avg_loss 3.8694035530090334 eval_avg_acc: 0.19711056032328428 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:17:35] [32mIntermediate result: 0.19711056032328428  (Index 55)[0m
================Epoch: 57================
Iteration 1: train_loss 3.865633487701416
Iteration 2: train_loss 4.017792701721191
Iteration 3: train_loss 3.7944979667663574
Iteration 4: train_loss 3.937061309814453
Iteration 5: train_loss 3.857682466506958
Iteration 6: train_loss 3.832132339477539
Iteration 7: train_loss 3.8040974140167236
Iteration 8: train_loss 3.7801618576049805
Iteration 9: train_loss 3.7634048461914062
Iteration 10: train_loss 3.863363027572632
Iteration 11: train_loss 3.8143739700317383
Iteration 12: train_loss 3.76778244972229
Iteration 13: train_loss 4.019672870635986
Iteration 14: train_loss 3.7806556224823
Iteration 15: train_loss 3.884780168533325
Iteration 16: train_loss 3.9789416790008545
Iteration 17: train_loss 3.931823253631592
Iteration 18: train_loss 3.9155898094177246
Iteration 19: train_loss 3.8315250873565674
Iteration 20: train_loss 3.8431150913238525
Iteration 21: train_loss 4.0169677734375
Iteration 22: train_loss 3.9218664169311523
Iteration 23: train_loss 3.9185972213745117
Iteration 24: train_loss 3.8311967849731445
Iteration 25: train_loss 3.9782297611236572
Iteration 26: train_loss 4.055810928344727
Iteration 27: train_loss 3.842027187347412
Iteration 28: train_loss 3.7048473358154297
Iteration 29: train_loss 3.765916585922241
Iteration 30: train_loss 3.7094757556915283
Iteration 31: train_loss 3.683467149734497
Iteration 32: train_loss 3.856781005859375
Iteration 33: train_loss 3.822946071624756
Iteration 34: train_loss 3.8552348613739014
Iteration 35: train_loss 3.856682777404785
Iteration 36: train_loss 3.739711284637451
Iteration 37: train_loss 4.0084733963012695
Iteration 38: train_loss 3.890174150466919
Iteration 39: train_loss 3.9293222427368164
Iteration 40: train_loss 3.802387237548828
Epoch 57: train_avg_loss 3.8618550837039947 eval_avg_acc: 0.19949998441069036 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:17:46] [32mIntermediate result: 0.19949998441069036  (Index 56)[0m
================Epoch: 58================
Iteration 1: train_loss 3.864363431930542
Iteration 2: train_loss 3.8516063690185547
Iteration 3: train_loss 3.915156126022339
Iteration 4: train_loss 3.8989639282226562
Iteration 5: train_loss 3.8257083892822266
Iteration 6: train_loss 3.898928165435791
Iteration 7: train_loss 3.885479211807251
Iteration 8: train_loss 3.875175952911377
Iteration 9: train_loss 3.7584950923919678
Iteration 10: train_loss 3.854926586151123
Iteration 11: train_loss 3.8062705993652344
Iteration 12: train_loss 3.9200944900512695
Iteration 13: train_loss 3.7721922397613525
Iteration 14: train_loss 3.9637603759765625
Iteration 15: train_loss 3.7839622497558594
Iteration 16: train_loss 3.8639934062957764
Iteration 17: train_loss 3.7538862228393555
Iteration 18: train_loss 3.8087644577026367
Iteration 19: train_loss 3.8399057388305664
Iteration 20: train_loss 3.635061264038086
Iteration 21: train_loss 3.8772478103637695
Iteration 22: train_loss 3.8985612392425537
Iteration 23: train_loss 3.6650032997131348
Iteration 24: train_loss 3.9116854667663574
Iteration 25: train_loss 3.7688300609588623
Iteration 26: train_loss 3.8472135066986084
Iteration 27: train_loss 3.8095076084136963
Iteration 28: train_loss 3.922628402709961
Iteration 29: train_loss 3.9234015941619873
Iteration 30: train_loss 3.7830379009246826
Iteration 31: train_loss 3.8287906646728516
Iteration 32: train_loss 3.9030349254608154
Iteration 33: train_loss 3.9773294925689697
Iteration 34: train_loss 3.7360031604766846
Iteration 35: train_loss 3.7921934127807617
Iteration 36: train_loss 3.8784074783325195
Iteration 37: train_loss 3.788607597351074
Iteration 38: train_loss 3.8088533878326416
Iteration 39: train_loss 3.8632662296295166
Iteration 40: train_loss 4.007633686065674
Epoch 58: train_avg_loss 3.844198280572891 eval_avg_acc: 0.1970786590415879 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:17:58] [32mIntermediate result: 0.1970786590415879  (Index 57)[0m
================Epoch: 59================
Iteration 1: train_loss 3.995950222015381
Iteration 2: train_loss 4.010858058929443
Iteration 3: train_loss 3.831434726715088
Iteration 4: train_loss 3.8888230323791504
Iteration 5: train_loss 3.839449405670166
Iteration 6: train_loss 3.875498056411743
Iteration 7: train_loss 3.729151964187622
Iteration 8: train_loss 3.7727162837982178
Iteration 9: train_loss 3.757314920425415
Iteration 10: train_loss 3.7908987998962402
Iteration 11: train_loss 3.879302740097046
Iteration 12: train_loss 3.9064652919769287
Iteration 13: train_loss 3.917776107788086
Iteration 14: train_loss 3.8615310192108154
Iteration 15: train_loss 3.7456538677215576
Iteration 16: train_loss 3.7691750526428223
Iteration 17: train_loss 3.727130889892578
Iteration 18: train_loss 3.81005859375
Iteration 19: train_loss 3.756666898727417
Iteration 20: train_loss 3.866476058959961
Iteration 21: train_loss 3.963837146759033
Iteration 22: train_loss 3.8744611740112305
Iteration 23: train_loss 3.763134717941284
Iteration 24: train_loss 3.694547176361084
Iteration 25: train_loss 3.7742130756378174
Iteration 26: train_loss 3.754631280899048
Iteration 27: train_loss 3.940325975418091
Iteration 28: train_loss 3.8617029190063477
Iteration 29: train_loss 3.9791951179504395
Iteration 30: train_loss 3.890183925628662
Iteration 31: train_loss 3.7895994186401367
Iteration 32: train_loss 3.7850327491760254
Iteration 33: train_loss 3.791259288787842
Iteration 34: train_loss 3.9468841552734375
Iteration 35: train_loss 3.7853453159332275
Iteration 36: train_loss 3.8055295944213867
Iteration 37: train_loss 4.0181803703308105
Iteration 38: train_loss 4.022370338439941
Iteration 39: train_loss 3.853970766067505
Iteration 40: train_loss 3.989838123321533
Epoch 59: train_avg_loss 3.850414365530014 eval_avg_acc: 0.1973606243724791 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:18:09] [32mIntermediate result: 0.1973606243724791  (Index 58)[0m
================Epoch: 60================
Iteration 1: train_loss 3.9647793769836426
Iteration 2: train_loss 3.770237922668457
Iteration 3: train_loss 3.9605886936187744
Iteration 4: train_loss 3.78708815574646
Iteration 5: train_loss 3.8788342475891113
Iteration 6: train_loss 3.7185282707214355
Iteration 7: train_loss 3.8742423057556152
Iteration 8: train_loss 3.8481757640838623
Iteration 9: train_loss 3.6731929779052734
Iteration 10: train_loss 3.8475546836853027
Iteration 11: train_loss 3.890902280807495
Iteration 12: train_loss 3.7920219898223877
Iteration 13: train_loss 3.9101855754852295
Iteration 14: train_loss 3.853877067565918
Iteration 15: train_loss 3.811645030975342
Iteration 16: train_loss 3.662407159805298
Iteration 17: train_loss 3.8163986206054688
Iteration 18: train_loss 3.780918598175049
Iteration 19: train_loss 3.7674412727355957
Iteration 20: train_loss 3.8749594688415527
Iteration 21: train_loss 3.8353164196014404
Iteration 22: train_loss 3.739157199859619
Iteration 23: train_loss 3.652373790740967
Iteration 24: train_loss 3.778298854827881
Iteration 25: train_loss 3.7968122959136963
Iteration 26: train_loss 3.7719924449920654
Iteration 27: train_loss 3.840479612350464
Iteration 28: train_loss 3.766345739364624
Iteration 29: train_loss 3.680600643157959
Iteration 30: train_loss 3.771660566329956
Iteration 31: train_loss 3.743420124053955
Iteration 32: train_loss 3.911620616912842
Iteration 33: train_loss 3.9365909099578857
Iteration 34: train_loss 3.838930606842041
Iteration 35: train_loss 3.8124256134033203
Iteration 36: train_loss 3.7798478603363037
Iteration 37: train_loss 3.8626632690429688
Iteration 38: train_loss 3.7617194652557373
Iteration 39: train_loss 3.8560328483581543
Iteration 40: train_loss 3.673330545425415
Epoch 60: train_avg_loss 3.8073399722576142 eval_avg_acc: 0.20229545319794204 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:18:21] [32mIntermediate result: 0.20229545319794204  (Index 59)[0m
================Epoch: 61================
Iteration 1: train_loss 3.925891160964966
Iteration 2: train_loss 3.9687845706939697
Iteration 3: train_loss 3.690467119216919
Iteration 4: train_loss 3.7338194847106934
Iteration 5: train_loss 3.7652523517608643
Iteration 6: train_loss 3.7621443271636963
Iteration 7: train_loss 3.762423038482666
Iteration 8: train_loss 3.8552839756011963
Iteration 9: train_loss 3.713470935821533
Iteration 10: train_loss 3.6839637756347656
Iteration 11: train_loss 3.7842776775360107
Iteration 12: train_loss 3.7607693672180176
Iteration 13: train_loss 3.7742347717285156
Iteration 14: train_loss 3.799978256225586
Iteration 15: train_loss 3.8455145359039307
Iteration 16: train_loss 3.80329966545105
Iteration 17: train_loss 3.6862330436706543
Iteration 18: train_loss 3.8582983016967773
Iteration 19: train_loss 3.6787757873535156
Iteration 20: train_loss 3.7257657051086426
Iteration 21: train_loss 3.7558391094207764
Iteration 22: train_loss 3.728416919708252
Iteration 23: train_loss 3.82747745513916
Iteration 24: train_loss 3.828054666519165
Iteration 25: train_loss 3.810499668121338
Iteration 26: train_loss 3.8637490272521973
Iteration 27: train_loss 3.810631275177002
Iteration 28: train_loss 3.7880849838256836
Iteration 29: train_loss 3.6867520809173584
Iteration 30: train_loss 3.7894437313079834
Iteration 31: train_loss 3.7913882732391357
Iteration 32: train_loss 3.6944215297698975
Iteration 33: train_loss 3.7004752159118652
Iteration 34: train_loss 3.7702622413635254
Iteration 35: train_loss 3.918592691421509
Iteration 36: train_loss 3.877389430999756
Iteration 37: train_loss 3.988002061843872
Iteration 38: train_loss 3.8346667289733887
Iteration 39: train_loss 3.748518705368042
Iteration 40: train_loss 3.373411178588867
Epoch 61: train_avg_loss 3.7791181206703186 eval_avg_acc: 0.19820523520723726 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:18:33] [32mIntermediate result: 0.19820523520723726  (Index 60)[0m
================Epoch: 62================
Iteration 1: train_loss 3.7473671436309814
Iteration 2: train_loss 3.767866611480713
Iteration 3: train_loss 3.7860007286071777
Iteration 4: train_loss 3.6408610343933105
Iteration 5: train_loss 3.76749587059021
Iteration 6: train_loss 3.7929952144622803
Iteration 7: train_loss 3.7803754806518555
Iteration 8: train_loss 3.786184310913086
Iteration 9: train_loss 3.765530586242676
Iteration 10: train_loss 3.7828407287597656
Iteration 11: train_loss 3.757580280303955
Iteration 12: train_loss 4.03800630569458
Iteration 13: train_loss 3.6370913982391357
Iteration 14: train_loss 3.9456233978271484
Iteration 15: train_loss 3.677717685699463
Iteration 16: train_loss 3.6626126766204834
Iteration 17: train_loss 3.6868274211883545
Iteration 18: train_loss 3.57963228225708
Iteration 19: train_loss 3.7563414573669434
Iteration 20: train_loss 3.831353187561035
Iteration 21: train_loss 3.835108757019043
Iteration 22: train_loss 3.6804850101470947
Iteration 23: train_loss 3.7150917053222656
Iteration 24: train_loss 3.8018958568573
Iteration 25: train_loss 3.744105339050293
Iteration 26: train_loss 3.7205076217651367
Iteration 27: train_loss 3.659308910369873
Iteration 28: train_loss 3.689901113510132
Iteration 29: train_loss 3.7465555667877197
Iteration 30: train_loss 3.744138717651367
Iteration 31: train_loss 3.8587279319763184
Iteration 32: train_loss 3.5759363174438477
Iteration 33: train_loss 3.787911891937256
Iteration 34: train_loss 3.734029531478882
Iteration 35: train_loss 3.7151596546173096
Iteration 36: train_loss 3.8036675453186035
Iteration 37: train_loss 3.7689201831817627
Iteration 38: train_loss 3.848367691040039
Iteration 39: train_loss 3.706165313720703
Iteration 40: train_loss 3.2143819332122803
Epoch 62: train_avg_loss 3.7385167598724367 eval_avg_acc: 0.20079785066859918 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:18:44] [32mIntermediate result: 0.20079785066859918  (Index 61)[0m
================Epoch: 63================
Iteration 1: train_loss 3.912864923477173
Iteration 2: train_loss 3.7160985469818115
Iteration 3: train_loss 3.7670931816101074
Iteration 4: train_loss 3.820060968399048
Iteration 5: train_loss 3.7792346477508545
Iteration 6: train_loss 3.597623109817505
Iteration 7: train_loss 3.6360979080200195
Iteration 8: train_loss 3.7642531394958496
Iteration 9: train_loss 3.829695701599121
Iteration 10: train_loss 3.6994271278381348
Iteration 11: train_loss 3.7696728706359863
Iteration 12: train_loss 3.7210426330566406
Iteration 13: train_loss 3.6257240772247314
Iteration 14: train_loss 3.5271553993225098
Iteration 15: train_loss 3.6162662506103516
Iteration 16: train_loss 3.774789810180664
Iteration 17: train_loss 3.653616189956665
Iteration 18: train_loss 3.6831412315368652
Iteration 19: train_loss 3.6535940170288086
Iteration 20: train_loss 3.7189414501190186
Iteration 21: train_loss 3.7728421688079834
Iteration 22: train_loss 3.6321794986724854
Iteration 23: train_loss 3.8519163131713867
Iteration 24: train_loss 3.688204288482666
Iteration 25: train_loss 3.7813732624053955
Iteration 26: train_loss 3.688809394836426
Iteration 27: train_loss 3.6995255947113037
Iteration 28: train_loss 3.865076780319214
Iteration 29: train_loss 3.716320514678955
Iteration 30: train_loss 3.6498031616210938
Iteration 31: train_loss 3.846133232116699
Iteration 32: train_loss 3.6470775604248047
Iteration 33: train_loss 3.9781272411346436
Iteration 34: train_loss 3.8564841747283936
Iteration 35: train_loss 3.7535400390625
Iteration 36: train_loss 3.7739124298095703
Iteration 37: train_loss 3.6503982543945312
Iteration 38: train_loss 3.7437491416931152
Iteration 39: train_loss 3.7937557697296143
Iteration 40: train_loss 4.336927890777588
Epoch 63: train_avg_loss 3.749813747406006 eval_avg_acc: 0.20114063190822123 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:18:56] [32mIntermediate result: 0.20114063190822123  (Index 62)[0m
================Epoch: 64================
Iteration 1: train_loss 3.7965569496154785
Iteration 2: train_loss 3.8262228965759277
Iteration 3: train_loss 3.7549450397491455
Iteration 4: train_loss 3.792478084564209
Iteration 5: train_loss 3.8456668853759766
Iteration 6: train_loss 3.6426985263824463
Iteration 7: train_loss 3.6704163551330566
Iteration 8: train_loss 3.738776922225952
Iteration 9: train_loss 3.620582103729248
Iteration 10: train_loss 3.560392379760742
Iteration 11: train_loss 3.570561408996582
Iteration 12: train_loss 3.798067331314087
Iteration 13: train_loss 3.693178653717041
Iteration 14: train_loss 3.6783792972564697
Iteration 15: train_loss 3.7189066410064697
Iteration 16: train_loss 3.7603442668914795
Iteration 17: train_loss 3.639040946960449
Iteration 18: train_loss 3.615563154220581
Iteration 19: train_loss 3.813710927963257
Iteration 20: train_loss 3.904102325439453
Iteration 21: train_loss 3.8246662616729736
Iteration 22: train_loss 3.7269535064697266
Iteration 23: train_loss 3.707566738128662
Iteration 24: train_loss 3.6532363891601562
Iteration 25: train_loss 3.663693428039551
Iteration 26: train_loss 3.7120468616485596
Iteration 27: train_loss 3.759272336959839
Iteration 28: train_loss 3.7635910511016846
Iteration 29: train_loss 3.7574636936187744
Iteration 30: train_loss 3.596355438232422
Iteration 31: train_loss 3.7024972438812256
Iteration 32: train_loss 3.792232036590576
Iteration 33: train_loss 3.836559295654297
Iteration 34: train_loss 3.8055245876312256
Iteration 35: train_loss 3.7019643783569336
Iteration 36: train_loss 3.7777976989746094
Iteration 37: train_loss 3.7892353534698486
Iteration 38: train_loss 3.664217948913574
Iteration 39: train_loss 3.7316739559173584
Iteration 40: train_loss 3.6744441986083984
Epoch 64: train_avg_loss 3.7270395874977114 eval_avg_acc: 0.2047581675042216 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:19:08] [32mIntermediate result: 0.2047581675042216  (Index 63)[0m
================Epoch: 65================
Iteration 1: train_loss 3.6961026191711426
Iteration 2: train_loss 3.773080587387085
Iteration 3: train_loss 3.7932095527648926
Iteration 4: train_loss 3.7173869609832764
Iteration 5: train_loss 3.767857551574707
Iteration 6: train_loss 3.567753314971924
Iteration 7: train_loss 3.6576337814331055
Iteration 8: train_loss 3.652419090270996
Iteration 9: train_loss 3.6795434951782227
Iteration 10: train_loss 3.6018576622009277
Iteration 11: train_loss 3.6759843826293945
Iteration 12: train_loss 3.674355983734131
Iteration 13: train_loss 3.7832043170928955
Iteration 14: train_loss 3.602991819381714
Iteration 15: train_loss 3.5099709033966064
Iteration 16: train_loss 3.5019843578338623
Iteration 17: train_loss 3.639629602432251
Iteration 18: train_loss 3.6686129570007324
Iteration 19: train_loss 3.6020867824554443
Iteration 20: train_loss 3.5174190998077393
Iteration 21: train_loss 3.7763776779174805
Iteration 22: train_loss 3.6804428100585938
Iteration 23: train_loss 3.751845121383667
Iteration 24: train_loss 3.7358150482177734
Iteration 25: train_loss 3.776477098464966
Iteration 26: train_loss 3.6184306144714355
Iteration 27: train_loss 3.8360836505889893
Iteration 28: train_loss 3.6570889949798584
Iteration 29: train_loss 3.9964897632598877
Iteration 30: train_loss 3.9358770847320557
Iteration 31: train_loss 3.718536138534546
Iteration 32: train_loss 3.8236513137817383
Iteration 33: train_loss 3.6542420387268066
Iteration 34: train_loss 3.6865015029907227
Iteration 35: train_loss 3.9050703048706055
Iteration 36: train_loss 3.653928518295288
Iteration 37: train_loss 3.654458522796631
Iteration 38: train_loss 3.771456718444824
Iteration 39: train_loss 3.703091859817505
Iteration 40: train_loss 3.7229325771331787
Epoch 65: train_avg_loss 3.70354705452919 eval_avg_acc: 0.20608184313580297 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:19:19] [32mIntermediate result: 0.20608184313580297  (Index 64)[0m
================Epoch: 66================
Iteration 1: train_loss 3.7640388011932373
Iteration 2: train_loss 3.734645366668701
Iteration 3: train_loss 3.6705117225646973
Iteration 4: train_loss 3.5999245643615723
Iteration 5: train_loss 3.7473320960998535
Iteration 6: train_loss 3.7453250885009766
Iteration 7: train_loss 3.76962947845459
Iteration 8: train_loss 3.704481363296509
Iteration 9: train_loss 3.6242105960845947
Iteration 10: train_loss 3.640986919403076
Iteration 11: train_loss 3.608567237854004
Iteration 12: train_loss 3.7069172859191895
Iteration 13: train_loss 3.7469537258148193
Iteration 14: train_loss 3.6117119789123535
Iteration 15: train_loss 3.5283591747283936
Iteration 16: train_loss 3.6051223278045654
Iteration 17: train_loss 3.5101704597473145
Iteration 18: train_loss 3.604940414428711
Iteration 19: train_loss 3.6660876274108887
Iteration 20: train_loss 3.698840618133545
Iteration 21: train_loss 3.495563268661499
Iteration 22: train_loss 3.593796730041504
Iteration 23: train_loss 3.648399591445923
Iteration 24: train_loss 3.694248676300049
Iteration 25: train_loss 3.6021077632904053
Iteration 26: train_loss 3.706960439682007
Iteration 27: train_loss 3.8450334072113037
Iteration 28: train_loss 3.7005422115325928
Iteration 29: train_loss 3.6245453357696533
Iteration 30: train_loss 3.763260841369629
Iteration 31: train_loss 3.7282683849334717
Iteration 32: train_loss 3.670966863632202
Iteration 33: train_loss 3.733316421508789
Iteration 34: train_loss 3.7218751907348633
Iteration 35: train_loss 3.742739200592041
Iteration 36: train_loss 3.664940595626831
Iteration 37: train_loss 3.637892723083496
Iteration 38: train_loss 3.6298654079437256
Iteration 39: train_loss 3.6174752712249756
Iteration 40: train_loss 3.5575523376464844
Epoch 66: train_avg_loss 3.666702687740326 eval_avg_acc: 0.21027223830637826 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:19:31] [32mIntermediate result: 0.21027223830637826  (Index 65)[0m
================Epoch: 67================
Iteration 1: train_loss 3.663158416748047
Iteration 2: train_loss 3.8400285243988037
Iteration 3: train_loss 3.6062963008880615
Iteration 4: train_loss 3.6654441356658936
Iteration 5: train_loss 3.588836431503296
Iteration 6: train_loss 3.6802866458892822
Iteration 7: train_loss 3.71364164352417
Iteration 8: train_loss 3.688995122909546
Iteration 9: train_loss 3.704944610595703
Iteration 10: train_loss 3.5714902877807617
Iteration 11: train_loss 3.479387044906616
Iteration 12: train_loss 3.479227304458618
Iteration 13: train_loss 3.5888874530792236
Iteration 14: train_loss 3.5747997760772705
Iteration 15: train_loss 3.604238986968994
Iteration 16: train_loss 3.6815927028656006
Iteration 17: train_loss 3.567692756652832
Iteration 18: train_loss 3.61867094039917
Iteration 19: train_loss 3.696504592895508
Iteration 20: train_loss 3.7647244930267334
Iteration 21: train_loss 3.6452724933624268
Iteration 22: train_loss 3.6361021995544434
Iteration 23: train_loss 3.612771511077881
Iteration 24: train_loss 3.734495162963867
Iteration 25: train_loss 3.821719169616699
Iteration 26: train_loss 3.6287145614624023
Iteration 27: train_loss 3.869170665740967
Iteration 28: train_loss 3.622215509414673
Iteration 29: train_loss 3.6908938884735107
Iteration 30: train_loss 3.610487461090088
Iteration 31: train_loss 3.729515790939331
Iteration 32: train_loss 3.6538798809051514
Iteration 33: train_loss 3.6217641830444336
Iteration 34: train_loss 3.746899366378784
Iteration 35: train_loss 3.709408760070801
Iteration 36: train_loss 3.852842092514038
Iteration 37: train_loss 3.6882338523864746
Iteration 38: train_loss 3.7896416187286377
Iteration 39: train_loss 3.5462119579315186
Iteration 40: train_loss 3.702554941177368
Epoch 67: train_avg_loss 3.6672910809516908 eval_avg_acc: 0.2077565478872375 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:19:44] [32mIntermediate result: 0.2077565478872375  (Index 66)[0m
================Epoch: 68================
Iteration 1: train_loss 3.7361252307891846
Iteration 2: train_loss 3.7458789348602295
Iteration 3: train_loss 3.6493072509765625
Iteration 4: train_loss 3.782111406326294
Iteration 5: train_loss 3.481649875640869
Iteration 6: train_loss 3.7606968879699707
Iteration 7: train_loss 3.586503505706787
Iteration 8: train_loss 3.6721787452697754
Iteration 9: train_loss 3.6079022884368896
Iteration 10: train_loss 3.5387775897979736
Iteration 11: train_loss 3.616393804550171
Iteration 12: train_loss 3.501091480255127
Iteration 13: train_loss 3.579056978225708
Iteration 14: train_loss 3.5534207820892334
Iteration 15: train_loss 3.5729382038116455
Iteration 16: train_loss 3.5585432052612305
Iteration 17: train_loss 3.577816963195801
Iteration 18: train_loss 3.5936553478240967
Iteration 19: train_loss 3.5780417919158936
Iteration 20: train_loss 3.622307777404785
Iteration 21: train_loss 3.599090337753296
Iteration 22: train_loss 3.7287487983703613
Iteration 23: train_loss 3.6355323791503906
Iteration 24: train_loss 3.7401351928710938
Iteration 25: train_loss 3.5687572956085205
Iteration 26: train_loss 3.7983052730560303
Iteration 27: train_loss 3.7242259979248047
Iteration 28: train_loss 3.535940647125244
Iteration 29: train_loss 3.69498610496521
Iteration 30: train_loss 3.67905592918396
Iteration 31: train_loss 3.700442314147949
Iteration 32: train_loss 3.627459764480591
Iteration 33: train_loss 3.7242236137390137
Iteration 34: train_loss 3.6656646728515625
Iteration 35: train_loss 3.5610077381134033
Iteration 36: train_loss 3.80835223197937
Iteration 37: train_loss 3.5576298236846924
Iteration 38: train_loss 3.629481792449951
Iteration 39: train_loss 3.77346134185791
Iteration 40: train_loss 3.8202133178710938
Epoch 68: train_avg_loss 3.647177815437317 eval_avg_acc: 0.20567939441133976 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:19:56] [32mIntermediate result: 0.20567939441133976  (Index 67)[0m
================Epoch: 69================
Iteration 1: train_loss 3.6183090209960938
Iteration 2: train_loss 3.702594757080078
Iteration 3: train_loss 3.6574409008026123
Iteration 4: train_loss 3.515899658203125
Iteration 5: train_loss 3.5957536697387695
Iteration 6: train_loss 3.6656413078308105
Iteration 7: train_loss 3.6391115188598633
Iteration 8: train_loss 3.654994010925293
Iteration 9: train_loss 3.510331392288208
Iteration 10: train_loss 3.638366937637329
Iteration 11: train_loss 3.5113229751586914
Iteration 12: train_loss 3.552802562713623
Iteration 13: train_loss 3.603231906890869
Iteration 14: train_loss 3.624663829803467
Iteration 15: train_loss 3.552030324935913
Iteration 16: train_loss 3.5867760181427
Iteration 17: train_loss 3.683603048324585
Iteration 18: train_loss 3.6281440258026123
Iteration 19: train_loss 3.722661256790161
Iteration 20: train_loss 3.5735292434692383
Iteration 21: train_loss 3.5177724361419678
Iteration 22: train_loss 3.679138660430908
Iteration 23: train_loss 3.701528787612915
Iteration 24: train_loss 3.6746082305908203
Iteration 25: train_loss 3.58512282371521
Iteration 26: train_loss 3.5401077270507812
Iteration 27: train_loss 3.625065803527832
Iteration 28: train_loss 3.626530647277832
Iteration 29: train_loss 3.671687602996826
Iteration 30: train_loss 3.643587350845337
Iteration 31: train_loss 3.802379608154297
Iteration 32: train_loss 3.449943780899048
Iteration 33: train_loss 3.747143268585205
Iteration 34: train_loss 3.704916477203369
Iteration 35: train_loss 3.5656323432922363
Iteration 36: train_loss 3.7346343994140625
Iteration 37: train_loss 3.4223105907440186
Iteration 38: train_loss 3.7763543128967285
Iteration 39: train_loss 3.698012590408325
Iteration 40: train_loss 3.4686973094940186
Epoch 69: train_avg_loss 3.6218095779418946 eval_avg_acc: 0.2063227890169788 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:20:08] [32mIntermediate result: 0.2063227890169788  (Index 68)[0m
================Epoch: 70================
Iteration 1: train_loss 3.6701982021331787
Iteration 2: train_loss 3.6789181232452393
Iteration 3: train_loss 3.643115997314453
Iteration 4: train_loss 3.705467700958252
Iteration 5: train_loss 3.6140525341033936
Iteration 6: train_loss 3.6796092987060547
Iteration 7: train_loss 3.563422203063965
Iteration 8: train_loss 3.656424045562744
Iteration 9: train_loss 3.670424461364746
Iteration 10: train_loss 3.65539288520813
Iteration 11: train_loss 3.8614556789398193
Iteration 12: train_loss 3.650651216506958
Iteration 13: train_loss 3.6189308166503906
Iteration 14: train_loss 3.606524705886841
Iteration 15: train_loss 3.5946197509765625
Iteration 16: train_loss 3.700096607208252
Iteration 17: train_loss 3.513333797454834
Iteration 18: train_loss 3.4401755332946777
Iteration 19: train_loss 3.646831750869751
Iteration 20: train_loss 3.601797342300415
Iteration 21: train_loss 3.626854181289673
Iteration 22: train_loss 3.622316360473633
Iteration 23: train_loss 3.583719253540039
Iteration 24: train_loss 3.562730073928833
Iteration 25: train_loss 3.6351959705352783
Iteration 26: train_loss 3.5997512340545654
Iteration 27: train_loss 3.4467732906341553
Iteration 28: train_loss 3.5885496139526367
Iteration 29: train_loss 3.675185441970825
Iteration 30: train_loss 3.490675687789917
Iteration 31: train_loss 3.732851982116699
Iteration 32: train_loss 3.707711696624756
Iteration 33: train_loss 3.561903715133667
Iteration 34: train_loss 3.5530014038085938
Iteration 35: train_loss 3.6745827198028564
Iteration 36: train_loss 3.6192288398742676
Iteration 37: train_loss 3.474909782409668
Iteration 38: train_loss 3.5190765857696533
Iteration 39: train_loss 3.5494401454925537
Iteration 40: train_loss 3.025646448135376
Epoch 70: train_avg_loss 3.6005386769771577 eval_avg_acc: 0.21330515487936394 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:20:20] [32mIntermediate result: 0.21330515487936394  (Index 69)[0m
================Epoch: 71================
Iteration 1: train_loss 3.663975238800049
Iteration 2: train_loss 3.585615634918213
Iteration 3: train_loss 3.4418420791625977
Iteration 4: train_loss 3.538945436477661
Iteration 5: train_loss 3.588783025741577
Iteration 6: train_loss 3.6074228286743164
Iteration 7: train_loss 3.5803141593933105
Iteration 8: train_loss 3.4299991130828857
Iteration 9: train_loss 3.5506391525268555
Iteration 10: train_loss 3.4418411254882812
Iteration 11: train_loss 3.5970356464385986
Iteration 12: train_loss 3.4800784587860107
Iteration 13: train_loss 3.4967870712280273
Iteration 14: train_loss 3.6517090797424316
Iteration 15: train_loss 3.6317930221557617
Iteration 16: train_loss 3.587538719177246
Iteration 17: train_loss 3.5861973762512207
Iteration 18: train_loss 3.521679639816284
Iteration 19: train_loss 3.5792508125305176
Iteration 20: train_loss 3.5805201530456543
Iteration 21: train_loss 3.553522825241089
Iteration 22: train_loss 3.6505496501922607
Iteration 23: train_loss 3.714503049850464
Iteration 24: train_loss 3.621015787124634
Iteration 25: train_loss 3.5864715576171875
Iteration 26: train_loss 3.457195997238159
Iteration 27: train_loss 3.600886821746826
Iteration 28: train_loss 3.714395761489868
Iteration 29: train_loss 3.5741961002349854
Iteration 30: train_loss 3.5484237670898438
Iteration 31: train_loss 3.693394184112549
Iteration 32: train_loss 3.686621904373169
Iteration 33: train_loss 3.5893032550811768
Iteration 34: train_loss 3.7014474868774414
Iteration 35: train_loss 3.503662109375
Iteration 36: train_loss 3.5717711448669434
Iteration 37: train_loss 3.583745241165161
Iteration 38: train_loss 3.5953052043914795
Iteration 39: train_loss 3.769350528717041
Iteration 40: train_loss 3.4630751609802246
Epoch 71: train_avg_loss 3.5830201327800753 eval_avg_acc: 0.21010184072470542 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:20:33] [32mIntermediate result: 0.21010184072470542  (Index 70)[0m
================Epoch: 72================
Iteration 1: train_loss 3.642852306365967
Iteration 2: train_loss 3.4359047412872314
Iteration 3: train_loss 3.5679264068603516
Iteration 4: train_loss 3.6802334785461426
Iteration 5: train_loss 3.5207290649414062
Iteration 6: train_loss 3.688398838043213
Iteration 7: train_loss 3.7718446254730225
Iteration 8: train_loss 3.447901487350464
Iteration 9: train_loss 3.6970529556274414
Iteration 10: train_loss 3.6142852306365967
Iteration 11: train_loss 3.4670395851135254
Iteration 12: train_loss 3.620027542114258
Iteration 13: train_loss 3.6353299617767334
Iteration 14: train_loss 3.4531772136688232
Iteration 15: train_loss 3.626516580581665
Iteration 16: train_loss 3.5602591037750244
Iteration 17: train_loss 3.51943039894104
Iteration 18: train_loss 3.6753275394439697
Iteration 19: train_loss 3.719182014465332
Iteration 20: train_loss 3.516171455383301
Iteration 21: train_loss 3.680647611618042
Iteration 22: train_loss 3.4145407676696777
Iteration 23: train_loss 3.5251965522766113
Iteration 24: train_loss 3.6089136600494385
Iteration 25: train_loss 3.687089443206787
Iteration 26: train_loss 3.6466100215911865
Iteration 27: train_loss 3.635432004928589
Iteration 28: train_loss 3.5491485595703125
Iteration 29: train_loss 3.56977915763855
Iteration 30: train_loss 3.574014186859131
Iteration 31: train_loss 3.5900163650512695
Iteration 32: train_loss 3.564833879470825
Iteration 33: train_loss 3.702617883682251
Iteration 34: train_loss 3.6296327114105225
Iteration 35: train_loss 3.6199147701263428
Iteration 36: train_loss 3.52583909034729
Iteration 37: train_loss 3.5491278171539307
Iteration 38: train_loss 3.7098920345306396
Iteration 39: train_loss 3.6139330863952637
Iteration 40: train_loss 3.2164478302001953
Epoch 72: train_avg_loss 3.5868304491043093 eval_avg_acc: 0.21076342631880957 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:20:45] [32mIntermediate result: 0.21076342631880957  (Index 71)[0m
================Epoch: 73================
Iteration 1: train_loss 3.584195852279663
Iteration 2: train_loss 3.637580156326294
Iteration 3: train_loss 3.7416841983795166
Iteration 4: train_loss 3.606804132461548
Iteration 5: train_loss 3.5245401859283447
Iteration 6: train_loss 3.5195648670196533
Iteration 7: train_loss 3.555155038833618
Iteration 8: train_loss 3.590820074081421
Iteration 9: train_loss 3.6922101974487305
Iteration 10: train_loss 3.7348694801330566
Iteration 11: train_loss 3.4066030979156494
Iteration 12: train_loss 3.5092225074768066
Iteration 13: train_loss 3.5135064125061035
Iteration 14: train_loss 3.552523612976074
Iteration 15: train_loss 3.392469644546509
Iteration 16: train_loss 3.473292589187622
Iteration 17: train_loss 3.5318922996520996
Iteration 18: train_loss 3.6008379459381104
Iteration 19: train_loss 3.6339850425720215
Iteration 20: train_loss 3.6891164779663086
Iteration 21: train_loss 3.6205925941467285
Iteration 22: train_loss 3.755181074142456
Iteration 23: train_loss 3.490684747695923
Iteration 24: train_loss 3.431232452392578
Iteration 25: train_loss 3.621755361557007
Iteration 26: train_loss 3.454540729522705
Iteration 27: train_loss 3.546494483947754
Iteration 28: train_loss 3.5519015789031982
Iteration 29: train_loss 3.6143903732299805
Iteration 30: train_loss 3.6221301555633545
Iteration 31: train_loss 3.7091681957244873
Iteration 32: train_loss 3.579787492752075
Iteration 33: train_loss 3.762467384338379
Iteration 34: train_loss 3.526611566543579
Iteration 35: train_loss 3.6354246139526367
Iteration 36: train_loss 3.8422629833221436
Iteration 37: train_loss 3.5371978282928467
Iteration 38: train_loss 3.71482515335083
Iteration 39: train_loss 3.629889726638794
Iteration 40: train_loss 3.6249301433563232
Epoch 73: train_avg_loss 3.594058561325073 eval_avg_acc: 0.20872611487754766 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:20:57] [32mIntermediate result: 0.20872611487754766  (Index 72)[0m
================Epoch: 74================
Iteration 1: train_loss 3.5404200553894043
Iteration 2: train_loss 3.6739919185638428
Iteration 3: train_loss 3.5770022869110107
Iteration 4: train_loss 3.7005510330200195
Iteration 5: train_loss 3.533092498779297
Iteration 6: train_loss 3.436842203140259
Iteration 7: train_loss 3.597533941268921
Iteration 8: train_loss 3.4777169227600098
Iteration 9: train_loss 3.549602746963501
Iteration 10: train_loss 3.626814126968384
Iteration 11: train_loss 3.4565205574035645
Iteration 12: train_loss 3.455207347869873
Iteration 13: train_loss 3.486330509185791
Iteration 14: train_loss 3.5041494369506836
Iteration 15: train_loss 3.4454643726348877
Iteration 16: train_loss 3.5026092529296875
Iteration 17: train_loss 3.546787977218628
Iteration 18: train_loss 3.3758955001831055
Iteration 19: train_loss 3.546865463256836
Iteration 20: train_loss 3.5330963134765625
Iteration 21: train_loss 3.5604147911071777
Iteration 22: train_loss 3.554903030395508
Iteration 23: train_loss 3.6156907081604004
Iteration 24: train_loss 3.32610821723938
Iteration 25: train_loss 3.5518672466278076
Iteration 26: train_loss 3.5056397914886475
Iteration 27: train_loss 3.5214622020721436
Iteration 28: train_loss 3.5387890338897705
Iteration 29: train_loss 3.581671714782715
Iteration 30: train_loss 3.569844961166382
Iteration 31: train_loss 3.5629265308380127
Iteration 32: train_loss 3.5926921367645264
Iteration 33: train_loss 3.621035575866699
Iteration 34: train_loss 3.5902342796325684
Iteration 35: train_loss 3.640016555786133
Iteration 36: train_loss 3.7060914039611816
Iteration 37: train_loss 3.6304776668548584
Iteration 38: train_loss 3.629667043685913
Iteration 39: train_loss 3.636997938156128
Iteration 40: train_loss 3.6204516887664795
Epoch 74: train_avg_loss 3.5530869245529173 eval_avg_acc: 0.20798986194253705 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:21:09] [32mIntermediate result: 0.20798986194253705  (Index 73)[0m
================Epoch: 75================
Iteration 1: train_loss 3.690962791442871
Iteration 2: train_loss 3.6223995685577393
Iteration 3: train_loss 3.4656007289886475
Iteration 4: train_loss 3.6148836612701416
Iteration 5: train_loss 3.6296048164367676
Iteration 6: train_loss 3.544523000717163
Iteration 7: train_loss 3.647169351577759
Iteration 8: train_loss 3.384430170059204
Iteration 9: train_loss 3.6969988346099854
Iteration 10: train_loss 3.534090757369995
Iteration 11: train_loss 3.475862741470337
Iteration 12: train_loss 3.7312467098236084
Iteration 13: train_loss 3.534419298171997
Iteration 14: train_loss 3.717621326446533
Iteration 15: train_loss 3.626638174057007
Iteration 16: train_loss 3.5426318645477295
Iteration 17: train_loss 3.608617067337036
Iteration 18: train_loss 3.6664071083068848
Iteration 19: train_loss 3.4541585445404053
Iteration 20: train_loss 3.5105831623077393
Iteration 21: train_loss 3.4160449504852295
Iteration 22: train_loss 3.602332353591919
Iteration 23: train_loss 3.452306032180786
Iteration 24: train_loss 3.5136351585388184
Iteration 25: train_loss 3.613215684890747
Iteration 26: train_loss 3.4623537063598633
Iteration 27: train_loss 3.5478811264038086
Iteration 28: train_loss 3.5292000770568848
Iteration 29: train_loss 3.5708348751068115
Iteration 30: train_loss 3.4905807971954346
Iteration 31: train_loss 3.4030160903930664
Iteration 32: train_loss 3.6701834201812744
Iteration 33: train_loss 3.5197157859802246
Iteration 34: train_loss 3.550386428833008
Iteration 35: train_loss 3.464087963104248
Iteration 36: train_loss 3.5446560382843018
Iteration 37: train_loss 3.5058650970458984
Iteration 38: train_loss 3.3198845386505127
Iteration 39: train_loss 3.576267957687378
Iteration 40: train_loss 3.715601921081543
Epoch 75: train_avg_loss 3.554172492027283 eval_avg_acc: 0.21443318292898167 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:21:21] [32mIntermediate result: 0.21443318292898167  (Index 74)[0m
================Epoch: 76================
Iteration 1: train_loss 3.449266195297241
Iteration 2: train_loss 3.474646806716919
Iteration 3: train_loss 3.56536865234375
Iteration 4: train_loss 3.3861446380615234
Iteration 5: train_loss 3.5223467350006104
Iteration 6: train_loss 3.513453483581543
Iteration 7: train_loss 3.6092562675476074
Iteration 8: train_loss 3.4366798400878906
Iteration 9: train_loss 3.56319522857666
Iteration 10: train_loss 3.4472827911376953
Iteration 11: train_loss 3.368828535079956
Iteration 12: train_loss 3.5592217445373535
Iteration 13: train_loss 3.3819100856781006
Iteration 14: train_loss 3.5613884925842285
Iteration 15: train_loss 3.422140598297119
Iteration 16: train_loss 3.4112014770507812
Iteration 17: train_loss 3.4703335762023926
Iteration 18: train_loss 3.5374834537506104
Iteration 19: train_loss 3.4118638038635254
Iteration 20: train_loss 3.563568115234375
Iteration 21: train_loss 3.4773406982421875
Iteration 22: train_loss 3.465949296951294
Iteration 23: train_loss 3.562598943710327
Iteration 24: train_loss 3.5376369953155518
Iteration 25: train_loss 3.4424853324890137
Iteration 26: train_loss 3.437617063522339
Iteration 27: train_loss 3.4364748001098633
Iteration 28: train_loss 3.5776665210723877
Iteration 29: train_loss 3.5736684799194336
Iteration 30: train_loss 3.579942464828491
Iteration 31: train_loss 3.59124755859375
Iteration 32: train_loss 3.479451894760132
Iteration 33: train_loss 3.557573080062866
Iteration 34: train_loss 3.5781002044677734
Iteration 35: train_loss 3.501410961151123
Iteration 36: train_loss 3.650371551513672
Iteration 37: train_loss 3.5436604022979736
Iteration 38: train_loss 3.609382152557373
Iteration 39: train_loss 3.6134166717529297
Iteration 40: train_loss 3.2494330406188965
Epoch 76: train_avg_loss 3.5030252158641817 eval_avg_acc: 0.21392889397116832 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:21:33] [32mIntermediate result: 0.21392889397116832  (Index 75)[0m
================Epoch: 77================
Iteration 1: train_loss 3.457486152648926
Iteration 2: train_loss 3.673055410385132
Iteration 3: train_loss 3.5727930068969727
Iteration 4: train_loss 3.5752463340759277
Iteration 5: train_loss 3.50372052192688
Iteration 6: train_loss 3.5278728008270264
Iteration 7: train_loss 3.529496192932129
Iteration 8: train_loss 3.4830853939056396
Iteration 9: train_loss 3.660893440246582
Iteration 10: train_loss 3.5934252738952637
Iteration 11: train_loss 3.6533310413360596
Iteration 12: train_loss 3.576239585876465
Iteration 13: train_loss 3.551973342895508
Iteration 14: train_loss 3.4643914699554443
Iteration 15: train_loss 3.4422714710235596
Iteration 16: train_loss 3.478628635406494
Iteration 17: train_loss 3.460052967071533
Iteration 18: train_loss 3.6297969818115234
Iteration 19: train_loss 3.5528769493103027
Iteration 20: train_loss 3.489450693130493
Iteration 21: train_loss 3.434795618057251
Iteration 22: train_loss 3.387605667114258
Iteration 23: train_loss 3.5843465328216553
Iteration 24: train_loss 3.3588106632232666
Iteration 25: train_loss 3.61015248298645
Iteration 26: train_loss 3.529627561569214
Iteration 27: train_loss 3.48384952545166
Iteration 28: train_loss 3.4611711502075195
Iteration 29: train_loss 3.4341800212860107
Iteration 30: train_loss 3.4859888553619385
Iteration 31: train_loss 3.5614356994628906
Iteration 32: train_loss 3.5794854164123535
Iteration 33: train_loss 3.482919216156006
Iteration 34: train_loss 3.476693630218506
Iteration 35: train_loss 3.444018602371216
Iteration 36: train_loss 3.504789113998413
Iteration 37: train_loss 3.3277196884155273
Iteration 38: train_loss 3.592819929122925
Iteration 39: train_loss 3.408367156982422
Iteration 40: train_loss 3.7384750843048096
Epoch 77: train_avg_loss 3.519083482027054 eval_avg_acc: 0.21664802484049445 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:21:45] [32mIntermediate result: 0.21664802484049445  (Index 76)[0m
================Epoch: 78================
Iteration 1: train_loss 3.4199438095092773
Iteration 2: train_loss 3.501584529876709
Iteration 3: train_loss 3.493816614151001
Iteration 4: train_loss 3.3843271732330322
Iteration 5: train_loss 3.365163564682007
Iteration 6: train_loss 3.568537712097168
Iteration 7: train_loss 3.4060161113739014
Iteration 8: train_loss 3.567960500717163
Iteration 9: train_loss 3.3121471405029297
Iteration 10: train_loss 3.4278640747070312
Iteration 11: train_loss 3.4032602310180664
Iteration 12: train_loss 3.448216438293457
Iteration 13: train_loss 3.3624978065490723
Iteration 14: train_loss 3.395446538925171
Iteration 15: train_loss 3.4515726566314697
Iteration 16: train_loss 3.457521915435791
Iteration 17: train_loss 3.4971542358398438
Iteration 18: train_loss 3.5143520832061768
Iteration 19: train_loss 3.587019920349121
Iteration 20: train_loss 3.5817172527313232
Iteration 21: train_loss 3.376368522644043
Iteration 22: train_loss 3.4732167720794678
Iteration 23: train_loss 3.4456911087036133
Iteration 24: train_loss 3.496462821960449
Iteration 25: train_loss 3.460545063018799
Iteration 26: train_loss 3.467664957046509
Iteration 27: train_loss 3.4279301166534424
Iteration 28: train_loss 3.5002498626708984
Iteration 29: train_loss 3.3071014881134033
Iteration 30: train_loss 3.5646145343780518
Iteration 31: train_loss 3.6006646156311035
Iteration 32: train_loss 3.5805649757385254
Iteration 33: train_loss 3.508880615234375
Iteration 34: train_loss 3.522228956222534
Iteration 35: train_loss 3.601940870285034
Iteration 36: train_loss 3.4157140254974365
Iteration 37: train_loss 3.6067910194396973
Iteration 38: train_loss 3.596050977706909
Iteration 39: train_loss 3.6445956230163574
Iteration 40: train_loss 2.839425802230835
Epoch 78: train_avg_loss 3.46457057595253 eval_avg_acc: 0.2107132775489206 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:21:57] [32mIntermediate result: 0.2107132775489206  (Index 77)[0m
================Epoch: 79================
Iteration 1: train_loss 3.7498364448547363
Iteration 2: train_loss 3.544379234313965
Iteration 3: train_loss 3.3493432998657227
Iteration 4: train_loss 3.5750582218170166
Iteration 5: train_loss 3.497187614440918
Iteration 6: train_loss 3.5491037368774414
Iteration 7: train_loss 3.451390266418457
Iteration 8: train_loss 3.3992109298706055
Iteration 9: train_loss 3.436838150024414
Iteration 10: train_loss 3.662437677383423
Iteration 11: train_loss 3.545248508453369
Iteration 12: train_loss 3.489224910736084
Iteration 13: train_loss 3.6680233478546143
Iteration 14: train_loss 3.401540994644165
Iteration 15: train_loss 3.465097427368164
Iteration 16: train_loss 3.3363852500915527
Iteration 17: train_loss 3.4768431186676025
Iteration 18: train_loss 3.435850143432617
Iteration 19: train_loss 3.461662530899048
Iteration 20: train_loss 3.522613763809204
Iteration 21: train_loss 3.3595261573791504
Iteration 22: train_loss 3.363560199737549
Iteration 23: train_loss 3.3914530277252197
Iteration 24: train_loss 3.368903875350952
Iteration 25: train_loss 3.454068660736084
Iteration 26: train_loss 3.444896936416626
Iteration 27: train_loss 3.376412868499756
Iteration 28: train_loss 3.4735348224639893
Iteration 29: train_loss 3.425143003463745
Iteration 30: train_loss 3.5613160133361816
Iteration 31: train_loss 3.4152588844299316
Iteration 32: train_loss 3.4638373851776123
Iteration 33: train_loss 3.324989080429077
Iteration 34: train_loss 3.457448720932007
Iteration 35: train_loss 3.55690336227417
Iteration 36: train_loss 3.531771659851074
Iteration 37: train_loss 3.281583786010742
Iteration 38: train_loss 3.5012834072113037
Iteration 39: train_loss 3.3958239555358887
Iteration 40: train_loss 3.9340293407440186
Epoch 79: train_avg_loss 3.477475517988205 eval_avg_acc: 0.21090251602232132 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:22:10] [32mIntermediate result: 0.21090251602232132  (Index 78)[0m
================Epoch: 80================
Iteration 1: train_loss 3.595749616622925
Iteration 2: train_loss 3.5323214530944824
Iteration 3: train_loss 3.280942678451538
Iteration 4: train_loss 3.4207499027252197
Iteration 5: train_loss 3.478341817855835
Iteration 6: train_loss 3.3516929149627686
Iteration 7: train_loss 3.4940736293792725
Iteration 8: train_loss 3.344616174697876
Iteration 9: train_loss 3.579423427581787
Iteration 10: train_loss 3.4108943939208984
Iteration 11: train_loss 3.3928332328796387
Iteration 12: train_loss 3.604536533355713
Iteration 13: train_loss 3.466191530227661
Iteration 14: train_loss 3.4946651458740234
Iteration 15: train_loss 3.5506651401519775
Iteration 16: train_loss 3.2317686080932617
Iteration 17: train_loss 3.5288004875183105
Iteration 18: train_loss 3.398630142211914
Iteration 19: train_loss 3.630915403366089
Iteration 20: train_loss 3.5042850971221924
Iteration 21: train_loss 3.523770809173584
Iteration 22: train_loss 3.4162940979003906
Iteration 23: train_loss 3.5401947498321533
Iteration 24: train_loss 3.403700113296509
Iteration 25: train_loss 3.50565505027771
Iteration 26: train_loss 3.3924741744995117
Iteration 27: train_loss 3.3110218048095703
Iteration 28: train_loss 3.3214523792266846
Iteration 29: train_loss 3.444488286972046
Iteration 30: train_loss 3.5013856887817383
Iteration 31: train_loss 3.3610825538635254
Iteration 32: train_loss 3.353937864303589
Iteration 33: train_loss 3.576002597808838
Iteration 34: train_loss 3.4561405181884766
Iteration 35: train_loss 3.4532999992370605
Iteration 36: train_loss 3.333106517791748
Iteration 37: train_loss 3.497361660003662
Iteration 38: train_loss 3.4946041107177734
Iteration 39: train_loss 3.6022465229034424
Iteration 40: train_loss 3.1250646114349365
Epoch 80: train_avg_loss 3.447634536027908 eval_avg_acc: 0.20517796460713927 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:22:22] [32mIntermediate result: 0.20517796460713927  (Index 79)[0m
================Epoch: 81================
Iteration 1: train_loss 3.6048974990844727
Iteration 2: train_loss 3.5416295528411865
Iteration 3: train_loss 3.6140716075897217
Iteration 4: train_loss 3.66955304145813
Iteration 5: train_loss 3.5843381881713867
Iteration 6: train_loss 3.6066784858703613
Iteration 7: train_loss 3.5777647495269775
Iteration 8: train_loss 3.394977569580078
Iteration 9: train_loss 3.5651869773864746
Iteration 10: train_loss 3.4710826873779297
Iteration 11: train_loss 3.4660205841064453
Iteration 12: train_loss 3.420417547225952
Iteration 13: train_loss 3.511920213699341
Iteration 14: train_loss 3.565013885498047
Iteration 15: train_loss 3.371873140335083
Iteration 16: train_loss 3.449364185333252
Iteration 17: train_loss 3.366987705230713
Iteration 18: train_loss 3.4058451652526855
Iteration 19: train_loss 3.442981719970703
Iteration 20: train_loss 3.683292865753174
Iteration 21: train_loss 3.3865435123443604
Iteration 22: train_loss 3.399517059326172
Iteration 23: train_loss 3.520801544189453
Iteration 24: train_loss 3.4580678939819336
Iteration 25: train_loss 3.452611207962036
Iteration 26: train_loss 3.399458646774292
Iteration 27: train_loss 3.4753618240356445
Iteration 28: train_loss 3.3752641677856445
Iteration 29: train_loss 3.531378746032715
Iteration 30: train_loss 3.4260363578796387
Iteration 31: train_loss 3.496267080307007
Iteration 32: train_loss 3.3656625747680664
Iteration 33: train_loss 3.4187936782836914
Iteration 34: train_loss 3.4017069339752197
Iteration 35: train_loss 3.4384758472442627
Iteration 36: train_loss 3.499173402786255
Iteration 37: train_loss 3.474560499191284
Iteration 38: train_loss 3.511608362197876
Iteration 39: train_loss 3.4234261512756348
Iteration 40: train_loss 3.3882524967193604
Epoch 81: train_avg_loss 3.4789216339588167 eval_avg_acc: 0.2135528681880424 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:22:33] [32mIntermediate result: 0.2135528681880424  (Index 80)[0m
================Epoch: 82================
Iteration 1: train_loss 3.540086269378662
Iteration 2: train_loss 3.5527148246765137
Iteration 3: train_loss 3.5113067626953125
Iteration 4: train_loss 3.65824031829834
Iteration 5: train_loss 3.548105478286743
Iteration 6: train_loss 3.363027334213257
Iteration 7: train_loss 3.4350600242614746
Iteration 8: train_loss 3.4091413021087646
Iteration 9: train_loss 3.6228578090667725
Iteration 10: train_loss 3.457197904586792
Iteration 11: train_loss 3.4163503646850586
Iteration 12: train_loss 3.4589483737945557
Iteration 13: train_loss 3.498880624771118
Iteration 14: train_loss 3.5702600479125977
Iteration 15: train_loss 3.497199058532715
Iteration 16: train_loss 3.4472053050994873
Iteration 17: train_loss 3.465679407119751
Iteration 18: train_loss 3.505800247192383
Iteration 19: train_loss 3.444058656692505
Iteration 20: train_loss 3.4277632236480713
Iteration 21: train_loss 3.5612268447875977
Iteration 22: train_loss 3.4153623580932617
Iteration 23: train_loss 3.3885388374328613
Iteration 24: train_loss 3.41457462310791
Iteration 25: train_loss 3.3200721740722656
Iteration 26: train_loss 3.421656608581543
Iteration 27: train_loss 3.2990777492523193
Iteration 28: train_loss 3.4403035640716553
Iteration 29: train_loss 3.4374001026153564
Iteration 30: train_loss 3.3941307067871094
Iteration 31: train_loss 3.5387825965881348
Iteration 32: train_loss 3.302182912826538
Iteration 33: train_loss 3.2955853939056396
Iteration 34: train_loss 3.50797700881958
Iteration 35: train_loss 3.4174022674560547
Iteration 36: train_loss 3.362717628479004
Iteration 37: train_loss 3.333024501800537
Iteration 38: train_loss 3.4379422664642334
Iteration 39: train_loss 3.4977543354034424
Iteration 40: train_loss 2.4689629077911377
Epoch 82: train_avg_loss 3.427113968133926 eval_avg_acc: 0.20789445790822608 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:22:46] [32mIntermediate result: 0.20789445790822608  (Index 81)[0m
================Epoch: 83================
Iteration 1: train_loss 3.508655548095703
Iteration 2: train_loss 3.553358316421509
Iteration 3: train_loss 3.350632429122925
Iteration 4: train_loss 3.505180597305298
Iteration 5: train_loss 3.436664342880249
Iteration 6: train_loss 3.7636148929595947
Iteration 7: train_loss 3.398362636566162
Iteration 8: train_loss 3.3310177326202393
Iteration 9: train_loss 3.4301726818084717
Iteration 10: train_loss 3.556810140609741
Iteration 11: train_loss 3.5134289264678955
Iteration 12: train_loss 3.515533685684204
Iteration 13: train_loss 3.5209388732910156
Iteration 14: train_loss 3.3253912925720215
Iteration 15: train_loss 3.376893997192383
Iteration 16: train_loss 3.3210132122039795
Iteration 17: train_loss 3.4377503395080566
Iteration 18: train_loss 3.2659010887145996
Iteration 19: train_loss 3.4206595420837402
Iteration 20: train_loss 3.432986259460449
Iteration 21: train_loss 3.4458703994750977
Iteration 22: train_loss 3.3587493896484375
Iteration 23: train_loss 3.4838693141937256
Iteration 24: train_loss 3.4450697898864746
Iteration 25: train_loss 3.3278818130493164
Iteration 26: train_loss 3.4938008785247803
Iteration 27: train_loss 3.4672179222106934
Iteration 28: train_loss 3.2922093868255615
Iteration 29: train_loss 3.47448468208313
Iteration 30: train_loss 3.374314785003662
Iteration 31: train_loss 3.5394299030303955
Iteration 32: train_loss 3.3430166244506836
Iteration 33: train_loss 3.345452308654785
Iteration 34: train_loss 3.401489496231079
Iteration 35: train_loss 3.4355990886688232
Iteration 36: train_loss 3.504009246826172
Iteration 37: train_loss 3.384356737136841
Iteration 38: train_loss 3.519534111022949
Iteration 39: train_loss 3.5507936477661133
Iteration 40: train_loss 3.328911066055298
Epoch 83: train_avg_loss 3.4370256781578066 eval_avg_acc: 0.21065154854227255 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:22:58] [32mIntermediate result: 0.21065154854227255  (Index 82)[0m
================Epoch: 84================
Iteration 1: train_loss 3.4255530834198
Iteration 2: train_loss 3.361750364303589
Iteration 3: train_loss 3.4236111640930176
Iteration 4: train_loss 3.4247078895568848
Iteration 5: train_loss 3.409006357192993
Iteration 6: train_loss 3.367483377456665
Iteration 7: train_loss 3.387317180633545
Iteration 8: train_loss 3.42517352104187
Iteration 9: train_loss 3.3671207427978516
Iteration 10: train_loss 3.3951361179351807
Iteration 11: train_loss 3.5305161476135254
Iteration 12: train_loss 3.432753562927246
Iteration 13: train_loss 3.3298146724700928
Iteration 14: train_loss 3.4785256385803223
Iteration 15: train_loss 3.5705008506774902
Iteration 16: train_loss 3.371002435684204
Iteration 17: train_loss 3.625774383544922
Iteration 18: train_loss 3.4556174278259277
Iteration 19: train_loss 3.3604676723480225
Iteration 20: train_loss 3.4007253646850586
Iteration 21: train_loss 3.516049385070801
Iteration 22: train_loss 3.419689416885376
Iteration 23: train_loss 3.2986419200897217
Iteration 24: train_loss 3.4521827697753906
Iteration 25: train_loss 3.2962043285369873
Iteration 26: train_loss 3.3484504222869873
Iteration 27: train_loss 3.500382423400879
Iteration 28: train_loss 3.428109884262085
Iteration 29: train_loss 3.578104257583618
Iteration 30: train_loss 3.507964611053467
Iteration 31: train_loss 3.3981804847717285
Iteration 32: train_loss 3.3859989643096924
Iteration 33: train_loss 3.5742733478546143
Iteration 34: train_loss 3.38771390914917
Iteration 35: train_loss 3.387529134750366
Iteration 36: train_loss 3.401489734649658
Iteration 37: train_loss 3.4134490489959717
Iteration 38: train_loss 3.4278464317321777
Iteration 39: train_loss 3.5136806964874268
Iteration 40: train_loss 3.5116817951202393
Epoch 84: train_avg_loss 3.432254523038864 eval_avg_acc: 0.2124520419117118 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:23:10] [32mIntermediate result: 0.2124520419117118  (Index 83)[0m
================Epoch: 85================
Iteration 1: train_loss 3.43607759475708
Iteration 2: train_loss 3.516972541809082
Iteration 3: train_loss 3.2939188480377197
Iteration 4: train_loss 3.40533185005188
Iteration 5: train_loss 3.3494279384613037
Iteration 6: train_loss 3.3641834259033203
Iteration 7: train_loss 3.392868757247925
Iteration 8: train_loss 3.2737324237823486
Iteration 9: train_loss 3.140732765197754
Iteration 10: train_loss 3.357987403869629
Iteration 11: train_loss 3.406092405319214
Iteration 12: train_loss 3.353398561477661
Iteration 13: train_loss 3.395305871963501
Iteration 14: train_loss 3.2611348628997803
Iteration 15: train_loss 3.3118538856506348
Iteration 16: train_loss 3.300520181655884
Iteration 17: train_loss 3.3839077949523926
Iteration 18: train_loss 3.2461190223693848
Iteration 19: train_loss 3.461097478866577
Iteration 20: train_loss 3.442333698272705
Iteration 21: train_loss 3.4481611251831055
Iteration 22: train_loss 3.4220666885375977
Iteration 23: train_loss 3.3958938121795654
Iteration 24: train_loss 3.3868532180786133
Iteration 25: train_loss 3.40490460395813
Iteration 26: train_loss 3.4157023429870605
Iteration 27: train_loss 3.410728931427002
Iteration 28: train_loss 3.4342458248138428
Iteration 29: train_loss 3.4277896881103516
Iteration 30: train_loss 3.404468536376953
Iteration 31: train_loss 3.566848039627075
Iteration 32: train_loss 3.3011066913604736
Iteration 33: train_loss 3.4486658573150635
Iteration 34: train_loss 3.3999500274658203
Iteration 35: train_loss 3.5001142024993896
Iteration 36: train_loss 3.5231833457946777
Iteration 37: train_loss 3.5076029300689697
Iteration 38: train_loss 3.4325106143951416
Iteration 39: train_loss 3.60429310798645
Iteration 40: train_loss 3.599444627761841
Epoch 85: train_avg_loss 3.4031882882118225 eval_avg_acc: 0.21027169994387007 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:23:22] [32mIntermediate result: 0.21027169994387007  (Index 84)[0m
================Epoch: 86================
Iteration 1: train_loss 3.477278470993042
Iteration 2: train_loss 3.4932992458343506
Iteration 3: train_loss 3.3109869956970215
Iteration 4: train_loss 3.4409120082855225
Iteration 5: train_loss 3.370445966720581
Iteration 6: train_loss 3.4513893127441406
Iteration 7: train_loss 3.332028388977051
Iteration 8: train_loss 3.3870723247528076
Iteration 9: train_loss 3.3511271476745605
Iteration 10: train_loss 3.4153144359588623
Iteration 11: train_loss 3.5033280849456787
Iteration 12: train_loss 3.1255581378936768
Iteration 13: train_loss 3.4105656147003174
Iteration 14: train_loss 3.386106014251709
Iteration 15: train_loss 3.2887120246887207
Iteration 16: train_loss 3.35846209526062
Iteration 17: train_loss 3.5091381072998047
Iteration 18: train_loss 3.4243507385253906
Iteration 19: train_loss 3.321375608444214
Iteration 20: train_loss 3.4138338565826416
Iteration 21: train_loss 3.3011653423309326
Iteration 22: train_loss 3.4523463249206543
Iteration 23: train_loss 3.5171592235565186
Iteration 24: train_loss 3.5333759784698486
Iteration 25: train_loss 3.460566759109497
Iteration 26: train_loss 3.4369630813598633
Iteration 27: train_loss 3.353654384613037
Iteration 28: train_loss 3.4367294311523438
Iteration 29: train_loss 3.425952911376953
Iteration 30: train_loss 3.531740427017212
Iteration 31: train_loss 3.3469398021698
Iteration 32: train_loss 3.3846943378448486
Iteration 33: train_loss 3.502361297607422
Iteration 34: train_loss 3.3636598587036133
Iteration 35: train_loss 3.3424065113067627
Iteration 36: train_loss 3.513913154602051
Iteration 37: train_loss 3.5212440490722656
Iteration 38: train_loss 3.3437588214874268
Iteration 39: train_loss 3.4479427337646484
Iteration 40: train_loss 3.0727200508117676
Epoch 86: train_avg_loss 3.4015144765377046 eval_avg_acc: 0.21449229216792656 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:23:35] [32mIntermediate result: 0.21449229216792656  (Index 85)[0m
================Epoch: 87================
Iteration 1: train_loss 3.5173394680023193
Iteration 2: train_loss 3.5041816234588623
Iteration 3: train_loss 3.4880435466766357
Iteration 4: train_loss 3.4990663528442383
Iteration 5: train_loss 3.4796390533447266
Iteration 6: train_loss 3.472682476043701
Iteration 7: train_loss 3.4079337120056152
Iteration 8: train_loss 3.2522988319396973
Iteration 9: train_loss 3.344632387161255
Iteration 10: train_loss 3.392735719680786
Iteration 11: train_loss 3.3756508827209473
Iteration 12: train_loss 3.265825033187866
Iteration 13: train_loss 3.597357749938965
Iteration 14: train_loss 3.445249557495117
Iteration 15: train_loss 3.292356491088867
Iteration 16: train_loss 3.408273220062256
Iteration 17: train_loss 3.2261037826538086
Iteration 18: train_loss 3.311044931411743
Iteration 19: train_loss 3.2980265617370605
Iteration 20: train_loss 3.4341611862182617
Iteration 21: train_loss 3.374220132827759
Iteration 22: train_loss 3.3673524856567383
Iteration 23: train_loss 3.272886276245117
Iteration 24: train_loss 3.4009926319122314
Iteration 25: train_loss 3.446526050567627
Iteration 26: train_loss 3.3747639656066895
Iteration 27: train_loss 3.3026936054229736
Iteration 28: train_loss 3.393036365509033
Iteration 29: train_loss 3.411705255508423
Iteration 30: train_loss 3.321754217147827
Iteration 31: train_loss 3.3880832195281982
Iteration 32: train_loss 3.391122341156006
Iteration 33: train_loss 3.4057776927948
Iteration 34: train_loss 3.2896196842193604
Iteration 35: train_loss 3.4650144577026367
Iteration 36: train_loss 3.5112338066101074
Iteration 37: train_loss 3.387049674987793
Iteration 38: train_loss 3.4185423851013184
Iteration 39: train_loss 3.3869056701660156
Iteration 40: train_loss 3.473301887512207
Epoch 87: train_avg_loss 3.3948796093463898 eval_avg_acc: 0.21047861645971558 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:23:47] [32mIntermediate result: 0.21047861645971558  (Index 86)[0m
================Epoch: 88================
Iteration 1: train_loss 3.4771437644958496
Iteration 2: train_loss 3.543207883834839
Iteration 3: train_loss 3.3109757900238037
Iteration 4: train_loss 3.3678226470947266
Iteration 5: train_loss 3.456088066101074
Iteration 6: train_loss 3.4199507236480713
Iteration 7: train_loss 3.2661821842193604
Iteration 8: train_loss 3.4459187984466553
Iteration 9: train_loss 3.326369047164917
Iteration 10: train_loss 3.4036009311676025
Iteration 11: train_loss 3.4220101833343506
Iteration 12: train_loss 3.292555570602417
Iteration 13: train_loss 3.3538148403167725
Iteration 14: train_loss 3.1945340633392334
Iteration 15: train_loss 3.3663887977600098
Iteration 16: train_loss 3.455697774887085
Iteration 17: train_loss 3.3655970096588135
Iteration 18: train_loss 3.4451725482940674
Iteration 19: train_loss 3.3673789501190186
Iteration 20: train_loss 3.27974271774292
Iteration 21: train_loss 3.3801145553588867
Iteration 22: train_loss 3.257695436477661
Iteration 23: train_loss 3.2568416595458984
Iteration 24: train_loss 3.426851511001587
Iteration 25: train_loss 3.4282917976379395
Iteration 26: train_loss 3.485252857208252
Iteration 27: train_loss 3.46061372756958
Iteration 28: train_loss 3.3082258701324463
Iteration 29: train_loss 3.3669514656066895
Iteration 30: train_loss 3.240194797515869
Iteration 31: train_loss 3.437126398086548
Iteration 32: train_loss 3.301020383834839
Iteration 33: train_loss 3.3855350017547607
Iteration 34: train_loss 3.4170820713043213
Iteration 35: train_loss 3.268854856491089
Iteration 36: train_loss 3.4469404220581055
Iteration 37: train_loss 3.4087347984313965
Iteration 38: train_loss 3.344755172729492
Iteration 39: train_loss 3.3478198051452637
Iteration 40: train_loss 3.7180016040802
Epoch 88: train_avg_loss 3.3811764121055603 eval_avg_acc: 0.21622754423877005 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:24:00] [32mIntermediate result: 0.21622754423877005  (Index 87)[0m
================Epoch: 89================
Iteration 1: train_loss 3.382869005203247
Iteration 2: train_loss 3.3634543418884277
Iteration 3: train_loss 3.5015759468078613
Iteration 4: train_loss 3.3702149391174316
Iteration 5: train_loss 3.329387664794922
Iteration 6: train_loss 3.2707605361938477
Iteration 7: train_loss 3.395418167114258
Iteration 8: train_loss 3.376894950866699
Iteration 9: train_loss 3.275749683380127
Iteration 10: train_loss 3.4568893909454346
Iteration 11: train_loss 3.451878070831299
Iteration 12: train_loss 3.265580177307129
Iteration 13: train_loss 3.2500357627868652
Iteration 14: train_loss 3.499816417694092
Iteration 15: train_loss 3.252911329269409
Iteration 16: train_loss 3.3803036212921143
Iteration 17: train_loss 3.381146192550659
Iteration 18: train_loss 3.3878848552703857
Iteration 19: train_loss 3.3152267932891846
Iteration 20: train_loss 3.360853672027588
Iteration 21: train_loss 3.322331190109253
Iteration 22: train_loss 3.3558461666107178
Iteration 23: train_loss 3.533708095550537
Iteration 24: train_loss 3.326153516769409
Iteration 25: train_loss 3.4438540935516357
Iteration 26: train_loss 3.387005567550659
Iteration 27: train_loss 3.291922092437744
Iteration 28: train_loss 3.442575216293335
Iteration 29: train_loss 3.4750871658325195
Iteration 30: train_loss 3.2948904037475586
Iteration 31: train_loss 3.396411180496216
Iteration 32: train_loss 3.3614871501922607
Iteration 33: train_loss 3.340982675552368
Iteration 34: train_loss 3.3183481693267822
Iteration 35: train_loss 3.1070563793182373
Iteration 36: train_loss 3.2400994300842285
Iteration 37: train_loss 3.3168771266937256
Iteration 38: train_loss 3.2612414360046387
Iteration 39: train_loss 3.408561944961548
Iteration 40: train_loss 3.468388080596924
Epoch 89: train_avg_loss 3.359041965007782 eval_avg_acc: 0.21410196905348838 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:24:12] [32mIntermediate result: 0.21410196905348838  (Index 88)[0m
================Epoch: 90================
Iteration 1: train_loss 3.203273296356201
Iteration 2: train_loss 3.4664878845214844
Iteration 3: train_loss 3.43784499168396
Iteration 4: train_loss 3.414090633392334
Iteration 5: train_loss 3.4854862689971924
Iteration 6: train_loss 3.4207165241241455
Iteration 7: train_loss 3.378072738647461
Iteration 8: train_loss 3.365833282470703
Iteration 9: train_loss 3.35160756111145
Iteration 10: train_loss 3.3839073181152344
Iteration 11: train_loss 3.247887372970581
Iteration 12: train_loss 3.261906147003174
Iteration 13: train_loss 3.348618984222412
Iteration 14: train_loss 3.3952877521514893
Iteration 15: train_loss 3.4060723781585693
Iteration 16: train_loss 3.251248836517334
Iteration 17: train_loss 3.325247049331665
Iteration 18: train_loss 3.297950029373169
Iteration 19: train_loss 3.2599215507507324
Iteration 20: train_loss 3.3556807041168213
Iteration 21: train_loss 3.253034830093384
Iteration 22: train_loss 3.3397066593170166
Iteration 23: train_loss 3.2294437885284424
Iteration 24: train_loss 3.2757222652435303
Iteration 25: train_loss 3.210782289505005
Iteration 26: train_loss 3.2212212085723877
Iteration 27: train_loss 3.2903904914855957
Iteration 28: train_loss 3.381476879119873
Iteration 29: train_loss 3.337419033050537
Iteration 30: train_loss 3.3443028926849365
Iteration 31: train_loss 3.4497053623199463
Iteration 32: train_loss 3.3374640941619873
Iteration 33: train_loss 3.2289648056030273
Iteration 34: train_loss 3.398886203765869
Iteration 35: train_loss 3.4137864112854004
Iteration 36: train_loss 3.3926873207092285
Iteration 37: train_loss 3.424802541732788
Iteration 38: train_loss 3.321536064147949
Iteration 39: train_loss 3.3262124061584473
Iteration 40: train_loss 3.571397304534912
Epoch 90: train_avg_loss 3.3451521039009093 eval_avg_acc: 0.2176638456615898 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:24:24] [32mIntermediate result: 0.2176638456615898  (Index 89)[0m
================Epoch: 91================
Iteration 1: train_loss 3.301257610321045
Iteration 2: train_loss 3.2617597579956055
Iteration 3: train_loss 3.4212396144866943
Iteration 4: train_loss 3.382693290710449
Iteration 5: train_loss 3.3894011974334717
Iteration 6: train_loss 3.426945447921753
Iteration 7: train_loss 3.2838571071624756
Iteration 8: train_loss 3.32312273979187
Iteration 9: train_loss 3.233206272125244
Iteration 10: train_loss 3.482328414916992
Iteration 11: train_loss 3.342568874359131
Iteration 12: train_loss 3.32220721244812
Iteration 13: train_loss 3.349862575531006
Iteration 14: train_loss 3.5021212100982666
Iteration 15: train_loss 3.3178768157958984
Iteration 16: train_loss 3.268328905105591
Iteration 17: train_loss 3.2635419368743896
Iteration 18: train_loss 3.288769245147705
Iteration 19: train_loss 3.492377758026123
Iteration 20: train_loss 3.421268939971924
Iteration 21: train_loss 3.342268228530884
Iteration 22: train_loss 3.38956880569458
Iteration 23: train_loss 3.370157241821289
Iteration 24: train_loss 3.224776029586792
Iteration 25: train_loss 3.251225709915161
Iteration 26: train_loss 3.3693671226501465
Iteration 27: train_loss 3.2273857593536377
Iteration 28: train_loss 3.4722883701324463
Iteration 29: train_loss 3.2640578746795654
Iteration 30: train_loss 3.360405206680298
Iteration 31: train_loss 3.3162403106689453
Iteration 32: train_loss 3.2802019119262695
Iteration 33: train_loss 3.368147373199463
Iteration 34: train_loss 3.3986551761627197
Iteration 35: train_loss 3.368431806564331
Iteration 36: train_loss 3.371407985687256
Iteration 37: train_loss 3.2986488342285156
Iteration 38: train_loss 3.384841203689575
Iteration 39: train_loss 3.3752851486206055
Iteration 40: train_loss 3.491813898086548
Epoch 91: train_avg_loss 3.3499977231025695 eval_avg_acc: 0.21654864737223925 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:24:37] [32mIntermediate result: 0.21654864737223925  (Index 90)[0m
================Epoch: 92================
Iteration 1: train_loss 3.2215287685394287
Iteration 2: train_loss 3.362579822540283
Iteration 3: train_loss 3.2609925270080566
Iteration 4: train_loss 3.12888765335083
Iteration 5: train_loss 3.3204431533813477
Iteration 6: train_loss 3.239699602127075
Iteration 7: train_loss 3.183237075805664
Iteration 8: train_loss 3.1946585178375244
Iteration 9: train_loss 3.3520829677581787
Iteration 10: train_loss 3.2253940105438232
Iteration 11: train_loss 3.2954020500183105
Iteration 12: train_loss 3.1832363605499268
Iteration 13: train_loss 3.372321844100952
Iteration 14: train_loss 3.3531830310821533
Iteration 15: train_loss 3.264730215072632
Iteration 16: train_loss 3.2303004264831543
Iteration 17: train_loss 3.092200756072998
Iteration 18: train_loss 3.272141695022583
Iteration 19: train_loss 3.3182878494262695
Iteration 20: train_loss 3.2735137939453125
Iteration 21: train_loss 3.2643215656280518
Iteration 22: train_loss 3.4243767261505127
Iteration 23: train_loss 3.475311517715454
Iteration 24: train_loss 3.3863823413848877
Iteration 25: train_loss 3.4041988849639893
Iteration 26: train_loss 3.38093638420105
Iteration 27: train_loss 3.2767345905303955
Iteration 28: train_loss 3.330489158630371
Iteration 29: train_loss 3.3807435035705566
Iteration 30: train_loss 3.4612555503845215
Iteration 31: train_loss 3.2926619052886963
Iteration 32: train_loss 3.3339877128601074
Iteration 33: train_loss 3.1835134029388428
Iteration 34: train_loss 3.6027979850769043
Iteration 35: train_loss 3.385279417037964
Iteration 36: train_loss 3.3978092670440674
Iteration 37: train_loss 3.3735904693603516
Iteration 38: train_loss 3.5145697593688965
Iteration 39: train_loss 3.4571633338928223
Iteration 40: train_loss 3.2015817165374756
Epoch 92: train_avg_loss 3.3168131828308107 eval_avg_acc: 0.22003711228982725 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:24:48] [32mIntermediate result: 0.22003711228982725  (Index 91)[0m
================Epoch: 93================
Iteration 1: train_loss 3.297926902770996
Iteration 2: train_loss 3.339553117752075
Iteration 3: train_loss 3.351670503616333
Iteration 4: train_loss 3.2724239826202393
Iteration 5: train_loss 3.411022424697876
Iteration 6: train_loss 3.303668975830078
Iteration 7: train_loss 3.2924389839172363
Iteration 8: train_loss 3.2753593921661377
Iteration 9: train_loss 3.2375309467315674
Iteration 10: train_loss 3.343216896057129
Iteration 11: train_loss 3.2980945110321045
Iteration 12: train_loss 3.320237874984741
Iteration 13: train_loss 3.361426591873169
Iteration 14: train_loss 3.1702258586883545
Iteration 15: train_loss 3.2347683906555176
Iteration 16: train_loss 3.325289249420166
Iteration 17: train_loss 3.3111133575439453
Iteration 18: train_loss 3.1641860008239746
Iteration 19: train_loss 3.27355694770813
Iteration 20: train_loss 3.2212717533111572
Iteration 21: train_loss 3.1798930168151855
Iteration 22: train_loss 3.339034080505371
Iteration 23: train_loss 3.276698350906372
Iteration 24: train_loss 3.3538146018981934
Iteration 25: train_loss 3.260962724685669
Iteration 26: train_loss 3.3992605209350586
Iteration 27: train_loss 3.3680105209350586
Iteration 28: train_loss 3.313474416732788
Iteration 29: train_loss 3.436755418777466
Iteration 30: train_loss 3.2853949069976807
Iteration 31: train_loss 3.3137879371643066
Iteration 32: train_loss 3.3953261375427246
Iteration 33: train_loss 3.290153980255127
Iteration 34: train_loss 3.4307987689971924
Iteration 35: train_loss 3.3850278854370117
Iteration 36: train_loss 3.298769235610962
Iteration 37: train_loss 3.287691593170166
Iteration 38: train_loss 3.2684593200683594
Iteration 39: train_loss 3.402299404144287
Iteration 40: train_loss 3.4442813396453857
Epoch 93: train_avg_loss 3.3133719205856322 eval_avg_acc: 0.22275284503399023 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:25:01] [32mIntermediate result: 0.22275284503399023  (Index 92)[0m
================Epoch: 94================
Iteration 1: train_loss 3.269131898880005
Iteration 2: train_loss 3.241330862045288
Iteration 3: train_loss 3.332435369491577
Iteration 4: train_loss 3.248128652572632
Iteration 5: train_loss 3.3939616680145264
Iteration 6: train_loss 3.104736566543579
Iteration 7: train_loss 3.2464044094085693
Iteration 8: train_loss 3.2728991508483887
Iteration 9: train_loss 3.1255300045013428
Iteration 10: train_loss 3.227614402770996
Iteration 11: train_loss 3.234635591506958
Iteration 12: train_loss 3.2021074295043945
Iteration 13: train_loss 3.2058796882629395
Iteration 14: train_loss 3.206354856491089
Iteration 15: train_loss 3.135781764984131
Iteration 16: train_loss 3.2816083431243896
Iteration 17: train_loss 3.2327215671539307
Iteration 18: train_loss 3.25441837310791
Iteration 19: train_loss 3.2513465881347656
Iteration 20: train_loss 3.1941113471984863
Iteration 21: train_loss 3.146911144256592
Iteration 22: train_loss 3.1791887283325195
Iteration 23: train_loss 3.1979520320892334
Iteration 24: train_loss 3.3429057598114014
Iteration 25: train_loss 3.209867238998413
Iteration 26: train_loss 3.298412322998047
Iteration 27: train_loss 3.321697235107422
Iteration 28: train_loss 3.239633321762085
Iteration 29: train_loss 3.395901679992676
Iteration 30: train_loss 3.2839691638946533
Iteration 31: train_loss 3.316333770751953
Iteration 32: train_loss 3.251034736633301
Iteration 33: train_loss 3.274869203567505
Iteration 34: train_loss 3.3390703201293945
Iteration 35: train_loss 3.1064748764038086
Iteration 36: train_loss 3.255707263946533
Iteration 37: train_loss 3.319093942642212
Iteration 38: train_loss 3.5297393798828125
Iteration 39: train_loss 3.468622922897339
Iteration 40: train_loss 2.77724552154541
Epoch 94: train_avg_loss 3.2478942275047302 eval_avg_acc: 0.2138110561492256 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:25:13] [32mIntermediate result: 0.2138110561492256  (Index 93)[0m
================Epoch: 95================
Iteration 1: train_loss 3.5221197605133057
Iteration 2: train_loss 3.3287956714630127
Iteration 3: train_loss 3.306760549545288
Iteration 4: train_loss 3.3670756816864014
Iteration 5: train_loss 3.221261739730835
Iteration 6: train_loss 3.317626953125
Iteration 7: train_loss 3.2470972537994385
Iteration 8: train_loss 3.2401559352874756
Iteration 9: train_loss 3.312422513961792
Iteration 10: train_loss 3.2819550037384033
Iteration 11: train_loss 3.266618490219116
Iteration 12: train_loss 3.298219680786133
Iteration 13: train_loss 3.2224581241607666
Iteration 14: train_loss 3.286062479019165
Iteration 15: train_loss 3.155909538269043
Iteration 16: train_loss 3.295316457748413
Iteration 17: train_loss 3.3431267738342285
Iteration 18: train_loss 3.3979032039642334
Iteration 19: train_loss 3.32525897026062
Iteration 20: train_loss 3.1668636798858643
Iteration 21: train_loss 3.368335723876953
Iteration 22: train_loss 3.370053768157959
Iteration 23: train_loss 3.303180694580078
Iteration 24: train_loss 3.1769745349884033
Iteration 25: train_loss 3.2553138732910156
Iteration 26: train_loss 3.2820534706115723
Iteration 27: train_loss 3.2689597606658936
Iteration 28: train_loss 3.3907930850982666
Iteration 29: train_loss 3.3929007053375244
Iteration 30: train_loss 3.3351709842681885
Iteration 31: train_loss 3.3826510906219482
Iteration 32: train_loss 3.3509464263916016
Iteration 33: train_loss 3.3379220962524414
Iteration 34: train_loss 3.4054858684539795
Iteration 35: train_loss 3.29422664642334
Iteration 36: train_loss 3.4201228618621826
Iteration 37: train_loss 3.2123937606811523
Iteration 38: train_loss 3.299009323120117
Iteration 39: train_loss 3.2800910472869873
Iteration 40: train_loss 3.279536485671997
Epoch 95: train_avg_loss 3.3077282667160035 eval_avg_acc: 0.21745895785859792 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:25:25] [32mIntermediate result: 0.21745895785859792  (Index 94)[0m
================Epoch: 96================
Iteration 1: train_loss 3.3487966060638428
Iteration 2: train_loss 3.3964381217956543
Iteration 3: train_loss 3.221315383911133
Iteration 4: train_loss 3.420308828353882
Iteration 5: train_loss 3.262021780014038
Iteration 6: train_loss 3.346734046936035
Iteration 7: train_loss 3.2450003623962402
Iteration 8: train_loss 3.2840704917907715
Iteration 9: train_loss 3.289760112762451
Iteration 10: train_loss 3.275599956512451
Iteration 11: train_loss 3.222107172012329
Iteration 12: train_loss 3.2520925998687744
Iteration 13: train_loss 3.2301578521728516
Iteration 14: train_loss 3.441918134689331
Iteration 15: train_loss 3.287330389022827
Iteration 16: train_loss 3.215808153152466
Iteration 17: train_loss 3.4144701957702637
Iteration 18: train_loss 3.298797845840454
Iteration 19: train_loss 3.1068148612976074
Iteration 20: train_loss 3.2406063079833984
Iteration 21: train_loss 3.3476319313049316
Iteration 22: train_loss 3.1997005939483643
Iteration 23: train_loss 3.268071413040161
Iteration 24: train_loss 3.3689138889312744
Iteration 25: train_loss 3.492706537246704
Iteration 26: train_loss 3.2823948860168457
Iteration 27: train_loss 3.318110466003418
Iteration 28: train_loss 3.045053482055664
Iteration 29: train_loss 3.3711163997650146
Iteration 30: train_loss 3.337486505508423
Iteration 31: train_loss 3.245459794998169
Iteration 32: train_loss 3.5090179443359375
Iteration 33: train_loss 3.268737554550171
Iteration 34: train_loss 3.4146535396575928
Iteration 35: train_loss 3.34294056892395
Iteration 36: train_loss 3.361415386199951
Iteration 37: train_loss 3.3299145698547363
Iteration 38: train_loss 3.366522789001465
Iteration 39: train_loss 3.400996685028076
Iteration 40: train_loss 3.4487695693969727
Epoch 96: train_avg_loss 3.3129940927028656 eval_avg_acc: 0.2192167915022992 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:25:38] [32mIntermediate result: 0.2192167915022992  (Index 95)[0m
================Epoch: 97================
Iteration 1: train_loss 3.2896060943603516
Iteration 2: train_loss 3.2103536128997803
Iteration 3: train_loss 3.3565807342529297
Iteration 4: train_loss 3.310636281967163
Iteration 5: train_loss 3.346738576889038
Iteration 6: train_loss 3.345008373260498
Iteration 7: train_loss 3.1610090732574463
Iteration 8: train_loss 3.3354976177215576
Iteration 9: train_loss 3.3907434940338135
Iteration 10: train_loss 3.3880605697631836
Iteration 11: train_loss 3.340996026992798
Iteration 12: train_loss 3.304313898086548
Iteration 13: train_loss 3.4240870475769043
Iteration 14: train_loss 3.447690486907959
Iteration 15: train_loss 3.3713574409484863
Iteration 16: train_loss 3.326382875442505
Iteration 17: train_loss 3.2226250171661377
Iteration 18: train_loss 3.5050647258758545
Iteration 19: train_loss 3.4208521842956543
Iteration 20: train_loss 3.315558910369873
Iteration 21: train_loss 3.2798094749450684
Iteration 22: train_loss 3.398158550262451
Iteration 23: train_loss 3.3539340496063232
Iteration 24: train_loss 3.267531156539917
Iteration 25: train_loss 3.24118709564209
Iteration 26: train_loss 3.3927178382873535
Iteration 27: train_loss 3.2694694995880127
Iteration 28: train_loss 3.190980911254883
Iteration 29: train_loss 3.2502059936523438
Iteration 30: train_loss 3.3678030967712402
Iteration 31: train_loss 3.2304019927978516
Iteration 32: train_loss 3.299631357192993
Iteration 33: train_loss 3.4233198165893555
Iteration 34: train_loss 3.379587173461914
Iteration 35: train_loss 3.301488161087036
Iteration 36: train_loss 3.1400606632232666
Iteration 37: train_loss 3.367042064666748
Iteration 38: train_loss 3.260782480239868
Iteration 39: train_loss 3.114372491836548
Iteration 40: train_loss 3.4488465785980225
Epoch 97: train_avg_loss 3.319762337207794 eval_avg_acc: 0.21472664305549327 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:25:50] [32mIntermediate result: 0.21472664305549327  (Index 96)[0m
================Epoch: 98================
Iteration 1: train_loss 3.2660186290740967
Iteration 2: train_loss 3.375148057937622
Iteration 3: train_loss 3.259211540222168
Iteration 4: train_loss 3.2461740970611572
Iteration 5: train_loss 3.2829854488372803
Iteration 6: train_loss 3.1922473907470703
Iteration 7: train_loss 3.0794103145599365
Iteration 8: train_loss 3.3229808807373047
Iteration 9: train_loss 3.2600021362304688
Iteration 10: train_loss 3.180293321609497
Iteration 11: train_loss 3.226875066757202
Iteration 12: train_loss 3.282646656036377
Iteration 13: train_loss 3.341998338699341
Iteration 14: train_loss 3.253392457962036
Iteration 15: train_loss 3.2274692058563232
Iteration 16: train_loss 3.313807725906372
Iteration 17: train_loss 3.3516790866851807
Iteration 18: train_loss 3.3638405799865723
Iteration 19: train_loss 3.2284839153289795
Iteration 20: train_loss 3.415177583694458
Iteration 21: train_loss 3.4132847785949707
Iteration 22: train_loss 3.4276537895202637
Iteration 23: train_loss 3.3078479766845703
Iteration 24: train_loss 3.196074962615967
Iteration 25: train_loss 3.3873069286346436
Iteration 26: train_loss 3.2244441509246826
Iteration 27: train_loss 3.427114486694336
Iteration 28: train_loss 3.3336710929870605
Iteration 29: train_loss 3.2772903442382812
Iteration 30: train_loss 3.267812728881836
Iteration 31: train_loss 3.4606282711029053
Iteration 32: train_loss 3.273967981338501
Iteration 33: train_loss 3.267848491668701
Iteration 34: train_loss 3.3194427490234375
Iteration 35: train_loss 3.2679460048675537
Iteration 36: train_loss 3.2959530353546143
Iteration 37: train_loss 3.2543773651123047
Iteration 38: train_loss 3.2137351036071777
Iteration 39: train_loss 3.2790801525115967
Iteration 40: train_loss 3.3078622817993164
Epoch 98: train_avg_loss 3.2918296277523043 eval_avg_acc: 0.2174994848108156 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:26:02] [32mIntermediate result: 0.2174994848108156  (Index 97)[0m
================Epoch: 99================
Iteration 1: train_loss 3.40400767326355
Iteration 2: train_loss 3.251192808151245
Iteration 3: train_loss 3.1983797550201416
Iteration 4: train_loss 3.3289904594421387
Iteration 5: train_loss 3.3842532634735107
Iteration 6: train_loss 3.3377106189727783
Iteration 7: train_loss 3.2846016883850098
Iteration 8: train_loss 3.183803081512451
Iteration 9: train_loss 3.4214885234832764
Iteration 10: train_loss 3.2149672508239746
Iteration 11: train_loss 3.288374423980713
Iteration 12: train_loss 3.3049774169921875
Iteration 13: train_loss 3.188612699508667
Iteration 14: train_loss 3.2381796836853027
Iteration 15: train_loss 3.353764295578003
Iteration 16: train_loss 3.216219425201416
Iteration 17: train_loss 3.2922840118408203
Iteration 18: train_loss 3.223336696624756
Iteration 19: train_loss 3.217064142227173
Iteration 20: train_loss 3.214372158050537
Iteration 21: train_loss 3.302140474319458
Iteration 22: train_loss 3.2920305728912354
Iteration 23: train_loss 3.258782148361206
Iteration 24: train_loss 3.301220417022705
Iteration 25: train_loss 3.232217311859131
Iteration 26: train_loss 3.376842498779297
Iteration 27: train_loss 3.2643725872039795
Iteration 28: train_loss 3.258831739425659
Iteration 29: train_loss 3.2556352615356445
Iteration 30: train_loss 3.233247756958008
Iteration 31: train_loss 3.4121322631835938
Iteration 32: train_loss 3.3656458854675293
Iteration 33: train_loss 3.343231201171875
Iteration 34: train_loss 3.2565762996673584
Iteration 35: train_loss 3.427338123321533
Iteration 36: train_loss 3.3061153888702393
Iteration 37: train_loss 3.2563464641571045
Iteration 38: train_loss 3.285169839859009
Iteration 39: train_loss 3.1934852600097656
Iteration 40: train_loss 3.114672899246216
Epoch 99: train_avg_loss 3.282065361738205 eval_avg_acc: 0.21991446032103035 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:26:15] [32mIntermediate result: 0.21991446032103035  (Index 98)[0m
================Epoch: 100================
Iteration 1: train_loss 3.3069300651550293
Iteration 2: train_loss 3.2219672203063965
Iteration 3: train_loss 3.230553388595581
Iteration 4: train_loss 3.2239270210266113
Iteration 5: train_loss 3.2956736087799072
Iteration 6: train_loss 3.2039127349853516
Iteration 7: train_loss 3.149260997772217
Iteration 8: train_loss 3.0452189445495605
Iteration 9: train_loss 3.1914899349212646
Iteration 10: train_loss 3.1492884159088135
Iteration 11: train_loss 3.300969123840332
Iteration 12: train_loss 3.2250711917877197
Iteration 13: train_loss 3.424940347671509
Iteration 14: train_loss 3.283252716064453
Iteration 15: train_loss 3.3139097690582275
Iteration 16: train_loss 3.319690465927124
Iteration 17: train_loss 3.2557528018951416
Iteration 18: train_loss 3.244633674621582
Iteration 19: train_loss 3.396554470062256
Iteration 20: train_loss 3.3173022270202637
Iteration 21: train_loss 3.2069075107574463
Iteration 22: train_loss 3.1984870433807373
Iteration 23: train_loss 3.152379274368286
Iteration 24: train_loss 3.3848533630371094
Iteration 25: train_loss 3.286630392074585
Iteration 26: train_loss 3.2963101863861084
Iteration 27: train_loss 3.189131498336792
Iteration 28: train_loss 3.261854648590088
Iteration 29: train_loss 3.221810817718506
Iteration 30: train_loss 3.239065647125244
Iteration 31: train_loss 3.4557945728302
Iteration 32: train_loss 3.13558030128479
Iteration 33: train_loss 3.2597551345825195
Iteration 34: train_loss 3.150651216506958
Iteration 35: train_loss 3.3998146057128906
Iteration 36: train_loss 3.2534286975860596
Iteration 37: train_loss 3.1486291885375977
Iteration 38: train_loss 3.343418598175049
Iteration 39: train_loss 3.2153685092926025
Iteration 40: train_loss 3.07004714012146
Epoch 100: train_avg_loss 3.249255436658859 eval_avg_acc: 0.2153703043330689 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:26:27] [32mIntermediate result: 0.2153703043330689  (Index 99)[0m
================Epoch: 101================
Iteration 1: train_loss 3.161409616470337
Iteration 2: train_loss 3.3626480102539062
Iteration 3: train_loss 3.2641055583953857
Iteration 4: train_loss 3.1953468322753906
Iteration 5: train_loss 3.353726387023926
Iteration 6: train_loss 3.336411714553833
Iteration 7: train_loss 3.5274343490600586
Iteration 8: train_loss 3.3791329860687256
Iteration 9: train_loss 3.2959015369415283
Iteration 10: train_loss 3.193061113357544
Iteration 11: train_loss 3.136394739151001
Iteration 12: train_loss 3.21195387840271
Iteration 13: train_loss 3.2386538982391357
Iteration 14: train_loss 3.3806698322296143
Iteration 15: train_loss 3.2128114700317383
Iteration 16: train_loss 3.1822152137756348
Iteration 17: train_loss 3.1203932762145996
Iteration 18: train_loss 3.25789737701416
Iteration 19: train_loss 3.2882604598999023
Iteration 20: train_loss 3.3090033531188965
Iteration 21: train_loss 3.0348784923553467
Iteration 22: train_loss 3.1653292179107666
Iteration 23: train_loss 3.255282402038574
Iteration 24: train_loss 3.1564722061157227
Iteration 25: train_loss 3.193108558654785
Iteration 26: train_loss 3.1720404624938965
Iteration 27: train_loss 3.267709493637085
Iteration 28: train_loss 3.3779404163360596
Iteration 29: train_loss 3.242701530456543
Iteration 30: train_loss 3.103365182876587
Iteration 31: train_loss 3.272944688796997
Iteration 32: train_loss 3.2452285289764404
Iteration 33: train_loss 3.292696952819824
Iteration 34: train_loss 3.336737632751465
Iteration 35: train_loss 3.275768518447876
Iteration 36: train_loss 3.234912157058716
Iteration 37: train_loss 3.0471174716949463
Iteration 38: train_loss 3.2791905403137207
Iteration 39: train_loss 3.3383569717407227
Iteration 40: train_loss 3.4268290996551514
Epoch 101: train_avg_loss 3.2531510531902312 eval_avg_acc: 0.21376440277525594 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:26:39] [32mIntermediate result: 0.21376440277525594  (Index 100)[0m
================Epoch: 102================
Iteration 1: train_loss 3.1818246841430664
Iteration 2: train_loss 3.2493457794189453
Iteration 3: train_loss 3.145432949066162
Iteration 4: train_loss 3.2773942947387695
Iteration 5: train_loss 3.376641273498535
Iteration 6: train_loss 3.3089590072631836
Iteration 7: train_loss 3.345916509628296
Iteration 8: train_loss 3.410238742828369
Iteration 9: train_loss 3.2036964893341064
Iteration 10: train_loss 3.227672815322876
Iteration 11: train_loss 3.328472375869751
Iteration 12: train_loss 3.3172051906585693
Iteration 13: train_loss 3.1280829906463623
Iteration 14: train_loss 3.3239312171936035
Iteration 15: train_loss 3.256338119506836
Iteration 16: train_loss 3.2560455799102783
Iteration 17: train_loss 3.453310012817383
Iteration 18: train_loss 3.275831699371338
Iteration 19: train_loss 3.365985870361328
Iteration 20: train_loss 3.3033082485198975
Iteration 21: train_loss 3.254246950149536
Iteration 22: train_loss 3.173980951309204
Iteration 23: train_loss 3.198314666748047
Iteration 24: train_loss 3.2283785343170166
Iteration 25: train_loss 3.1367011070251465
Iteration 26: train_loss 3.166959762573242
Iteration 27: train_loss 3.2945680618286133
Iteration 28: train_loss 3.192502975463867
Iteration 29: train_loss 3.099233627319336
Iteration 30: train_loss 3.084231376647949
Iteration 31: train_loss 3.163496732711792
Iteration 32: train_loss 3.1419031620025635
Iteration 33: train_loss 3.1927900314331055
Iteration 34: train_loss 3.1739349365234375
Iteration 35: train_loss 3.252498149871826
Iteration 36: train_loss 3.337204933166504
Iteration 37: train_loss 3.1121604442596436
Iteration 38: train_loss 3.2152581214904785
Iteration 39: train_loss 3.2600696086883545
Iteration 40: train_loss 3.572469472885132
Epoch 102: train_avg_loss 3.249663436412811 eval_avg_acc: 0.21937450628628388 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:26:51] [32mIntermediate result: 0.21937450628628388  (Index 101)[0m
================Epoch: 103================
Iteration 1: train_loss 3.206460475921631
Iteration 2: train_loss 3.0946598052978516
Iteration 3: train_loss 3.275146484375
Iteration 4: train_loss 3.1908798217773438
Iteration 5: train_loss 3.2094552516937256
Iteration 6: train_loss 3.1218526363372803
Iteration 7: train_loss 3.230506658554077
Iteration 8: train_loss 3.3047139644622803
Iteration 9: train_loss 3.2519731521606445
Iteration 10: train_loss 3.251646041870117
Iteration 11: train_loss 3.2518444061279297
Iteration 12: train_loss 3.1056933403015137
Iteration 13: train_loss 2.990769386291504
Iteration 14: train_loss 3.3058488368988037
Iteration 15: train_loss 3.16544771194458
Iteration 16: train_loss 3.203695058822632
Iteration 17: train_loss 3.1810038089752197
Iteration 18: train_loss 3.132096529006958
Iteration 19: train_loss 3.1832423210144043
Iteration 20: train_loss 3.2297260761260986
Iteration 21: train_loss 3.371593713760376
Iteration 22: train_loss 3.2617859840393066
Iteration 23: train_loss 3.231808662414551
Iteration 24: train_loss 3.2553298473358154
Iteration 25: train_loss 3.2741029262542725
Iteration 26: train_loss 3.248323440551758
Iteration 27: train_loss 3.1682746410369873
Iteration 28: train_loss 3.175445079803467
Iteration 29: train_loss 3.1967825889587402
Iteration 30: train_loss 3.0226383209228516
Iteration 31: train_loss 3.3081612586975098
Iteration 32: train_loss 3.224484443664551
Iteration 33: train_loss 3.1758687496185303
Iteration 34: train_loss 3.148766279220581
Iteration 35: train_loss 3.26369571685791
Iteration 36: train_loss 3.4038538932800293
Iteration 37: train_loss 3.1799402236938477
Iteration 38: train_loss 3.12982439994812
Iteration 39: train_loss 3.1352782249450684
Iteration 40: train_loss 3.01395845413208
Epoch 103: train_avg_loss 3.2019144654273988 eval_avg_acc: 0.2197605725170731 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:27:03] [32mIntermediate result: 0.2197605725170731  (Index 102)[0m
================Epoch: 104================
Iteration 1: train_loss 3.1016993522644043
Iteration 2: train_loss 3.1750805377960205
Iteration 3: train_loss 3.2145960330963135
Iteration 4: train_loss 3.166067361831665
Iteration 5: train_loss 3.2517611980438232
Iteration 6: train_loss 3.2484543323516846
Iteration 7: train_loss 3.427180767059326
Iteration 8: train_loss 3.2095489501953125
Iteration 9: train_loss 3.2698090076446533
Iteration 10: train_loss 3.3562633991241455
Iteration 11: train_loss 3.2110531330108643
Iteration 12: train_loss 3.233166217803955
Iteration 13: train_loss 3.224545955657959
Iteration 14: train_loss 3.1756086349487305
Iteration 15: train_loss 3.265780448913574
Iteration 16: train_loss 3.3879199028015137
Iteration 17: train_loss 3.1937875747680664
Iteration 18: train_loss 3.1509130001068115
Iteration 19: train_loss 3.288100004196167
Iteration 20: train_loss 3.1740989685058594
Iteration 21: train_loss 3.0571765899658203
Iteration 22: train_loss 3.2527389526367188
Iteration 23: train_loss 3.2286574840545654
Iteration 24: train_loss 3.2721190452575684
Iteration 25: train_loss 3.414377212524414
Iteration 26: train_loss 3.2805683612823486
Iteration 27: train_loss 3.289746046066284
Iteration 28: train_loss 3.272078275680542
Iteration 29: train_loss 3.331596612930298
Iteration 30: train_loss 3.246511220932007
Iteration 31: train_loss 3.357767343521118
Iteration 32: train_loss 3.2020504474639893
Iteration 33: train_loss 3.2245168685913086
Iteration 34: train_loss 3.289177417755127
Iteration 35: train_loss 3.387547016143799
Iteration 36: train_loss 3.2370173931121826
Iteration 37: train_loss 3.0756123065948486
Iteration 38: train_loss 3.3440167903900146
Iteration 39: train_loss 3.177201747894287
Iteration 40: train_loss 3.4869728088378906
Epoch 104: train_avg_loss 3.2538221180438995 eval_avg_acc: 0.2217319402310231 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:27:15] [32mIntermediate result: 0.2217319402310231  (Index 103)[0m
================Epoch: 105================
Iteration 1: train_loss 3.2606680393218994
Iteration 2: train_loss 3.287102222442627
Iteration 3: train_loss 3.117267370223999
Iteration 4: train_loss 3.2287416458129883
Iteration 5: train_loss 3.1062216758728027
Iteration 6: train_loss 3.3380415439605713
Iteration 7: train_loss 3.1841273307800293
Iteration 8: train_loss 3.148871660232544
Iteration 9: train_loss 3.1131584644317627
Iteration 10: train_loss 3.1303248405456543
Iteration 11: train_loss 3.182742118835449
Iteration 12: train_loss 3.3049278259277344
Iteration 13: train_loss 3.0945725440979004
Iteration 14: train_loss 3.1861724853515625
Iteration 15: train_loss 3.253803253173828
Iteration 16: train_loss 3.205519437789917
Iteration 17: train_loss 3.2941854000091553
Iteration 18: train_loss 3.2180702686309814
Iteration 19: train_loss 3.294734001159668
Iteration 20: train_loss 3.2364752292633057
Iteration 21: train_loss 3.212587356567383
Iteration 22: train_loss 3.235823631286621
Iteration 23: train_loss 3.29803729057312
Iteration 24: train_loss 3.188657522201538
Iteration 25: train_loss 3.261964797973633
Iteration 26: train_loss 3.194037437438965
Iteration 27: train_loss 3.185903787612915
Iteration 28: train_loss 3.1247737407684326
Iteration 29: train_loss 3.075564384460449
Iteration 30: train_loss 3.09248948097229
Iteration 31: train_loss 3.1449391841888428
Iteration 32: train_loss 3.1902337074279785
Iteration 33: train_loss 3.2818729877471924
Iteration 34: train_loss 3.1344563961029053
Iteration 35: train_loss 2.972332715988159
Iteration 36: train_loss 3.134986639022827
Iteration 37: train_loss 3.200059175491333
Iteration 38: train_loss 3.1385223865509033
Iteration 39: train_loss 3.1571850776672363
Iteration 40: train_loss 3.2119882106781006
Epoch 105: train_avg_loss 3.19055358171463 eval_avg_acc: 0.22157359986146075 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:27:27] [32mIntermediate result: 0.22157359986146075  (Index 104)[0m
================Epoch: 106================
Iteration 1: train_loss 3.2244036197662354
Iteration 2: train_loss 3.061063289642334
Iteration 3: train_loss 3.1742382049560547
Iteration 4: train_loss 3.249346971511841
Iteration 5: train_loss 3.1211230754852295
Iteration 6: train_loss 3.150724172592163
Iteration 7: train_loss 3.1275455951690674
Iteration 8: train_loss 3.4258697032928467
Iteration 9: train_loss 3.2320070266723633
Iteration 10: train_loss 3.1559500694274902
Iteration 11: train_loss 3.2355399131774902
Iteration 12: train_loss 3.1588709354400635
Iteration 13: train_loss 3.1421115398406982
Iteration 14: train_loss 3.116760492324829
Iteration 15: train_loss 3.0737602710723877
Iteration 16: train_loss 3.122657299041748
Iteration 17: train_loss 3.1709513664245605
Iteration 18: train_loss 3.051481008529663
Iteration 19: train_loss 3.130218982696533
Iteration 20: train_loss 2.974496364593506
Iteration 21: train_loss 3.1684694290161133
Iteration 22: train_loss 3.0904664993286133
Iteration 23: train_loss 3.19539737701416
Iteration 24: train_loss 3.062845230102539
Iteration 25: train_loss 3.2271783351898193
Iteration 26: train_loss 3.0879716873168945
Iteration 27: train_loss 3.139526605606079
Iteration 28: train_loss 3.1315484046936035
Iteration 29: train_loss 3.178481340408325
Iteration 30: train_loss 3.2524101734161377
Iteration 31: train_loss 3.0650551319122314
Iteration 32: train_loss 3.240692377090454
Iteration 33: train_loss 3.1580312252044678
Iteration 34: train_loss 3.103499412536621
Iteration 35: train_loss 3.1744070053100586
Iteration 36: train_loss 3.1788995265960693
Iteration 37: train_loss 3.1430492401123047
Iteration 38: train_loss 3.2144792079925537
Iteration 39: train_loss 3.0603318214416504
Iteration 40: train_loss 3.5093441009521484
Epoch 106: train_avg_loss 3.1620301008224487 eval_avg_acc: 0.2195931727822667 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:27:39] [32mIntermediate result: 0.2195931727822667  (Index 105)[0m
================Epoch: 107================
Iteration 1: train_loss 3.2989838123321533
Iteration 2: train_loss 3.370709180831909
Iteration 3: train_loss 3.150067090988159
Iteration 4: train_loss 3.204479932785034
Iteration 5: train_loss 3.1801364421844482
Iteration 6: train_loss 3.05804181098938
Iteration 7: train_loss 3.1911227703094482
Iteration 8: train_loss 3.1436398029327393
Iteration 9: train_loss 3.27168345451355
Iteration 10: train_loss 3.2319962978363037
Iteration 11: train_loss 3.108596086502075
Iteration 12: train_loss 3.2740626335144043
Iteration 13: train_loss 3.238381862640381
Iteration 14: train_loss 3.172677755355835
Iteration 15: train_loss 3.1920006275177
Iteration 16: train_loss 3.1863811016082764
Iteration 17: train_loss 3.2404534816741943
Iteration 18: train_loss 3.011629819869995
Iteration 19: train_loss 3.225316047668457
Iteration 20: train_loss 3.3933019638061523
Iteration 21: train_loss 3.143949270248413
Iteration 22: train_loss 3.129173517227173
Iteration 23: train_loss 3.191662311553955
Iteration 24: train_loss 3.2132375240325928
Iteration 25: train_loss 3.2640180587768555
Iteration 26: train_loss 3.057250738143921
Iteration 27: train_loss 3.1732747554779053
Iteration 28: train_loss 3.1485307216644287
Iteration 29: train_loss 3.096649646759033
Iteration 30: train_loss 3.2563059329986572
Iteration 31: train_loss 3.123974323272705
Iteration 32: train_loss 3.1486170291900635
Iteration 33: train_loss 3.102853775024414
Iteration 34: train_loss 3.2110331058502197
Iteration 35: train_loss 3.194953203201294
Iteration 36: train_loss 3.2608654499053955
Iteration 37: train_loss 3.149000644683838
Iteration 38: train_loss 3.1156466007232666
Iteration 39: train_loss 2.9676573276519775
Iteration 40: train_loss 2.726404905319214
Epoch 107: train_avg_loss 3.1704680204391478 eval_avg_acc: 0.22261984044112665 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:27:51] [32mIntermediate result: 0.22261984044112665  (Index 106)[0m
================Epoch: 108================
Iteration 1: train_loss 3.0637192726135254
Iteration 2: train_loss 3.287574052810669
Iteration 3: train_loss 3.2551074028015137
Iteration 4: train_loss 3.158456325531006
Iteration 5: train_loss 3.1239116191864014
Iteration 6: train_loss 3.1546061038970947
Iteration 7: train_loss 3.062880754470825
Iteration 8: train_loss 3.291334390640259
Iteration 9: train_loss 3.259345769882202
Iteration 10: train_loss 3.2416625022888184
Iteration 11: train_loss 3.1047539710998535
Iteration 12: train_loss 3.2193210124969482
Iteration 13: train_loss 3.1582043170928955
Iteration 14: train_loss 3.1417124271392822
Iteration 15: train_loss 3.1547720432281494
Iteration 16: train_loss 3.2455012798309326
Iteration 17: train_loss 3.065138101577759
Iteration 18: train_loss 3.199028968811035
Iteration 19: train_loss 3.248567581176758
Iteration 20: train_loss 3.182143211364746
Iteration 21: train_loss 3.262648582458496
Iteration 22: train_loss 3.296132802963257
Iteration 23: train_loss 3.2158043384552
Iteration 24: train_loss 3.0996315479278564
Iteration 25: train_loss 3.121396541595459
Iteration 26: train_loss 3.3635942935943604
Iteration 27: train_loss 3.222172260284424
Iteration 28: train_loss 3.289339542388916
Iteration 29: train_loss 3.255854368209839
Iteration 30: train_loss 3.1297192573547363
Iteration 31: train_loss 3.3055901527404785
Iteration 32: train_loss 3.2369468212127686
Iteration 33: train_loss 3.181905508041382
Iteration 34: train_loss 3.2888176441192627
Iteration 35: train_loss 3.195631742477417
Iteration 36: train_loss 3.2968480587005615
Iteration 37: train_loss 3.1609580516815186
Iteration 38: train_loss 3.2625837326049805
Iteration 39: train_loss 3.297776222229004
Iteration 40: train_loss 3.0536322593688965
Epoch 108: train_avg_loss 3.203868120908737 eval_avg_acc: 0.21733346411954826 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:28:03] [32mIntermediate result: 0.21733346411954826  (Index 107)[0m
================Epoch: 109================
Iteration 1: train_loss 3.332786798477173
Iteration 2: train_loss 3.1821582317352295
Iteration 3: train_loss 3.20526123046875
Iteration 4: train_loss 3.1238412857055664
Iteration 5: train_loss 3.004507064819336
Iteration 6: train_loss 3.2246742248535156
Iteration 7: train_loss 3.1960182189941406
Iteration 8: train_loss 3.144416570663452
Iteration 9: train_loss 3.087095260620117
Iteration 10: train_loss 3.040235996246338
Iteration 11: train_loss 3.303558588027954
Iteration 12: train_loss 3.136944532394409
Iteration 13: train_loss 3.214388847351074
Iteration 14: train_loss 3.165705680847168
Iteration 15: train_loss 3.211388349533081
Iteration 16: train_loss 3.0758132934570312
Iteration 17: train_loss 3.1716411113739014
Iteration 18: train_loss 3.0247395038604736
Iteration 19: train_loss 3.1244356632232666
Iteration 20: train_loss 3.008974313735962
Iteration 21: train_loss 3.1051957607269287
Iteration 22: train_loss 3.072296619415283
Iteration 23: train_loss 3.1244773864746094
Iteration 24: train_loss 3.204521656036377
Iteration 25: train_loss 3.1978323459625244
Iteration 26: train_loss 3.057321071624756
Iteration 27: train_loss 3.1773664951324463
Iteration 28: train_loss 3.2493631839752197
Iteration 29: train_loss 3.2290046215057373
Iteration 30: train_loss 3.1864449977874756
Iteration 31: train_loss 3.2285070419311523
Iteration 32: train_loss 3.1062536239624023
Iteration 33: train_loss 3.164992332458496
Iteration 34: train_loss 3.2235350608825684
Iteration 35: train_loss 3.158910036087036
Iteration 36: train_loss 3.2274880409240723
Iteration 37: train_loss 3.1555209159851074
Iteration 38: train_loss 3.181086301803589
Iteration 39: train_loss 3.2072665691375732
Iteration 40: train_loss 2.6801254749298096
Epoch 109: train_avg_loss 3.1479023575782774 eval_avg_acc: 0.22428730453410214 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:28:14] [32mIntermediate result: 0.22428730453410214  (Index 108)[0m
================Epoch: 110================
Iteration 1: train_loss 3.065330982208252
Iteration 2: train_loss 3.2353780269622803
Iteration 3: train_loss 3.196702003479004
Iteration 4: train_loss 2.9806816577911377
Iteration 5: train_loss 3.100783586502075
Iteration 6: train_loss 3.0784597396850586
Iteration 7: train_loss 3.184893846511841
Iteration 8: train_loss 3.1174018383026123
Iteration 9: train_loss 3.1435484886169434
Iteration 10: train_loss 3.319591999053955
Iteration 11: train_loss 3.138005495071411
Iteration 12: train_loss 3.218494415283203
Iteration 13: train_loss 3.1190145015716553
Iteration 14: train_loss 3.1140968799591064
Iteration 15: train_loss 3.0162177085876465
Iteration 16: train_loss 3.0199782848358154
Iteration 17: train_loss 3.1516432762145996
Iteration 18: train_loss 3.133082389831543
Iteration 19: train_loss 3.1623940467834473
Iteration 20: train_loss 3.2363839149475098
Iteration 21: train_loss 3.0812525749206543
Iteration 22: train_loss 3.20546817779541
Iteration 23: train_loss 3.2769479751586914
Iteration 24: train_loss 3.1191513538360596
Iteration 25: train_loss 3.2492473125457764
Iteration 26: train_loss 3.170855760574341
Iteration 27: train_loss 3.0669631958007812
Iteration 28: train_loss 3.105929374694824
Iteration 29: train_loss 3.1467225551605225
Iteration 30: train_loss 3.1999106407165527
Iteration 31: train_loss 3.121277093887329
Iteration 32: train_loss 3.1292648315429688
Iteration 33: train_loss 3.1154417991638184
Iteration 34: train_loss 3.0954387187957764
Iteration 35: train_loss 3.1109585762023926
Iteration 36: train_loss 3.2158374786376953
Iteration 37: train_loss 3.162936210632324
Iteration 38: train_loss 3.1977720260620117
Iteration 39: train_loss 3.28122878074646
Iteration 40: train_loss 3.9510953426361084
Epoch 110: train_avg_loss 3.16839457154274 eval_avg_acc: 0.21737869135934562 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:28:26] [32mIntermediate result: 0.21737869135934562  (Index 109)[0m
================Epoch: 111================
Iteration 1: train_loss 3.135091543197632
Iteration 2: train_loss 3.3184471130371094
Iteration 3: train_loss 3.2945125102996826
Iteration 4: train_loss 3.29606556892395
Iteration 5: train_loss 3.2481658458709717
Iteration 6: train_loss 3.226499080657959
Iteration 7: train_loss 3.0491247177124023
Iteration 8: train_loss 3.1545095443725586
Iteration 9: train_loss 3.2242038249969482
Iteration 10: train_loss 2.974177122116089
Iteration 11: train_loss 3.21389102935791
Iteration 12: train_loss 3.110544204711914
Iteration 13: train_loss 3.226682424545288
Iteration 14: train_loss 3.06432843208313
Iteration 15: train_loss 3.2850396633148193
Iteration 16: train_loss 3.1561367511749268
Iteration 17: train_loss 3.0117759704589844
Iteration 18: train_loss 3.0883593559265137
Iteration 19: train_loss 3.1965341567993164
Iteration 20: train_loss 3.055701732635498
Iteration 21: train_loss 3.0400784015655518
Iteration 22: train_loss 3.2648935317993164
Iteration 23: train_loss 3.1923959255218506
Iteration 24: train_loss 3.1324596405029297
Iteration 25: train_loss 3.242861747741699
Iteration 26: train_loss 3.132331609725952
Iteration 27: train_loss 3.182492733001709
Iteration 28: train_loss 3.1841299533843994
Iteration 29: train_loss 3.202167510986328
Iteration 30: train_loss 3.226797580718994
Iteration 31: train_loss 3.1045219898223877
Iteration 32: train_loss 3.0622782707214355
Iteration 33: train_loss 3.163539171218872
Iteration 34: train_loss 3.2852377891540527
Iteration 35: train_loss 3.0279958248138428
Iteration 36: train_loss 3.112489700317383
Iteration 37: train_loss 3.1459081172943115
Iteration 38: train_loss 3.107084274291992
Iteration 39: train_loss 3.0465173721313477
Iteration 40: train_loss 3.04768705368042
Epoch 111: train_avg_loss 3.1558414697647095 eval_avg_acc: 0.22024671763327533 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:28:38] [32mIntermediate result: 0.22024671763327533  (Index 110)[0m
================Epoch: 112================
Iteration 1: train_loss 3.20439076423645
Iteration 2: train_loss 3.1483399868011475
Iteration 3: train_loss 3.135599374771118
Iteration 4: train_loss 3.1019513607025146
Iteration 5: train_loss 3.1307780742645264
Iteration 6: train_loss 3.2120745182037354
Iteration 7: train_loss 3.0331523418426514
Iteration 8: train_loss 3.1726534366607666
Iteration 9: train_loss 3.1950745582580566
Iteration 10: train_loss 3.1792185306549072
Iteration 11: train_loss 3.1480765342712402
Iteration 12: train_loss 3.0731842517852783
Iteration 13: train_loss 3.1699845790863037
Iteration 14: train_loss 3.225257158279419
Iteration 15: train_loss 3.112863540649414
Iteration 16: train_loss 3.092388391494751
Iteration 17: train_loss 3.0564584732055664
Iteration 18: train_loss 3.0733914375305176
Iteration 19: train_loss 3.1424500942230225
Iteration 20: train_loss 3.1226966381073
Iteration 21: train_loss 3.2465577125549316
Iteration 22: train_loss 3.1833431720733643
Iteration 23: train_loss 3.2388088703155518
Iteration 24: train_loss 3.324213981628418
Iteration 25: train_loss 3.252023220062256
Iteration 26: train_loss 3.273655891418457
Iteration 27: train_loss 3.2583727836608887
Iteration 28: train_loss 3.2085533142089844
Iteration 29: train_loss 3.2668330669403076
Iteration 30: train_loss 3.2837939262390137
Iteration 31: train_loss 2.9919753074645996
Iteration 32: train_loss 3.116814613342285
Iteration 33: train_loss 3.1933298110961914
Iteration 34: train_loss 3.176318645477295
Iteration 35: train_loss 3.1028852462768555
Iteration 36: train_loss 3.2775559425354004
Iteration 37: train_loss 3.1640214920043945
Iteration 38: train_loss 3.0955612659454346
Iteration 39: train_loss 3.145319700241089
Iteration 40: train_loss 3.2295007705688477
Epoch 112: train_avg_loss 3.168985569477081 eval_avg_acc: 0.21824740677199644 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:28:50] [32mIntermediate result: 0.21824740677199644  (Index 111)[0m
================Epoch: 113================
Iteration 1: train_loss 3.138547420501709
Iteration 2: train_loss 3.0828354358673096
Iteration 3: train_loss 3.212456703186035
Iteration 4: train_loss 3.0468966960906982
Iteration 5: train_loss 3.049654722213745
Iteration 6: train_loss 3.262075185775757
Iteration 7: train_loss 3.2142553329467773
Iteration 8: train_loss 3.065378427505493
Iteration 9: train_loss 3.0202572345733643
Iteration 10: train_loss 3.0842642784118652
Iteration 11: train_loss 3.2685189247131348
Iteration 12: train_loss 3.09423828125
Iteration 13: train_loss 3.0450515747070312
Iteration 14: train_loss 3.0832622051239014
Iteration 15: train_loss 3.0752806663513184
Iteration 16: train_loss 3.1175436973571777
Iteration 17: train_loss 3.2196333408355713
Iteration 18: train_loss 3.134629487991333
Iteration 19: train_loss 3.156454563140869
Iteration 20: train_loss 3.167685031890869
Iteration 21: train_loss 3.225024700164795
Iteration 22: train_loss 3.278826951980591
Iteration 23: train_loss 3.2420411109924316
Iteration 24: train_loss 3.0981740951538086
Iteration 25: train_loss 3.164008140563965
Iteration 26: train_loss 3.206892251968384
Iteration 27: train_loss 3.0277528762817383
Iteration 28: train_loss 3.1728296279907227
Iteration 29: train_loss 3.1526033878326416
Iteration 30: train_loss 3.10221266746521
Iteration 31: train_loss 3.0516395568847656
Iteration 32: train_loss 2.9337103366851807
Iteration 33: train_loss 3.228760004043579
Iteration 34: train_loss 3.0808653831481934
Iteration 35: train_loss 3.105696678161621
Iteration 36: train_loss 3.1626298427581787
Iteration 37: train_loss 3.2173962593078613
Iteration 38: train_loss 3.1494269371032715
Iteration 39: train_loss 3.1623754501342773
Iteration 40: train_loss 2.8121838569641113
Epoch 113: train_avg_loss 3.1278492331504824 eval_avg_acc: 0.21808616207280856 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:29:02] [32mIntermediate result: 0.21808616207280856  (Index 112)[0m
================Epoch: 114================
Iteration 1: train_loss 3.258166551589966
Iteration 2: train_loss 3.056393623352051
Iteration 3: train_loss 3.1337392330169678
Iteration 4: train_loss 3.1944119930267334
Iteration 5: train_loss 3.0818073749542236
Iteration 6: train_loss 3.1917169094085693
Iteration 7: train_loss 3.0094547271728516
Iteration 8: train_loss 3.2203710079193115
Iteration 9: train_loss 3.046750068664551
Iteration 10: train_loss 3.1380043029785156
Iteration 11: train_loss 3.1445491313934326
Iteration 12: train_loss 3.3060615062713623
Iteration 13: train_loss 3.207097291946411
Iteration 14: train_loss 3.132744312286377
Iteration 15: train_loss 3.061643600463867
Iteration 16: train_loss 2.973620891571045
Iteration 17: train_loss 3.1764018535614014
Iteration 18: train_loss 3.253654718399048
Iteration 19: train_loss 3.137096881866455
Iteration 20: train_loss 3.1328840255737305
Iteration 21: train_loss 3.2204952239990234
Iteration 22: train_loss 3.2470383644104004
Iteration 23: train_loss 3.1159067153930664
Iteration 24: train_loss 3.020930767059326
Iteration 25: train_loss 3.1026768684387207
Iteration 26: train_loss 3.20444917678833
Iteration 27: train_loss 2.9996628761291504
Iteration 28: train_loss 3.1199653148651123
Iteration 29: train_loss 3.096301794052124
Iteration 30: train_loss 3.0120182037353516
Iteration 31: train_loss 3.171295642852783
Iteration 32: train_loss 3.1379170417785645
Iteration 33: train_loss 3.003265380859375
Iteration 34: train_loss 3.225281000137329
Iteration 35: train_loss 3.0474908351898193
Iteration 36: train_loss 3.1464388370513916
Iteration 37: train_loss 2.9845428466796875
Iteration 38: train_loss 3.1334149837493896
Iteration 39: train_loss 3.2258894443511963
Iteration 40: train_loss 2.949296474456787
Epoch 114: train_avg_loss 3.125521194934845 eval_avg_acc: 0.2217720015673553 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:29:13] [32mIntermediate result: 0.2217720015673553  (Index 113)[0m
================Epoch: 115================
Iteration 1: train_loss 3.1642165184020996
Iteration 2: train_loss 3.1595804691314697
Iteration 3: train_loss 3.062873125076294
Iteration 4: train_loss 3.0912766456604004
Iteration 5: train_loss 3.1825079917907715
Iteration 6: train_loss 3.175868511199951
Iteration 7: train_loss 3.1495583057403564
Iteration 8: train_loss 3.1871695518493652
Iteration 9: train_loss 3.0162148475646973
Iteration 10: train_loss 3.2031428813934326
Iteration 11: train_loss 3.051501989364624
Iteration 12: train_loss 3.0387158393859863
Iteration 13: train_loss 3.0578484535217285
Iteration 14: train_loss 3.0900278091430664
Iteration 15: train_loss 3.154999256134033
Iteration 16: train_loss 2.942859649658203
Iteration 17: train_loss 3.158966064453125
Iteration 18: train_loss 3.0887813568115234
Iteration 19: train_loss 3.0604302883148193
Iteration 20: train_loss 3.267275333404541
Iteration 21: train_loss 3.183551788330078
Iteration 22: train_loss 3.12903094291687
Iteration 23: train_loss 3.240312099456787
Iteration 24: train_loss 3.3342227935791016
Iteration 25: train_loss 3.149097204208374
Iteration 26: train_loss 3.1890718936920166
Iteration 27: train_loss 3.162045955657959
Iteration 28: train_loss 2.9195408821105957
Iteration 29: train_loss 3.002286434173584
Iteration 30: train_loss 3.1258575916290283
Iteration 31: train_loss 2.935488224029541
Iteration 32: train_loss 3.1068379878997803
Iteration 33: train_loss 3.1091670989990234
Iteration 34: train_loss 2.9746878147125244
Iteration 35: train_loss 3.1133077144622803
Iteration 36: train_loss 3.106722831726074
Iteration 37: train_loss 2.9639768600463867
Iteration 38: train_loss 3.168292999267578
Iteration 39: train_loss 3.0244522094726562
Iteration 40: train_loss 2.8652167320251465
Epoch 115: train_avg_loss 3.102674573659897 eval_avg_acc: 0.22450883334268523 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:29:25] [32mIntermediate result: 0.22450883334268523  (Index 114)[0m
================Epoch: 116================
Iteration 1: train_loss 3.1125035285949707
Iteration 2: train_loss 2.998667001724243
Iteration 3: train_loss 3.0725395679473877
Iteration 4: train_loss 3.004244089126587
Iteration 5: train_loss 3.0248403549194336
Iteration 6: train_loss 3.133208990097046
Iteration 7: train_loss 2.9486048221588135
Iteration 8: train_loss 3.1943306922912598
Iteration 9: train_loss 3.0335729122161865
Iteration 10: train_loss 3.0519392490386963
Iteration 11: train_loss 3.1990323066711426
Iteration 12: train_loss 3.0597071647644043
Iteration 13: train_loss 2.9991981983184814
Iteration 14: train_loss 3.1058781147003174
Iteration 15: train_loss 3.1583030223846436
Iteration 16: train_loss 3.110595941543579
Iteration 17: train_loss 3.065925121307373
Iteration 18: train_loss 3.282097816467285
Iteration 19: train_loss 3.0068435668945312
Iteration 20: train_loss 3.0920166969299316
Iteration 21: train_loss 3.082479238510132
Iteration 22: train_loss 3.2687883377075195
Iteration 23: train_loss 3.241851329803467
Iteration 24: train_loss 3.278686285018921
Iteration 25: train_loss 3.146515369415283
Iteration 26: train_loss 3.055737257003784
Iteration 27: train_loss 3.153569459915161
Iteration 28: train_loss 3.170762300491333
Iteration 29: train_loss 3.1835520267486572
Iteration 30: train_loss 2.9836010932922363
Iteration 31: train_loss 3.065054178237915
Iteration 32: train_loss 3.04848313331604
Iteration 33: train_loss 3.119055986404419
Iteration 34: train_loss 2.9661996364593506
Iteration 35: train_loss 2.935079574584961
Iteration 36: train_loss 3.134817361831665
Iteration 37: train_loss 3.1371548175811768
Iteration 38: train_loss 3.130056142807007
Iteration 39: train_loss 3.164045572280884
Iteration 40: train_loss 3.035050630569458
Epoch 116: train_avg_loss 3.098864722251892 eval_avg_acc: 0.2195462368003434 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:29:37] [32mIntermediate result: 0.2195462368003434  (Index 115)[0m
================Epoch: 117================
Iteration 1: train_loss 3.2820334434509277
Iteration 2: train_loss 3.1982362270355225
Iteration 3: train_loss 3.0616209506988525
Iteration 4: train_loss 3.3056206703186035
Iteration 5: train_loss 3.3611037731170654
Iteration 6: train_loss 3.0136754512786865
Iteration 7: train_loss 3.240988254547119
Iteration 8: train_loss 3.0527443885803223
Iteration 9: train_loss 3.061587333679199
Iteration 10: train_loss 3.132753849029541
Iteration 11: train_loss 3.02248477935791
Iteration 12: train_loss 3.106051445007324
Iteration 13: train_loss 3.0867908000946045
Iteration 14: train_loss 3.096831798553467
Iteration 15: train_loss 3.286522626876831
Iteration 16: train_loss 2.9855754375457764
Iteration 17: train_loss 3.207679510116577
Iteration 18: train_loss 3.2170825004577637
Iteration 19: train_loss 3.0914711952209473
Iteration 20: train_loss 3.1178719997406006
Iteration 21: train_loss 3.0916192531585693
Iteration 22: train_loss 3.088487386703491
Iteration 23: train_loss 3.2009034156799316
Iteration 24: train_loss 3.106128215789795
Iteration 25: train_loss 3.0935661792755127
Iteration 26: train_loss 3.1731884479522705
Iteration 27: train_loss 2.9927451610565186
Iteration 28: train_loss 3.102271795272827
Iteration 29: train_loss 3.06097149848938
Iteration 30: train_loss 3.1830201148986816
Iteration 31: train_loss 2.991129159927368
Iteration 32: train_loss 2.939317464828491
Iteration 33: train_loss 3.025953531265259
Iteration 34: train_loss 3.001699924468994
Iteration 35: train_loss 3.1480066776275635
Iteration 36: train_loss 3.0801212787628174
Iteration 37: train_loss 3.1723642349243164
Iteration 38: train_loss 3.109982967376709
Iteration 39: train_loss 3.085512161254883
Iteration 40: train_loss 3.315458059310913
Epoch 117: train_avg_loss 3.122279334068298 eval_avg_acc: 0.2165972956035415 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:29:48] [32mIntermediate result: 0.2165972956035415  (Index 116)[0m
================Epoch: 118================
Iteration 1: train_loss 3.2952589988708496
Iteration 2: train_loss 3.1168975830078125
Iteration 3: train_loss 3.1214678287506104
Iteration 4: train_loss 3.181717872619629
Iteration 5: train_loss 2.986832618713379
Iteration 6: train_loss 3.170337677001953
Iteration 7: train_loss 3.039869546890259
Iteration 8: train_loss 3.116917371749878
Iteration 9: train_loss 3.074108123779297
Iteration 10: train_loss 2.988118886947632
Iteration 11: train_loss 3.1005098819732666
Iteration 12: train_loss 3.2853102684020996
Iteration 13: train_loss 3.052948236465454
Iteration 14: train_loss 3.1339001655578613
Iteration 15: train_loss 3.2399275302886963
Iteration 16: train_loss 2.9891393184661865
Iteration 17: train_loss 3.1608920097351074
Iteration 18: train_loss 3.145962953567505
Iteration 19: train_loss 3.1406540870666504
Iteration 20: train_loss 3.1098155975341797
Iteration 21: train_loss 3.151421070098877
Iteration 22: train_loss 3.1271188259124756
Iteration 23: train_loss 3.031407117843628
Iteration 24: train_loss 3.0784871578216553
Iteration 25: train_loss 3.2092864513397217
Iteration 26: train_loss 3.0218422412872314
Iteration 27: train_loss 3.169264793395996
Iteration 28: train_loss 3.008847236633301
Iteration 29: train_loss 3.0495643615722656
Iteration 30: train_loss 3.0172829627990723
Iteration 31: train_loss 3.024156332015991
Iteration 32: train_loss 3.1928839683532715
Iteration 33: train_loss 3.1691787242889404
Iteration 34: train_loss 3.1805338859558105
Iteration 35: train_loss 3.079535722732544
Iteration 36: train_loss 3.301215648651123
Iteration 37: train_loss 3.160449504852295
Iteration 38: train_loss 3.292546510696411
Iteration 39: train_loss 3.119831085205078
Iteration 40: train_loss 3.3582561016082764
Epoch 118: train_avg_loss 3.129842406511307 eval_avg_acc: 0.21665727536131163 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:29:59] [32mIntermediate result: 0.21665727536131163  (Index 117)[0m
================Epoch: 119================
Iteration 1: train_loss 3.1489312648773193
Iteration 2: train_loss 3.1219482421875
Iteration 3: train_loss 3.0573458671569824
Iteration 4: train_loss 3.0143353939056396
Iteration 5: train_loss 3.2080326080322266
Iteration 6: train_loss 3.1954398155212402
Iteration 7: train_loss 3.118109941482544
Iteration 8: train_loss 3.1578919887542725
Iteration 9: train_loss 3.148310661315918
Iteration 10: train_loss 3.0971601009368896
Iteration 11: train_loss 3.102254867553711
Iteration 12: train_loss 3.058971405029297
Iteration 13: train_loss 3.169485569000244
Iteration 14: train_loss 3.0812013149261475
Iteration 15: train_loss 3.0321426391601562
Iteration 16: train_loss 2.998573064804077
Iteration 17: train_loss 3.059713125228882
Iteration 18: train_loss 3.0511958599090576
Iteration 19: train_loss 3.132357120513916
Iteration 20: train_loss 3.1929800510406494
Iteration 21: train_loss 3.013557195663452
Iteration 22: train_loss 3.076042652130127
Iteration 23: train_loss 3.183971643447876
Iteration 24: train_loss 3.132596254348755
Iteration 25: train_loss 3.0367627143859863
Iteration 26: train_loss 3.1184275150299072
Iteration 27: train_loss 3.122628927230835
Iteration 28: train_loss 3.059694528579712
Iteration 29: train_loss 3.084881544113159
Iteration 30: train_loss 3.2027294635772705
Iteration 31: train_loss 3.0993685722351074
Iteration 32: train_loss 3.0104033946990967
Iteration 33: train_loss 3.112535238265991
Iteration 34: train_loss 3.095262289047241
Iteration 35: train_loss 3.166731119155884
Iteration 36: train_loss 3.0892367362976074
Iteration 37: train_loss 3.1814627647399902
Iteration 38: train_loss 3.161579132080078
Iteration 39: train_loss 3.1030330657958984
Iteration 40: train_loss 2.7556872367858887
Epoch 119: train_avg_loss 3.0988243222236633 eval_avg_acc: 0.21764400339624893 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:30:11] [32mIntermediate result: 0.21764400339624893  (Index 118)[0m
================Epoch: 120================
Iteration 1: train_loss 2.956482410430908
Iteration 2: train_loss 3.074355363845825
Iteration 3: train_loss 3.2375707626342773
Iteration 4: train_loss 3.1874454021453857
Iteration 5: train_loss 3.271944522857666
Iteration 6: train_loss 3.101123332977295
Iteration 7: train_loss 3.144087553024292
Iteration 8: train_loss 3.0944013595581055
Iteration 9: train_loss 3.067694664001465
Iteration 10: train_loss 3.1304073333740234
Iteration 11: train_loss 3.0601484775543213
Iteration 12: train_loss 3.1342732906341553
Iteration 13: train_loss 3.145191192626953
Iteration 14: train_loss 3.212630271911621
Iteration 15: train_loss 3.0894417762756348
Iteration 16: train_loss 3.0748181343078613
Iteration 17: train_loss 3.057572603225708
Iteration 18: train_loss 3.2479870319366455
Iteration 19: train_loss 3.1508054733276367
Iteration 20: train_loss 3.1147525310516357
Iteration 21: train_loss 3.2107245922088623
Iteration 22: train_loss 3.2103464603424072
Iteration 23: train_loss 3.0640265941619873
Iteration 24: train_loss 3.3226754665374756
Iteration 25: train_loss 3.214120626449585
Iteration 26: train_loss 3.0203778743743896
Iteration 27: train_loss 3.2103285789489746
Iteration 28: train_loss 3.111318588256836
Iteration 29: train_loss 3.1658146381378174
Iteration 30: train_loss 3.0560991764068604
Iteration 31: train_loss 3.197765827178955
Iteration 32: train_loss 3.068126678466797
Iteration 33: train_loss 3.057973623275757
Iteration 34: train_loss 3.2136149406433105
Iteration 35: train_loss 2.8626949787139893
Iteration 36: train_loss 3.0208470821380615
Iteration 37: train_loss 3.161299705505371
Iteration 38: train_loss 3.104787826538086
Iteration 39: train_loss 3.1525938510894775
Iteration 40: train_loss 3.6268270015716553
Epoch 120: train_avg_loss 3.140137439966202 eval_avg_acc: 0.21976518467379486 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:30:22] [32mIntermediate result: 0.21976518467379486  (Index 119)[0m
================Epoch: 121================
Iteration 1: train_loss 3.0575649738311768
Iteration 2: train_loss 3.152949810028076
Iteration 3: train_loss 3.1627228260040283
Iteration 4: train_loss 3.058696985244751
Iteration 5: train_loss 3.1910924911499023
Iteration 6: train_loss 3.183035135269165
Iteration 7: train_loss 3.024977684020996
Iteration 8: train_loss 2.9385011196136475
Iteration 9: train_loss 3.02659273147583
Iteration 10: train_loss 3.0238256454467773
Iteration 11: train_loss 3.058621406555176
Iteration 12: train_loss 3.1750693321228027
Iteration 13: train_loss 2.952516794204712
Iteration 14: train_loss 3.027174472808838
Iteration 15: train_loss 3.106574535369873
Iteration 16: train_loss 3.0820109844207764
Iteration 17: train_loss 3.0986416339874268
Iteration 18: train_loss 3.173459529876709
Iteration 19: train_loss 3.113875150680542
Iteration 20: train_loss 3.2314293384552
Iteration 21: train_loss 3.100374698638916
Iteration 22: train_loss 3.0412352085113525
Iteration 23: train_loss 3.2499334812164307
Iteration 24: train_loss 3.0667104721069336
Iteration 25: train_loss 3.0260872840881348
Iteration 26: train_loss 3.2080118656158447
Iteration 27: train_loss 3.0818557739257812
Iteration 28: train_loss 3.068415880203247
Iteration 29: train_loss 3.162238836288452
Iteration 30: train_loss 3.2444064617156982
Iteration 31: train_loss 3.08221697807312
Iteration 32: train_loss 2.876218318939209
Iteration 33: train_loss 3.0451693534851074
Iteration 34: train_loss 3.019420862197876
Iteration 35: train_loss 2.92851185798645
Iteration 36: train_loss 3.0053484439849854
Iteration 37: train_loss 2.9670891761779785
Iteration 38: train_loss 2.9999215602874756
Iteration 39: train_loss 2.9641318321228027
Iteration 40: train_loss 3.0212581157684326
Epoch 121: train_avg_loss 3.0749472260475157 eval_avg_acc: 0.21551990088178158 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:30:33] [32mIntermediate result: 0.21551990088178158  (Index 120)[0m
================Epoch: 122================
Iteration 1: train_loss 3.0522377490997314
Iteration 2: train_loss 3.1121835708618164
Iteration 3: train_loss 3.1584632396698
Iteration 4: train_loss 3.091803789138794
Iteration 5: train_loss 3.1300010681152344
Iteration 6: train_loss 3.1379971504211426
Iteration 7: train_loss 3.1058592796325684
Iteration 8: train_loss 3.130774974822998
Iteration 9: train_loss 3.0048654079437256
Iteration 10: train_loss 3.0914931297302246
Iteration 11: train_loss 2.9392266273498535
Iteration 12: train_loss 3.0171475410461426
Iteration 13: train_loss 2.9912607669830322
Iteration 14: train_loss 3.0885415077209473
Iteration 15: train_loss 3.076998710632324
Iteration 16: train_loss 2.9752585887908936
Iteration 17: train_loss 3.0018396377563477
Iteration 18: train_loss 3.127384662628174
Iteration 19: train_loss 3.105992317199707
Iteration 20: train_loss 3.0642457008361816
Iteration 21: train_loss 3.021455764770508
Iteration 22: train_loss 3.1463427543640137
Iteration 23: train_loss 3.0590853691101074
Iteration 24: train_loss 2.997835874557495
Iteration 25: train_loss 3.000250816345215
Iteration 26: train_loss 3.134206771850586
Iteration 27: train_loss 3.1115634441375732
Iteration 28: train_loss 2.9740021228790283
Iteration 29: train_loss 2.8985490798950195
Iteration 30: train_loss 3.0082180500030518
Iteration 31: train_loss 3.0278005599975586
Iteration 32: train_loss 3.040421485900879
Iteration 33: train_loss 3.066488742828369
Iteration 34: train_loss 3.1598846912384033
Iteration 35: train_loss 3.055630922317505
Iteration 36: train_loss 3.0726544857025146
Iteration 37: train_loss 3.0609817504882812
Iteration 38: train_loss 3.107459545135498
Iteration 39: train_loss 3.0919671058654785
Iteration 40: train_loss 2.52224063873291
Epoch 122: train_avg_loss 3.049015384912491 eval_avg_acc: 0.22153033548035225 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:30:44] [32mIntermediate result: 0.22153033548035225  (Index 121)[0m
================Epoch: 123================
Iteration 1: train_loss 2.946258306503296
Iteration 2: train_loss 3.229055404663086
Iteration 3: train_loss 3.1402218341827393
Iteration 4: train_loss 3.037179470062256
Iteration 5: train_loss 3.0971858501434326
Iteration 6: train_loss 3.0225679874420166
Iteration 7: train_loss 3.0673511028289795
Iteration 8: train_loss 3.1679599285125732
Iteration 9: train_loss 3.0475547313690186
Iteration 10: train_loss 3.083653688430786
Iteration 11: train_loss 3.150567054748535
Iteration 12: train_loss 3.1759722232818604
Iteration 13: train_loss 3.0466103553771973
Iteration 14: train_loss 3.065140962600708
Iteration 15: train_loss 3.0295562744140625
Iteration 16: train_loss 3.0157437324523926
Iteration 17: train_loss 3.116170883178711
Iteration 18: train_loss 3.0614466667175293
Iteration 19: train_loss 3.0217080116271973
Iteration 20: train_loss 3.2039315700531006
Iteration 21: train_loss 3.230196714401245
Iteration 22: train_loss 3.027348518371582
Iteration 23: train_loss 3.1134605407714844
Iteration 24: train_loss 3.1648993492126465
Iteration 25: train_loss 2.9918181896209717
Iteration 26: train_loss 2.99188232421875
Iteration 27: train_loss 3.052324056625366
Iteration 28: train_loss 3.1207797527313232
Iteration 29: train_loss 3.125338554382324
Iteration 30: train_loss 3.094046115875244
Iteration 31: train_loss 3.0776166915893555
Iteration 32: train_loss 3.1003763675689697
Iteration 33: train_loss 3.0456154346466064
Iteration 34: train_loss 2.9440126419067383
Iteration 35: train_loss 3.1098380088806152
Iteration 36: train_loss 3.015069007873535
Iteration 37: train_loss 2.9977588653564453
Iteration 38: train_loss 3.198335886001587
Iteration 39: train_loss 3.1123738288879395
Iteration 40: train_loss 2.524228096008301
Epoch 123: train_avg_loss 3.069078874588013 eval_avg_acc: 0.21672796454928095 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:30:56] [32mIntermediate result: 0.21672796454928095  (Index 122)[0m
================Epoch: 124================
Iteration 1: train_loss 3.257556438446045
Iteration 2: train_loss 3.280822277069092
Iteration 3: train_loss 3.109325885772705
Iteration 4: train_loss 3.1582717895507812
Iteration 5: train_loss 3.1333982944488525
Iteration 6: train_loss 3.178490400314331
Iteration 7: train_loss 3.1303820610046387
Iteration 8: train_loss 2.9646551609039307
Iteration 9: train_loss 3.0705161094665527
Iteration 10: train_loss 2.9897541999816895
Iteration 11: train_loss 2.935044765472412
Iteration 12: train_loss 3.0761289596557617
Iteration 13: train_loss 3.042269468307495
Iteration 14: train_loss 3.0622644424438477
Iteration 15: train_loss 3.013371467590332
Iteration 16: train_loss 3.1234378814697266
Iteration 17: train_loss 3.085658073425293
Iteration 18: train_loss 3.1076138019561768
Iteration 19: train_loss 3.137700319290161
Iteration 20: train_loss 3.0346531867980957
Iteration 21: train_loss 2.866008996963501
Iteration 22: train_loss 2.962939500808716
Iteration 23: train_loss 3.0916812419891357
Iteration 24: train_loss 2.8764467239379883
Iteration 25: train_loss 2.9466984272003174
Iteration 26: train_loss 2.898080825805664
Iteration 27: train_loss 2.8912501335144043
Iteration 28: train_loss 3.0118467807769775
Iteration 29: train_loss 3.0699753761291504
Iteration 30: train_loss 3.0180587768554688
Iteration 31: train_loss 2.949333667755127
Iteration 32: train_loss 3.011577844619751
Iteration 33: train_loss 3.0582289695739746
Iteration 34: train_loss 3.1120104789733887
Iteration 35: train_loss 3.078564167022705
Iteration 36: train_loss 3.063406467437744
Iteration 37: train_loss 2.985445976257324
Iteration 38: train_loss 2.9781646728515625
Iteration 39: train_loss 3.1250033378601074
Iteration 40: train_loss 3.4870383739471436
Epoch 124: train_avg_loss 3.059326893091202 eval_avg_acc: 0.22153721917488442 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:31:07] [32mIntermediate result: 0.22153721917488442  (Index 123)[0m
================Epoch: 125================
Iteration 1: train_loss 2.858992338180542
Iteration 2: train_loss 2.9352777004241943
Iteration 3: train_loss 3.0986380577087402
Iteration 4: train_loss 3.0374364852905273
Iteration 5: train_loss 3.035611867904663
Iteration 6: train_loss 3.1154592037200928
Iteration 7: train_loss 3.0550754070281982
Iteration 8: train_loss 3.006671667098999
Iteration 9: train_loss 3.0310468673706055
Iteration 10: train_loss 3.116572141647339
Iteration 11: train_loss 3.1238796710968018
Iteration 12: train_loss 3.057241439819336
Iteration 13: train_loss 2.8933279514312744
Iteration 14: train_loss 3.022038221359253
Iteration 15: train_loss 3.2407259941101074
Iteration 16: train_loss 3.002938985824585
Iteration 17: train_loss 3.148124933242798
Iteration 18: train_loss 3.0290541648864746
Iteration 19: train_loss 3.003269910812378
Iteration 20: train_loss 3.07243013381958
Iteration 21: train_loss 3.095978260040283
Iteration 22: train_loss 3.0023856163024902
Iteration 23: train_loss 3.0208020210266113
Iteration 24: train_loss 3.1787965297698975
Iteration 25: train_loss 3.2476003170013428
Iteration 26: train_loss 3.208569288253784
Iteration 27: train_loss 3.095705986022949
Iteration 28: train_loss 3.1912882328033447
Iteration 29: train_loss 3.185473918914795
Iteration 30: train_loss 3.007197380065918
Iteration 31: train_loss 3.131831407546997
Iteration 32: train_loss 3.346634864807129
Iteration 33: train_loss 3.005239725112915
Iteration 34: train_loss 3.03257417678833
Iteration 35: train_loss 3.005171775817871
Iteration 36: train_loss 3.2258880138397217
Iteration 37: train_loss 3.121997117996216
Iteration 38: train_loss 2.991840362548828
Iteration 39: train_loss 3.1714537143707275
Iteration 40: train_loss 2.6606106758117676
Epoch 125: train_avg_loss 3.07027131319046 eval_avg_acc: 0.22269862137656776 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:31:18] [32mIntermediate result: 0.22269862137656776  (Index 124)[0m
================Epoch: 126================
Iteration 1: train_loss 3.0388567447662354
Iteration 2: train_loss 3.05153489112854
Iteration 3: train_loss 3.1014459133148193
Iteration 4: train_loss 3.14555287361145
Iteration 5: train_loss 3.0983824729919434
Iteration 6: train_loss 3.0442581176757812
Iteration 7: train_loss 2.9592742919921875
Iteration 8: train_loss 3.042640447616577
Iteration 9: train_loss 3.0177853107452393
Iteration 10: train_loss 3.159517526626587
Iteration 11: train_loss 3.1418728828430176
Iteration 12: train_loss 2.956944465637207
Iteration 13: train_loss 2.895373821258545
Iteration 14: train_loss 2.9249589443206787
Iteration 15: train_loss 2.985111951828003
Iteration 16: train_loss 3.1014809608459473
Iteration 17: train_loss 2.920527458190918
Iteration 18: train_loss 2.9052600860595703
Iteration 19: train_loss 2.98720645904541
Iteration 20: train_loss 2.9805383682250977
Iteration 21: train_loss 3.041598320007324
Iteration 22: train_loss 3.097872734069824
Iteration 23: train_loss 3.119112491607666
Iteration 24: train_loss 3.077143669128418
Iteration 25: train_loss 3.017261266708374
Iteration 26: train_loss 3.007964849472046
Iteration 27: train_loss 3.2392401695251465
Iteration 28: train_loss 3.0945353507995605
Iteration 29: train_loss 3.013301372528076
Iteration 30: train_loss 3.0416998863220215
Iteration 31: train_loss 3.0187761783599854
Iteration 32: train_loss 3.046482801437378
Iteration 33: train_loss 3.0654563903808594
Iteration 34: train_loss 3.019807815551758
Iteration 35: train_loss 3.0135200023651123
Iteration 36: train_loss 3.014080047607422
Iteration 37: train_loss 2.93412446975708
Iteration 38: train_loss 3.069904327392578
Iteration 39: train_loss 3.094381809234619
Iteration 40: train_loss 2.5364978313446045
Epoch 126: train_avg_loss 3.02553214430809 eval_avg_acc: 0.21935235305717599 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:31:29] [32mIntermediate result: 0.21935235305717599  (Index 125)[0m
================Epoch: 127================
Iteration 1: train_loss 3.106084108352661
Iteration 2: train_loss 3.003878355026245
Iteration 3: train_loss 3.045658588409424
Iteration 4: train_loss 2.967661142349243
Iteration 5: train_loss 3.067805767059326
Iteration 6: train_loss 3.0055248737335205
Iteration 7: train_loss 3.022918224334717
Iteration 8: train_loss 3.0966219902038574
Iteration 9: train_loss 3.0842387676239014
Iteration 10: train_loss 3.0046863555908203
Iteration 11: train_loss 3.0200185775756836
Iteration 12: train_loss 3.107985734939575
Iteration 13: train_loss 3.114163637161255
Iteration 14: train_loss 3.0430197715759277
Iteration 15: train_loss 3.0341553688049316
Iteration 16: train_loss 3.1874196529388428
Iteration 17: train_loss 3.005061149597168
Iteration 18: train_loss 3.0785670280456543
Iteration 19: train_loss 3.012068748474121
Iteration 20: train_loss 2.9679770469665527
Iteration 21: train_loss 3.070695161819458
Iteration 22: train_loss 3.194282054901123
Iteration 23: train_loss 2.9560484886169434
Iteration 24: train_loss 3.0326030254364014
Iteration 25: train_loss 3.0095465183258057
Iteration 26: train_loss 3.159287691116333
Iteration 27: train_loss 2.922097682952881
Iteration 28: train_loss 3.0640265941619873
Iteration 29: train_loss 3.048197031021118
Iteration 30: train_loss 2.944857597351074
Iteration 31: train_loss 2.9880032539367676
Iteration 32: train_loss 3.086805820465088
Iteration 33: train_loss 3.1370105743408203
Iteration 34: train_loss 3.1176869869232178
Iteration 35: train_loss 3.126657009124756
Iteration 36: train_loss 3.1408817768096924
Iteration 37: train_loss 3.229804277420044
Iteration 38: train_loss 3.1104235649108887
Iteration 39: train_loss 3.163480758666992
Iteration 40: train_loss 2.948087215423584
Epoch 127: train_avg_loss 3.06064994931221 eval_avg_acc: 0.2182886617566265 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:31:40] [32mIntermediate result: 0.2182886617566265  (Index 126)[0m
================Epoch: 128================
Iteration 1: train_loss 3.011035203933716
Iteration 2: train_loss 3.032243490219116
Iteration 3: train_loss 3.0175836086273193
Iteration 4: train_loss 3.044346809387207
Iteration 5: train_loss 3.0301904678344727
Iteration 6: train_loss 3.0391931533813477
Iteration 7: train_loss 2.975132465362549
Iteration 8: train_loss 2.9195005893707275
Iteration 9: train_loss 3.035943031311035
Iteration 10: train_loss 3.033296823501587
Iteration 11: train_loss 3.016386032104492
Iteration 12: train_loss 3.0090198516845703
Iteration 13: train_loss 3.063892126083374
Iteration 14: train_loss 2.9757583141326904
Iteration 15: train_loss 3.123593330383301
Iteration 16: train_loss 3.030665159225464
Iteration 17: train_loss 3.1093969345092773
Iteration 18: train_loss 2.9867987632751465
Iteration 19: train_loss 2.997284173965454
Iteration 20: train_loss 3.0515458583831787
Iteration 21: train_loss 3.100541353225708
Iteration 22: train_loss 2.9511477947235107
Iteration 23: train_loss 3.0570805072784424
Iteration 24: train_loss 3.0468552112579346
Iteration 25: train_loss 3.1518633365631104
Iteration 26: train_loss 3.0393927097320557
Iteration 27: train_loss 3.0662150382995605
Iteration 28: train_loss 3.0028979778289795
Iteration 29: train_loss 3.065298080444336
Iteration 30: train_loss 3.173205614089966
Iteration 31: train_loss 3.0901689529418945
Iteration 32: train_loss 3.069249153137207
Iteration 33: train_loss 3.050541877746582
Iteration 34: train_loss 3.1497018337249756
Iteration 35: train_loss 3.138301134109497
Iteration 36: train_loss 3.0013444423675537
Iteration 37: train_loss 3.1236133575439453
Iteration 38: train_loss 3.1056902408599854
Iteration 39: train_loss 3.105395555496216
Iteration 40: train_loss 2.8032591342926025
Epoch 128: train_avg_loss 3.044864237308502 eval_avg_acc: 0.2225166888263908 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:31:52] [32mIntermediate result: 0.2225166888263908  (Index 127)[0m
================Epoch: 129================
Iteration 1: train_loss 3.0535426139831543
Iteration 2: train_loss 3.0517756938934326
Iteration 3: train_loss 3.0636260509490967
Iteration 4: train_loss 3.1016156673431396
Iteration 5: train_loss 3.0058727264404297
Iteration 6: train_loss 3.1746134757995605
Iteration 7: train_loss 3.004300832748413
Iteration 8: train_loss 3.22038197517395
Iteration 9: train_loss 3.0884361267089844
Iteration 10: train_loss 3.092453718185425
Iteration 11: train_loss 2.9219698905944824
Iteration 12: train_loss 3.0958547592163086
Iteration 13: train_loss 3.1811628341674805
Iteration 14: train_loss 2.9094009399414062
Iteration 15: train_loss 3.21193265914917
Iteration 16: train_loss 3.111599922180176
Iteration 17: train_loss 3.1906564235687256
Iteration 18: train_loss 2.898451089859009
Iteration 19: train_loss 2.938546895980835
Iteration 20: train_loss 2.939939498901367
Iteration 21: train_loss 3.1067192554473877
Iteration 22: train_loss 3.0202879905700684
Iteration 23: train_loss 2.938483953475952
Iteration 24: train_loss 3.0726137161254883
Iteration 25: train_loss 3.061249017715454
Iteration 26: train_loss 3.0202407836914062
Iteration 27: train_loss 2.957301139831543
Iteration 28: train_loss 3.0011255741119385
Iteration 29: train_loss 2.930156946182251
Iteration 30: train_loss 2.8906443119049072
Iteration 31: train_loss 3.1350209712982178
Iteration 32: train_loss 2.974794387817383
Iteration 33: train_loss 3.070779800415039
Iteration 34: train_loss 2.9948654174804688
Iteration 35: train_loss 2.9382457733154297
Iteration 36: train_loss 3.100926399230957
Iteration 37: train_loss 3.0654304027557373
Iteration 38: train_loss 3.0503737926483154
Iteration 39: train_loss 2.9786527156829834
Iteration 40: train_loss 3.0740044116973877
Epoch 129: train_avg_loss 3.0409512639045717 eval_avg_acc: 0.2265338560051639 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:32:03] [32mIntermediate result: 0.2265338560051639  (Index 128)[0m
================Epoch: 130================
Iteration 1: train_loss 2.9729464054107666
Iteration 2: train_loss 3.0666844844818115
Iteration 3: train_loss 2.8921937942504883
Iteration 4: train_loss 2.9348721504211426
Iteration 5: train_loss 3.005336284637451
Iteration 6: train_loss 3.064401388168335
Iteration 7: train_loss 2.8219428062438965
Iteration 8: train_loss 2.833232879638672
Iteration 9: train_loss 2.8999035358428955
Iteration 10: train_loss 2.9063854217529297
Iteration 11: train_loss 2.9890003204345703
Iteration 12: train_loss 2.96799635887146
Iteration 13: train_loss 2.9443113803863525
Iteration 14: train_loss 2.9746577739715576
Iteration 15: train_loss 2.992805004119873
Iteration 16: train_loss 3.0282061100006104
Iteration 17: train_loss 3.0439555644989014
Iteration 18: train_loss 3.1656627655029297
Iteration 19: train_loss 3.035861015319824
Iteration 20: train_loss 2.944037675857544
Iteration 21: train_loss 3.0520901679992676
Iteration 22: train_loss 3.0494329929351807
Iteration 23: train_loss 3.010446071624756
Iteration 24: train_loss 2.8917720317840576
Iteration 25: train_loss 3.1701791286468506
Iteration 26: train_loss 3.113800287246704
Iteration 27: train_loss 3.1114110946655273
Iteration 28: train_loss 3.025808811187744
Iteration 29: train_loss 2.88200306892395
Iteration 30: train_loss 3.053109884262085
Iteration 31: train_loss 3.0498604774475098
Iteration 32: train_loss 2.994447708129883
Iteration 33: train_loss 3.165015697479248
Iteration 34: train_loss 2.9689700603485107
Iteration 35: train_loss 3.0592188835144043
Iteration 36: train_loss 3.068470001220703
Iteration 37: train_loss 3.006375551223755
Iteration 38: train_loss 3.0464906692504883
Iteration 39: train_loss 3.0905685424804688
Iteration 40: train_loss 2.427314043045044
Epoch 130: train_avg_loss 2.993029457330704 eval_avg_acc: 0.22373638327772127 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:32:14] [32mIntermediate result: 0.22373638327772127  (Index 129)[0m
================Epoch: 131================
Iteration 1: train_loss 2.893127202987671
Iteration 2: train_loss 3.0607383251190186
Iteration 3: train_loss 3.1116268634796143
Iteration 4: train_loss 2.9678492546081543
Iteration 5: train_loss 2.950505256652832
Iteration 6: train_loss 3.0317351818084717
Iteration 7: train_loss 2.99442458152771
Iteration 8: train_loss 2.9384686946868896
Iteration 9: train_loss 3.0965495109558105
Iteration 10: train_loss 2.9514219760894775
Iteration 11: train_loss 3.0172553062438965
Iteration 12: train_loss 3.054675817489624
Iteration 13: train_loss 2.859011173248291
Iteration 14: train_loss 2.8787312507629395
Iteration 15: train_loss 2.8449361324310303
Iteration 16: train_loss 2.9949264526367188
Iteration 17: train_loss 2.861602544784546
Iteration 18: train_loss 3.105924367904663
Iteration 19: train_loss 2.9431750774383545
Iteration 20: train_loss 2.89819598197937
Iteration 21: train_loss 2.9603989124298096
Iteration 22: train_loss 3.058305501937866
Iteration 23: train_loss 2.9433467388153076
Iteration 24: train_loss 3.125483989715576
Iteration 25: train_loss 3.0591187477111816
Iteration 26: train_loss 3.0861430168151855
Iteration 27: train_loss 3.0744004249572754
Iteration 28: train_loss 3.055502414703369
Iteration 29: train_loss 3.0700125694274902
Iteration 30: train_loss 3.109666109085083
Iteration 31: train_loss 2.966506004333496
Iteration 32: train_loss 2.9991886615753174
Iteration 33: train_loss 2.976461172103882
Iteration 34: train_loss 2.9598240852355957
Iteration 35: train_loss 3.0154998302459717
Iteration 36: train_loss 3.002244472503662
Iteration 37: train_loss 3.266345977783203
Iteration 38: train_loss 3.0849313735961914
Iteration 39: train_loss 3.0612246990203857
Iteration 40: train_loss 3.027553081512451
Epoch 131: train_avg_loss 3.0089259684085845 eval_avg_acc: 0.2207863090811582 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:32:26] [32mIntermediate result: 0.2207863090811582  (Index 130)[0m
================Epoch: 132================
Iteration 1: train_loss 3.0132346153259277
Iteration 2: train_loss 3.1325769424438477
Iteration 3: train_loss 3.0381975173950195
Iteration 4: train_loss 3.1726138591766357
Iteration 5: train_loss 2.9145781993865967
Iteration 6: train_loss 3.1792514324188232
Iteration 7: train_loss 2.948997974395752
Iteration 8: train_loss 3.0083937644958496
Iteration 9: train_loss 3.0384230613708496
Iteration 10: train_loss 2.947617530822754
Iteration 11: train_loss 3.119380235671997
Iteration 12: train_loss 2.956336498260498
Iteration 13: train_loss 3.0005605220794678
Iteration 14: train_loss 2.9745988845825195
Iteration 15: train_loss 2.9298832416534424
Iteration 16: train_loss 3.0454232692718506
Iteration 17: train_loss 3.090785264968872
Iteration 18: train_loss 3.12503981590271
Iteration 19: train_loss 3.0049057006835938
Iteration 20: train_loss 2.9774022102355957
Iteration 21: train_loss 3.1442298889160156
Iteration 22: train_loss 3.0370659828186035
Iteration 23: train_loss 3.08557391166687
Iteration 24: train_loss 3.1363251209259033
Iteration 25: train_loss 3.088010549545288
Iteration 26: train_loss 3.114328145980835
Iteration 27: train_loss 3.133145570755005
Iteration 28: train_loss 3.128356695175171
Iteration 29: train_loss 3.002164363861084
Iteration 30: train_loss 2.860436201095581
Iteration 31: train_loss 2.9723927974700928
Iteration 32: train_loss 3.018738031387329
Iteration 33: train_loss 2.955501079559326
Iteration 34: train_loss 2.8950531482696533
Iteration 35: train_loss 2.958428382873535
Iteration 36: train_loss 2.942758083343506
Iteration 37: train_loss 3.000317096710205
Iteration 38: train_loss 2.8598666191101074
Iteration 39: train_loss 2.993332862854004
Iteration 40: train_loss 3.125248908996582
Epoch 132: train_avg_loss 3.0267368495464324 eval_avg_acc: 0.22350058248933383 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:32:37] [32mIntermediate result: 0.22350058248933383  (Index 131)[0m
================Epoch: 133================
Iteration 1: train_loss 2.877732992172241
Iteration 2: train_loss 2.943213939666748
Iteration 3: train_loss 3.0447914600372314
Iteration 4: train_loss 3.151620864868164
Iteration 5: train_loss 3.0207488536834717
Iteration 6: train_loss 3.075268507003784
Iteration 7: train_loss 2.9813196659088135
Iteration 8: train_loss 2.9475715160369873
Iteration 9: train_loss 2.8301429748535156
Iteration 10: train_loss 3.0502359867095947
Iteration 11: train_loss 3.040255546569824
Iteration 12: train_loss 3.1233861446380615
Iteration 13: train_loss 2.865589141845703
Iteration 14: train_loss 2.9732112884521484
Iteration 15: train_loss 3.0347578525543213
Iteration 16: train_loss 2.957772970199585
Iteration 17: train_loss 2.9831931591033936
Iteration 18: train_loss 3.128748655319214
Iteration 19: train_loss 2.9995133876800537
Iteration 20: train_loss 2.900742769241333
Iteration 21: train_loss 2.953855514526367
Iteration 22: train_loss 3.0037717819213867
Iteration 23: train_loss 3.0178744792938232
Iteration 24: train_loss 3.0603599548339844
Iteration 25: train_loss 2.9461312294006348
Iteration 26: train_loss 2.8459677696228027
Iteration 27: train_loss 2.976783275604248
Iteration 28: train_loss 2.8780314922332764
Iteration 29: train_loss 2.8569462299346924
Iteration 30: train_loss 3.0478296279907227
Iteration 31: train_loss 2.979642391204834
Iteration 32: train_loss 2.9679043292999268
Iteration 33: train_loss 2.987001419067383
Iteration 34: train_loss 2.909933567047119
Iteration 35: train_loss 2.9716484546661377
Iteration 36: train_loss 2.9701924324035645
Iteration 37: train_loss 3.0119829177856445
Iteration 38: train_loss 2.919466018676758
Iteration 39: train_loss 3.03129243850708
Iteration 40: train_loss 3.047154664993286
Epoch 133: train_avg_loss 2.9828396916389464 eval_avg_acc: 0.22109256373737907 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:32:49] [32mIntermediate result: 0.22109256373737907  (Index 132)[0m
================Epoch: 134================
Iteration 1: train_loss 2.889681816101074
Iteration 2: train_loss 2.9259493350982666
Iteration 3: train_loss 3.101792573928833
Iteration 4: train_loss 3.083394765853882
Iteration 5: train_loss 3.0027389526367188
Iteration 6: train_loss 2.9125311374664307
Iteration 7: train_loss 2.959043025970459
Iteration 8: train_loss 3.0752413272857666
Iteration 9: train_loss 2.861177682876587
Iteration 10: train_loss 2.9969520568847656
Iteration 11: train_loss 3.0010859966278076
Iteration 12: train_loss 3.1106655597686768
Iteration 13: train_loss 3.1397998332977295
Iteration 14: train_loss 3.0191028118133545
Iteration 15: train_loss 3.0347838401794434
Iteration 16: train_loss 3.0850026607513428
Iteration 17: train_loss 3.058079957962036
Iteration 18: train_loss 2.967141628265381
Iteration 19: train_loss 2.8291850090026855
Iteration 20: train_loss 3.0531797409057617
Iteration 21: train_loss 2.8103744983673096
Iteration 22: train_loss 2.9009392261505127
Iteration 23: train_loss 2.991891860961914
Iteration 24: train_loss 2.928900957107544
Iteration 25: train_loss 3.0493011474609375
Iteration 26: train_loss 2.949836015701294
Iteration 27: train_loss 2.8618643283843994
Iteration 28: train_loss 2.872042417526245
Iteration 29: train_loss 2.9874942302703857
Iteration 30: train_loss 2.910492420196533
Iteration 31: train_loss 3.0734522342681885
Iteration 32: train_loss 2.9866812229156494
Iteration 33: train_loss 2.98812198638916
Iteration 34: train_loss 3.0092668533325195
Iteration 35: train_loss 2.9708523750305176
Iteration 36: train_loss 2.895852565765381
Iteration 37: train_loss 2.9433093070983887
Iteration 38: train_loss 2.990614414215088
Iteration 39: train_loss 3.143561840057373
Iteration 40: train_loss 3.145026445388794
Epoch 134: train_avg_loss 2.9879101514816284 eval_avg_acc: 0.22302133262234078 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:33:01] [32mIntermediate result: 0.22302133262234078  (Index 133)[0m
================Epoch: 135================
Iteration 1: train_loss 3.0149309635162354
Iteration 2: train_loss 2.9556047916412354
Iteration 3: train_loss 3.0679409503936768
Iteration 4: train_loss 3.04268479347229
Iteration 5: train_loss 3.0178091526031494
Iteration 6: train_loss 3.0244054794311523
Iteration 7: train_loss 3.024261713027954
Iteration 8: train_loss 2.969186782836914
Iteration 9: train_loss 2.9918673038482666
Iteration 10: train_loss 3.069847583770752
Iteration 11: train_loss 2.9401488304138184
Iteration 12: train_loss 3.0933022499084473
Iteration 13: train_loss 3.007997512817383
Iteration 14: train_loss 3.085847854614258
Iteration 15: train_loss 2.9541964530944824
Iteration 16: train_loss 2.9660472869873047
Iteration 17: train_loss 2.908234119415283
Iteration 18: train_loss 3.0706117153167725
Iteration 19: train_loss 3.075059413909912
Iteration 20: train_loss 2.8684775829315186
Iteration 21: train_loss 2.9955663681030273
Iteration 22: train_loss 2.991563320159912
Iteration 23: train_loss 2.9573752880096436
Iteration 24: train_loss 3.0666303634643555
Iteration 25: train_loss 2.951284408569336
Iteration 26: train_loss 2.9074630737304688
Iteration 27: train_loss 2.950704574584961
Iteration 28: train_loss 3.024961233139038
Iteration 29: train_loss 2.9820117950439453
Iteration 30: train_loss 2.9342827796936035
Iteration 31: train_loss 3.016556739807129
Iteration 32: train_loss 2.9121875762939453
Iteration 33: train_loss 3.0699684619903564
Iteration 34: train_loss 3.025791883468628
Iteration 35: train_loss 3.037823438644409
Iteration 36: train_loss 2.9292984008789062
Iteration 37: train_loss 3.0372934341430664
Iteration 38: train_loss 2.9751133918762207
Iteration 39: train_loss 3.0272791385650635
Iteration 40: train_loss 2.861426591873169
Epoch 135: train_avg_loss 2.9950761198997498 eval_avg_acc: 0.21706949010930296 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:33:12] [32mIntermediate result: 0.21706949010930296  (Index 134)[0m
================Epoch: 136================
Iteration 1: train_loss 2.97672438621521
Iteration 2: train_loss 3.0778074264526367
Iteration 3: train_loss 3.050847053527832
Iteration 4: train_loss 2.97554612159729
Iteration 5: train_loss 3.004845142364502
Iteration 6: train_loss 2.9853246212005615
Iteration 7: train_loss 3.053863286972046
Iteration 8: train_loss 2.9543046951293945
Iteration 9: train_loss 2.965663194656372
Iteration 10: train_loss 2.926499128341675
Iteration 11: train_loss 2.9149043560028076
Iteration 12: train_loss 3.04351806640625
Iteration 13: train_loss 2.9745402336120605
Iteration 14: train_loss 2.9778480529785156
Iteration 15: train_loss 2.8500959873199463
Iteration 16: train_loss 2.9857990741729736
Iteration 17: train_loss 2.9247303009033203
Iteration 18: train_loss 3.0311667919158936
Iteration 19: train_loss 2.910365104675293
Iteration 20: train_loss 3.0272936820983887
Iteration 21: train_loss 3.057554006576538
Iteration 22: train_loss 2.9957351684570312
Iteration 23: train_loss 3.016768455505371
Iteration 24: train_loss 3.0068795680999756
Iteration 25: train_loss 2.9605515003204346
Iteration 26: train_loss 2.944995641708374
Iteration 27: train_loss 3.1285624504089355
Iteration 28: train_loss 3.0284531116485596
Iteration 29: train_loss 2.9326608180999756
Iteration 30: train_loss 2.7983367443084717
Iteration 31: train_loss 3.0781991481781006
Iteration 32: train_loss 2.899836778640747
Iteration 33: train_loss 2.995455503463745
Iteration 34: train_loss 2.9467267990112305
Iteration 35: train_loss 3.0025956630706787
Iteration 36: train_loss 3.1332151889801025
Iteration 37: train_loss 3.04333758354187
Iteration 38: train_loss 2.8845419883728027
Iteration 39: train_loss 2.879178047180176
Iteration 40: train_loss 3.098879814147949
Epoch 136: train_avg_loss 2.986103767156601 eval_avg_acc: 0.22257984073334502 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:33:23] [32mIntermediate result: 0.22257984073334502  (Index 135)[0m
================Epoch: 137================
Iteration 1: train_loss 3.0321826934814453
Iteration 2: train_loss 2.9343390464782715
Iteration 3: train_loss 3.065854072570801
Iteration 4: train_loss 2.988406181335449
Iteration 5: train_loss 3.0921850204467773
Iteration 6: train_loss 3.0960023403167725
Iteration 7: train_loss 2.877861976623535
Iteration 8: train_loss 2.978862762451172
Iteration 9: train_loss 2.9590749740600586
Iteration 10: train_loss 2.9876937866210938
Iteration 11: train_loss 2.967681884765625
Iteration 12: train_loss 3.0570662021636963
Iteration 13: train_loss 2.9140658378601074
Iteration 14: train_loss 2.818897247314453
Iteration 15: train_loss 3.0089664459228516
Iteration 16: train_loss 2.9973723888397217
Iteration 17: train_loss 3.0033791065216064
Iteration 18: train_loss 3.0631821155548096
Iteration 19: train_loss 3.0218989849090576
Iteration 20: train_loss 2.928312301635742
Iteration 21: train_loss 2.8293416500091553
Iteration 22: train_loss 2.9153242111206055
Iteration 23: train_loss 2.9844422340393066
Iteration 24: train_loss 2.9427506923675537
Iteration 25: train_loss 2.97493314743042
Iteration 26: train_loss 3.0410685539245605
Iteration 27: train_loss 2.9738080501556396
Iteration 28: train_loss 2.883760690689087
Iteration 29: train_loss 2.888225793838501
Iteration 30: train_loss 3.0305371284484863
Iteration 31: train_loss 3.1085073947906494
Iteration 32: train_loss 3.0860955715179443
Iteration 33: train_loss 3.030245304107666
Iteration 34: train_loss 3.007143974304199
Iteration 35: train_loss 2.9791927337646484
Iteration 36: train_loss 3.019481658935547
Iteration 37: train_loss 3.0164458751678467
Iteration 38: train_loss 2.9916775226593018
Iteration 39: train_loss 3.001589775085449
Iteration 40: train_loss 2.7957448959350586
Epoch 137: train_avg_loss 2.9823400557041166 eval_avg_acc: 0.22414901319833888 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:33:35] [32mIntermediate result: 0.22414901319833888  (Index 136)[0m
================Epoch: 138================
Iteration 1: train_loss 3.02396821975708
Iteration 2: train_loss 2.9582059383392334
Iteration 3: train_loss 2.9616923332214355
Iteration 4: train_loss 2.9759552478790283
Iteration 5: train_loss 2.9634950160980225
Iteration 6: train_loss 3.147451400756836
Iteration 7: train_loss 3.0316107273101807
Iteration 8: train_loss 2.859581708908081
Iteration 9: train_loss 3.0313172340393066
Iteration 10: train_loss 3.039386034011841
Iteration 11: train_loss 2.909681558609009
Iteration 12: train_loss 3.032557964324951
Iteration 13: train_loss 2.9630026817321777
Iteration 14: train_loss 3.0560693740844727
Iteration 15: train_loss 3.0321521759033203
Iteration 16: train_loss 2.9962496757507324
Iteration 17: train_loss 2.9918570518493652
Iteration 18: train_loss 3.093458890914917
Iteration 19: train_loss 3.039013147354126
Iteration 20: train_loss 2.9111878871917725
Iteration 21: train_loss 3.1236395835876465
Iteration 22: train_loss 3.1380724906921387
Iteration 23: train_loss 2.923633337020874
Iteration 24: train_loss 2.991852045059204
Iteration 25: train_loss 3.132843017578125
Iteration 26: train_loss 2.9431850910186768
Iteration 27: train_loss 2.983353614807129
Iteration 28: train_loss 3.095874786376953
Iteration 29: train_loss 2.8822972774505615
Iteration 30: train_loss 2.979257583618164
Iteration 31: train_loss 3.0069968700408936
Iteration 32: train_loss 2.9510228633880615
Iteration 33: train_loss 2.890007495880127
Iteration 34: train_loss 3.019911766052246
Iteration 35: train_loss 2.7973666191101074
Iteration 36: train_loss 2.8383560180664062
Iteration 37: train_loss 2.8966588973999023
Iteration 38: train_loss 2.8029274940490723
Iteration 39: train_loss 2.935497760772705
Iteration 40: train_loss 3.1829309463500977
Epoch 138: train_avg_loss 2.9883394956588747 eval_avg_acc: 0.22698765897094422 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:33:46] [32mIntermediate result: 0.22698765897094422  (Index 137)[0m
================Epoch: 139================
Iteration 1: train_loss 2.8947532176971436
Iteration 2: train_loss 2.901015520095825
Iteration 3: train_loss 2.917071580886841
Iteration 4: train_loss 2.925722360610962
Iteration 5: train_loss 2.792567491531372
Iteration 6: train_loss 2.9359285831451416
Iteration 7: train_loss 2.9399302005767822
Iteration 8: train_loss 2.9129953384399414
Iteration 9: train_loss 2.8874318599700928
Iteration 10: train_loss 2.955381155014038
Iteration 11: train_loss 2.9412639141082764
Iteration 12: train_loss 2.8668079376220703
Iteration 13: train_loss 2.8906259536743164
Iteration 14: train_loss 3.0155811309814453
Iteration 15: train_loss 2.9211502075195312
Iteration 16: train_loss 3.030733346939087
Iteration 17: train_loss 2.882446050643921
Iteration 18: train_loss 2.9083783626556396
Iteration 19: train_loss 2.8652503490448
Iteration 20: train_loss 2.840296983718872
Iteration 21: train_loss 2.908127546310425
Iteration 22: train_loss 2.9490668773651123
Iteration 23: train_loss 2.9669127464294434
Iteration 24: train_loss 2.865821599960327
Iteration 25: train_loss 3.030057668685913
Iteration 26: train_loss 2.869502544403076
Iteration 27: train_loss 2.7264065742492676
Iteration 28: train_loss 3.0137789249420166
Iteration 29: train_loss 2.777772903442383
Iteration 30: train_loss 2.798095464706421
Iteration 31: train_loss 2.8203086853027344
Iteration 32: train_loss 2.9286892414093018
Iteration 33: train_loss 2.802469491958618
Iteration 34: train_loss 2.9726529121398926
Iteration 35: train_loss 3.128873825073242
Iteration 36: train_loss 2.9487507343292236
Iteration 37: train_loss 3.0095489025115967
Iteration 38: train_loss 2.9821488857269287
Iteration 39: train_loss 2.90922212600708
Iteration 40: train_loss 2.5703117847442627
Epoch 139: train_avg_loss 2.9050962746143343 eval_avg_acc: 0.22126809405532902 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:33:58] [32mIntermediate result: 0.22126809405532902  (Index 138)[0m
================Epoch: 140================
Iteration 1: train_loss 2.993082284927368
Iteration 2: train_loss 2.9699010848999023
Iteration 3: train_loss 2.961447238922119
Iteration 4: train_loss 3.034404754638672
Iteration 5: train_loss 2.920591354370117
Iteration 6: train_loss 3.063983201980591
Iteration 7: train_loss 3.1194117069244385
Iteration 8: train_loss 2.8250365257263184
Iteration 9: train_loss 3.0483672618865967
Iteration 10: train_loss 3.0680761337280273
Iteration 11: train_loss 3.094891309738159
Iteration 12: train_loss 3.0325376987457275
Iteration 13: train_loss 2.992048978805542
Iteration 14: train_loss 3.0562853813171387
Iteration 15: train_loss 2.857944965362549
Iteration 16: train_loss 2.94726824760437
Iteration 17: train_loss 3.005441188812256
Iteration 18: train_loss 3.0577468872070312
Iteration 19: train_loss 2.8417704105377197
Iteration 20: train_loss 2.961528778076172
Iteration 21: train_loss 3.025773048400879
Iteration 22: train_loss 2.849778175354004
Iteration 23: train_loss 2.84945011138916
Iteration 24: train_loss 3.104879140853882
Iteration 25: train_loss 3.038135051727295
Iteration 26: train_loss 2.826442003250122
Iteration 27: train_loss 3.005584239959717
Iteration 28: train_loss 3.0326762199401855
Iteration 29: train_loss 2.9953055381774902
Iteration 30: train_loss 3.0313141345977783
Iteration 31: train_loss 2.973792314529419
Iteration 32: train_loss 2.791750907897949
Iteration 33: train_loss 3.0225439071655273
Iteration 34: train_loss 2.9820549488067627
Iteration 35: train_loss 3.0488228797912598
Iteration 36: train_loss 3.0149059295654297
Iteration 37: train_loss 2.977611780166626
Iteration 38: train_loss 3.0268871784210205
Iteration 39: train_loss 2.9554359912872314
Iteration 40: train_loss 3.5346357822418213
Epoch 140: train_avg_loss 2.9984886169433596 eval_avg_acc: 0.223391187292976 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:34:10] [32mIntermediate result: 0.223391187292976  (Index 139)[0m
================Epoch: 141================
Iteration 1: train_loss 2.9927568435668945
Iteration 2: train_loss 2.9857382774353027
Iteration 3: train_loss 2.8992702960968018
Iteration 4: train_loss 2.8850791454315186
Iteration 5: train_loss 2.8466713428497314
Iteration 6: train_loss 2.9579477310180664
Iteration 7: train_loss 2.9220499992370605
Iteration 8: train_loss 2.9906399250030518
Iteration 9: train_loss 2.908602714538574
Iteration 10: train_loss 2.8851282596588135
Iteration 11: train_loss 2.9373767375946045
Iteration 12: train_loss 2.8706107139587402
Iteration 13: train_loss 2.8109500408172607
Iteration 14: train_loss 2.9715914726257324
Iteration 15: train_loss 2.8738653659820557
Iteration 16: train_loss 2.967297315597534
Iteration 17: train_loss 2.912496328353882
Iteration 18: train_loss 2.9993228912353516
Iteration 19: train_loss 3.0076959133148193
Iteration 20: train_loss 3.0076870918273926
Iteration 21: train_loss 2.9040136337280273
Iteration 22: train_loss 3.033820152282715
Iteration 23: train_loss 2.994523048400879
Iteration 24: train_loss 2.9948863983154297
Iteration 25: train_loss 2.9776382446289062
Iteration 26: train_loss 2.9364047050476074
Iteration 27: train_loss 3.140451192855835
Iteration 28: train_loss 2.9952454566955566
Iteration 29: train_loss 2.862182378768921
Iteration 30: train_loss 2.9177796840667725
Iteration 31: train_loss 2.889671802520752
Iteration 32: train_loss 3.025505542755127
Iteration 33: train_loss 3.0051844120025635
Iteration 34: train_loss 3.047436237335205
Iteration 35: train_loss 2.907153606414795
Iteration 36: train_loss 2.9172792434692383
Iteration 37: train_loss 2.914005994796753
Iteration 38: train_loss 2.984208583831787
Iteration 39: train_loss 2.8815202713012695
Iteration 40: train_loss 2.797429084777832
Epoch 141: train_avg_loss 2.943977952003479 eval_avg_acc: 0.21921180259696355 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:34:22] [32mIntermediate result: 0.21921180259696355  (Index 140)[0m
================Epoch: 142================
Iteration 1: train_loss 2.9197239875793457
Iteration 2: train_loss 2.9243359565734863
Iteration 3: train_loss 2.92777681350708
Iteration 4: train_loss 2.9325551986694336
Iteration 5: train_loss 3.007319688796997
Iteration 6: train_loss 3.0161876678466797
Iteration 7: train_loss 3.216181993484497
Iteration 8: train_loss 3.0076098442077637
Iteration 9: train_loss 3.0734293460845947
Iteration 10: train_loss 2.9972784519195557
Iteration 11: train_loss 2.8972434997558594
Iteration 12: train_loss 2.9790802001953125
Iteration 13: train_loss 2.991095781326294
Iteration 14: train_loss 2.9089481830596924
Iteration 15: train_loss 3.0126357078552246
Iteration 16: train_loss 3.0391180515289307
Iteration 17: train_loss 3.0734305381774902
Iteration 18: train_loss 2.8563642501831055
Iteration 19: train_loss 2.9043729305267334
Iteration 20: train_loss 2.9190752506256104
Iteration 21: train_loss 2.9140162467956543
Iteration 22: train_loss 2.8235890865325928
Iteration 23: train_loss 2.918480634689331
Iteration 24: train_loss 2.7579545974731445
Iteration 25: train_loss 2.917699098587036
Iteration 26: train_loss 2.9103174209594727
Iteration 27: train_loss 2.983991861343384
Iteration 28: train_loss 3.0232748985290527
Iteration 29: train_loss 2.8345530033111572
Iteration 30: train_loss 2.9962215423583984
Iteration 31: train_loss 2.8568103313446045
Iteration 32: train_loss 3.0839977264404297
Iteration 33: train_loss 2.923091173171997
Iteration 34: train_loss 3.010748863220215
Iteration 35: train_loss 3.0539469718933105
Iteration 36: train_loss 3.1644530296325684
Iteration 37: train_loss 2.970916271209717
Iteration 38: train_loss 2.8878161907196045
Iteration 39: train_loss 2.847338914871216
Iteration 40: train_loss 2.9050323963165283
Epoch 142: train_avg_loss 2.9597003400325774 eval_avg_acc: 0.22421853156534222 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:34:34] [32mIntermediate result: 0.22421853156534222  (Index 141)[0m
================Epoch: 143================
Iteration 1: train_loss 2.8879640102386475
Iteration 2: train_loss 3.0132133960723877
Iteration 3: train_loss 2.9210588932037354
Iteration 4: train_loss 2.9933629035949707
Iteration 5: train_loss 3.048050880432129
Iteration 6: train_loss 2.9165070056915283
Iteration 7: train_loss 2.942664384841919
Iteration 8: train_loss 2.9260671138763428
Iteration 9: train_loss 2.8705620765686035
Iteration 10: train_loss 2.950049638748169
Iteration 11: train_loss 2.750957489013672
Iteration 12: train_loss 2.906432867050171
Iteration 13: train_loss 2.862290143966675
Iteration 14: train_loss 2.9058008193969727
Iteration 15: train_loss 2.9224355220794678
Iteration 16: train_loss 2.8529300689697266
Iteration 17: train_loss 2.8676199913024902
Iteration 18: train_loss 2.9369633197784424
Iteration 19: train_loss 3.0868797302246094
Iteration 20: train_loss 2.991892099380493
Iteration 21: train_loss 2.882662296295166
Iteration 22: train_loss 2.851531982421875
Iteration 23: train_loss 3.0891218185424805
Iteration 24: train_loss 2.7775959968566895
Iteration 25: train_loss 2.9422953128814697
Iteration 26: train_loss 3.024634599685669
Iteration 27: train_loss 3.057460308074951
Iteration 28: train_loss 2.962615728378296
Iteration 29: train_loss 2.8656890392303467
Iteration 30: train_loss 2.795935869216919
Iteration 31: train_loss 2.922546863555908
Iteration 32: train_loss 3.0762298107147217
Iteration 33: train_loss 3.033848524093628
Iteration 34: train_loss 2.958547353744507
Iteration 35: train_loss 2.9508373737335205
Iteration 36: train_loss 2.8915228843688965
Iteration 37: train_loss 2.9353158473968506
Iteration 38: train_loss 2.9264211654663086
Iteration 39: train_loss 2.8663859367370605
Iteration 40: train_loss 2.7292392253875732
Epoch 143: train_avg_loss 2.92735350728035 eval_avg_acc: 0.22472912079328533 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:34:45] [32mIntermediate result: 0.22472912079328533  (Index 142)[0m
================Epoch: 144================
Iteration 1: train_loss 2.9291884899139404
Iteration 2: train_loss 3.057671546936035
Iteration 3: train_loss 3.079782485961914
Iteration 4: train_loss 2.8597445487976074
Iteration 5: train_loss 3.0913355350494385
Iteration 6: train_loss 3.0042171478271484
Iteration 7: train_loss 2.9048635959625244
Iteration 8: train_loss 2.920811653137207
Iteration 9: train_loss 2.9375181198120117
Iteration 10: train_loss 3.023181915283203
Iteration 11: train_loss 2.972618818283081
Iteration 12: train_loss 2.8723156452178955
Iteration 13: train_loss 2.8904170989990234
Iteration 14: train_loss 2.9576752185821533
Iteration 15: train_loss 3.0998458862304688
Iteration 16: train_loss 3.0535521507263184
Iteration 17: train_loss 2.9787395000457764
Iteration 18: train_loss 2.939627170562744
Iteration 19: train_loss 2.913844108581543
Iteration 20: train_loss 2.8320770263671875
Iteration 21: train_loss 2.9293570518493652
Iteration 22: train_loss 2.840843439102173
Iteration 23: train_loss 2.968809127807617
Iteration 24: train_loss 2.960735321044922
Iteration 25: train_loss 2.879657030105591
Iteration 26: train_loss 3.0000076293945312
Iteration 27: train_loss 3.059882164001465
Iteration 28: train_loss 2.929537534713745
Iteration 29: train_loss 3.0370514392852783
Iteration 30: train_loss 2.97912859916687
Iteration 31: train_loss 2.9193084239959717
Iteration 32: train_loss 3.0607352256774902
Iteration 33: train_loss 2.9631004333496094
Iteration 34: train_loss 2.860872268676758
Iteration 35: train_loss 3.0297162532806396
Iteration 36: train_loss 2.7863874435424805
Iteration 37: train_loss 2.862821578979492
Iteration 38: train_loss 2.9355826377868652
Iteration 39: train_loss 2.8960278034210205
Iteration 40: train_loss 3.157839059829712
Epoch 144: train_avg_loss 2.9594107031822205 eval_avg_acc: 0.22042611276776897 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:34:54] [32mIntermediate result: 0.22042611276776897  (Index 143)[0m
================Epoch: 145================
Iteration 1: train_loss 3.009096622467041
Iteration 2: train_loss 3.093973159790039
Iteration 3: train_loss 3.0167059898376465
Iteration 4: train_loss 2.9547719955444336
Iteration 5: train_loss 2.980654001235962
Iteration 6: train_loss 2.9577555656433105
Iteration 7: train_loss 3.031766414642334
Iteration 8: train_loss 3.041327476501465
Iteration 9: train_loss 2.9398770332336426
Iteration 10: train_loss 2.9241182804107666
Iteration 11: train_loss 3.1223127841949463
Iteration 12: train_loss 3.04919695854187
Iteration 13: train_loss 3.0155575275421143
Iteration 14: train_loss 2.9014999866485596
Iteration 15: train_loss 2.9895331859588623
Iteration 16: train_loss 3.033170223236084
Iteration 17: train_loss 2.8717565536499023
Iteration 18: train_loss 2.9235026836395264
Iteration 19: train_loss 2.9510087966918945
Iteration 20: train_loss 3.012371301651001
Iteration 21: train_loss 2.913088798522949
Iteration 22: train_loss 2.942304849624634
Iteration 23: train_loss 2.9961636066436768
Iteration 24: train_loss 2.982328414916992
Iteration 25: train_loss 3.024451732635498
Iteration 26: train_loss 2.9030425548553467
Iteration 27: train_loss 2.9463233947753906
Iteration 28: train_loss 3.0244033336639404
Iteration 29: train_loss 2.8185620307922363
Iteration 30: train_loss 2.8307881355285645
Iteration 31: train_loss 2.9334311485290527
Iteration 32: train_loss 2.952840566635132
Iteration 33: train_loss 2.9897642135620117
Iteration 34: train_loss 2.927760601043701
Iteration 35: train_loss 2.909379005432129
Iteration 36: train_loss 2.9356346130371094
Iteration 37: train_loss 3.0176193714141846
Iteration 38: train_loss 2.896974563598633
Iteration 39: train_loss 2.984551429748535
Iteration 40: train_loss 2.662243604660034
Epoch 145: train_avg_loss 2.960290312767029 eval_avg_acc: 0.22096075011342475 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:35:03] [32mIntermediate result: 0.22096075011342475  (Index 144)[0m
================Epoch: 146================
Iteration 1: train_loss 2.8980486392974854
Iteration 2: train_loss 2.9898931980133057
Iteration 3: train_loss 2.9861371517181396
Iteration 4: train_loss 2.978881359100342
Iteration 5: train_loss 2.946315288543701
Iteration 6: train_loss 2.8518996238708496
Iteration 7: train_loss 2.9565396308898926
Iteration 8: train_loss 3.16943097114563
Iteration 9: train_loss 2.9462637901306152
Iteration 10: train_loss 2.8945388793945312
Iteration 11: train_loss 3.0261616706848145
Iteration 12: train_loss 2.842773914337158
Iteration 13: train_loss 2.996716260910034
Iteration 14: train_loss 2.961472272872925
Iteration 15: train_loss 2.946080207824707
Iteration 16: train_loss 3.002558469772339
Iteration 17: train_loss 3.01719069480896
Iteration 18: train_loss 2.92025089263916
Iteration 19: train_loss 3.0178465843200684
Iteration 20: train_loss 2.970592498779297
Iteration 21: train_loss 3.012106418609619
Iteration 22: train_loss 3.030827760696411
Iteration 23: train_loss 2.9543983936309814
Iteration 24: train_loss 2.846996545791626
Iteration 25: train_loss 2.8422365188598633
Iteration 26: train_loss 2.9059784412384033
Iteration 27: train_loss 2.9974820613861084
Iteration 28: train_loss 2.905482769012451
Iteration 29: train_loss 2.9921908378601074
Iteration 30: train_loss 2.868422508239746
Iteration 31: train_loss 2.9945337772369385
Iteration 32: train_loss 2.9626305103302
Iteration 33: train_loss 3.006901979446411
Iteration 34: train_loss 3.0120809078216553
Iteration 35: train_loss 2.895127058029175
Iteration 36: train_loss 2.9289426803588867
Iteration 37: train_loss 2.944702386856079
Iteration 38: train_loss 2.8490543365478516
Iteration 39: train_loss 2.9436049461364746
Iteration 40: train_loss 3.1651008129119873
Epoch 146: train_avg_loss 2.9594598412513733 eval_avg_acc: 0.2235411282956564 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:35:13] [32mIntermediate result: 0.2235411282956564  (Index 145)[0m
================Epoch: 147================
Iteration 1: train_loss 2.8661751747131348
Iteration 2: train_loss 2.8651278018951416
Iteration 3: train_loss 2.994138479232788
Iteration 4: train_loss 3.0764591693878174
Iteration 5: train_loss 2.9679534435272217
Iteration 6: train_loss 3.020366907119751
Iteration 7: train_loss 2.9571776390075684
Iteration 8: train_loss 2.8067634105682373
Iteration 9: train_loss 2.990678071975708
Iteration 10: train_loss 2.9671924114227295
Iteration 11: train_loss 3.0046615600585938
Iteration 12: train_loss 3.2112619876861572
Iteration 13: train_loss 3.011164665222168
Iteration 14: train_loss 2.9923317432403564
Iteration 15: train_loss 2.9672868251800537
Iteration 16: train_loss 2.8771746158599854
Iteration 17: train_loss 3.052777051925659
Iteration 18: train_loss 3.164850950241089
Iteration 19: train_loss 2.8619391918182373
Iteration 20: train_loss 2.9949193000793457
Iteration 21: train_loss 2.993989944458008
Iteration 22: train_loss 2.9294445514678955
Iteration 23: train_loss 2.9976067543029785
Iteration 24: train_loss 2.920572280883789
Iteration 25: train_loss 2.926748752593994
Iteration 26: train_loss 2.9704337120056152
Iteration 27: train_loss 2.802969217300415
Iteration 28: train_loss 2.9330849647521973
Iteration 29: train_loss 2.9907190799713135
Iteration 30: train_loss 2.9329299926757812
Iteration 31: train_loss 3.032170295715332
Iteration 32: train_loss 2.7632369995117188
Iteration 33: train_loss 2.918229103088379
Iteration 34: train_loss 2.920100450515747
Iteration 35: train_loss 2.9876270294189453
Iteration 36: train_loss 2.833987236022949
Iteration 37: train_loss 2.985419988632202
Iteration 38: train_loss 3.0387399196624756
Iteration 39: train_loss 3.0077810287475586
Iteration 40: train_loss 2.5310206413269043
Epoch 147: train_avg_loss 2.9516803085803986 eval_avg_acc: 0.21995740108514084 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:35:25] [32mIntermediate result: 0.21995740108514084  (Index 146)[0m
================Epoch: 148================
Iteration 1: train_loss 2.9512054920196533
Iteration 2: train_loss 2.9563589096069336
Iteration 3: train_loss 2.7948124408721924
Iteration 4: train_loss 2.913191318511963
Iteration 5: train_loss 2.9184725284576416
Iteration 6: train_loss 3.0152525901794434
Iteration 7: train_loss 2.920182704925537
Iteration 8: train_loss 2.9723081588745117
Iteration 9: train_loss 2.9578442573547363
Iteration 10: train_loss 2.9702515602111816
Iteration 11: train_loss 2.8154056072235107
Iteration 12: train_loss 2.956047534942627
Iteration 13: train_loss 2.9865963459014893
Iteration 14: train_loss 2.806175947189331
Iteration 15: train_loss 2.929868459701538
Iteration 16: train_loss 2.8035902976989746
Iteration 17: train_loss 2.8997530937194824
Iteration 18: train_loss 2.9044029712677
Iteration 19: train_loss 2.921085834503174
Iteration 20: train_loss 2.817793846130371
Iteration 21: train_loss 2.9157299995422363
Iteration 22: train_loss 3.0415515899658203
Iteration 23: train_loss 2.9958655834198
Iteration 24: train_loss 2.9314472675323486
Iteration 25: train_loss 2.9105916023254395
Iteration 26: train_loss 2.7905020713806152
Iteration 27: train_loss 2.888704538345337
Iteration 28: train_loss 2.9210565090179443
Iteration 29: train_loss 2.8568618297576904
Iteration 30: train_loss 2.9561691284179688
Iteration 31: train_loss 2.8956658840179443
Iteration 32: train_loss 2.951828956604004
Iteration 33: train_loss 2.940974235534668
Iteration 34: train_loss 2.9758849143981934
Iteration 35: train_loss 2.9912726879119873
Iteration 36: train_loss 2.987658977508545
Iteration 37: train_loss 2.885976791381836
Iteration 38: train_loss 2.8703112602233887
Iteration 39: train_loss 3.078390598297119
Iteration 40: train_loss 2.7748074531555176
Epoch 148: train_avg_loss 2.91929629445076 eval_avg_acc: 0.2244049690720976 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:35:37] [32mIntermediate result: 0.2244049690720976  (Index 147)[0m
================Epoch: 149================
Iteration 1: train_loss 2.9737050533294678
Iteration 2: train_loss 3.040710926055908
Iteration 3: train_loss 2.8852314949035645
Iteration 4: train_loss 3.0653884410858154
Iteration 5: train_loss 3.072125196456909
Iteration 6: train_loss 3.0469398498535156
Iteration 7: train_loss 3.04272723197937
Iteration 8: train_loss 3.105926275253296
Iteration 9: train_loss 2.9281668663024902
Iteration 10: train_loss 2.942260980606079
Iteration 11: train_loss 3.089327573776245
Iteration 12: train_loss 3.0400900840759277
Iteration 13: train_loss 3.15242862701416
Iteration 14: train_loss 3.06834077835083
Iteration 15: train_loss 3.065559148788452
Iteration 16: train_loss 3.0255661010742188
Iteration 17: train_loss 2.922593832015991
Iteration 18: train_loss 2.875514030456543
Iteration 19: train_loss 2.9365267753601074
Iteration 20: train_loss 2.784533739089966
Iteration 21: train_loss 2.9355387687683105
Iteration 22: train_loss 2.8621459007263184
Iteration 23: train_loss 2.9641733169555664
Iteration 24: train_loss 2.87817120552063
Iteration 25: train_loss 2.7780814170837402
Iteration 26: train_loss 2.9306676387786865
Iteration 27: train_loss 2.8256947994232178
Iteration 28: train_loss 2.8894221782684326
Iteration 29: train_loss 2.8358235359191895
Iteration 30: train_loss 2.793931722640991
Iteration 31: train_loss 2.8479764461517334
Iteration 32: train_loss 2.8168177604675293
Iteration 33: train_loss 2.8092503547668457
Iteration 34: train_loss 2.9010791778564453
Iteration 35: train_loss 2.8666024208068848
Iteration 36: train_loss 2.892746925354004
Iteration 37: train_loss 2.879934072494507
Iteration 38: train_loss 2.87954044342041
Iteration 39: train_loss 2.915161371231079
Iteration 40: train_loss 3.3700273036956787
Epoch 149: train_avg_loss 2.9484112441539763 eval_avg_acc: 0.22489922256865036 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:35:49] [32mIntermediate result: 0.22489922256865036  (Index 148)[0m
================Epoch: 150================
Iteration 1: train_loss 2.929522752761841
Iteration 2: train_loss 2.913224697113037
Iteration 3: train_loss 2.894556999206543
Iteration 4: train_loss 2.948765754699707
Iteration 5: train_loss 2.873248815536499
Iteration 6: train_loss 2.9549670219421387
Iteration 7: train_loss 2.8234236240386963
Iteration 8: train_loss 2.9513285160064697
Iteration 9: train_loss 2.989475965499878
Iteration 10: train_loss 2.8619000911712646
Iteration 11: train_loss 2.751863718032837
Iteration 12: train_loss 2.863020420074463
Iteration 13: train_loss 2.8559632301330566
Iteration 14: train_loss 2.76763653755188
Iteration 15: train_loss 2.784590244293213
Iteration 16: train_loss 2.8738155364990234
Iteration 17: train_loss 2.8458364009857178
Iteration 18: train_loss 2.96706223487854
Iteration 19: train_loss 2.940908670425415
Iteration 20: train_loss 2.87534499168396
Iteration 21: train_loss 2.8640499114990234
Iteration 22: train_loss 2.906324863433838
Iteration 23: train_loss 2.812905788421631
Iteration 24: train_loss 3.0029995441436768
Iteration 25: train_loss 2.8156325817108154
Iteration 26: train_loss 2.834348678588867
Iteration 27: train_loss 2.728895902633667
Iteration 28: train_loss 2.815969228744507
Iteration 29: train_loss 2.7611918449401855
Iteration 30: train_loss 2.940359115600586
Iteration 31: train_loss 2.7526702880859375
Iteration 32: train_loss 2.88683819770813
Iteration 33: train_loss 2.8726212978363037
Iteration 34: train_loss 2.998774528503418
Iteration 35: train_loss 2.798750877380371
Iteration 36: train_loss 2.9416823387145996
Iteration 37: train_loss 2.809366464614868
Iteration 38: train_loss 2.9195590019226074
Iteration 39: train_loss 2.8887290954589844
Iteration 40: train_loss 2.85725474357605
Epoch 150: train_avg_loss 2.871884512901306 eval_avg_acc: 0.2209554317543097 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:36:02] [32mIntermediate result: 0.2209554317543097  (Index 149)[0m
================Epoch: 151================
Iteration 1: train_loss 2.8905930519104004
Iteration 2: train_loss 2.90816330909729
Iteration 3: train_loss 2.725550889968872
Iteration 4: train_loss 2.99711537361145
Iteration 5: train_loss 2.939168691635132
Iteration 6: train_loss 2.898716688156128
Iteration 7: train_loss 2.8839495182037354
Iteration 8: train_loss 2.8200323581695557
Iteration 9: train_loss 2.8835394382476807
Iteration 10: train_loss 2.909818172454834
Iteration 11: train_loss 2.982497453689575
Iteration 12: train_loss 2.7824528217315674
Iteration 13: train_loss 2.9251177310943604
Iteration 14: train_loss 2.7603657245635986
Iteration 15: train_loss 2.9125869274139404
Iteration 16: train_loss 3.0220367908477783
Iteration 17: train_loss 2.9028615951538086
Iteration 18: train_loss 2.8075485229492188
Iteration 19: train_loss 2.965456485748291
Iteration 20: train_loss 2.9404959678649902
Iteration 21: train_loss 2.764258623123169
Iteration 22: train_loss 2.8631186485290527
Iteration 23: train_loss 2.951822519302368
Iteration 24: train_loss 2.860330104827881
Iteration 25: train_loss 2.777294397354126
Iteration 26: train_loss 2.775228500366211
Iteration 27: train_loss 2.9851953983306885
Iteration 28: train_loss 2.9890317916870117
Iteration 29: train_loss 2.850098133087158
Iteration 30: train_loss 2.901644468307495
Iteration 31: train_loss 2.874187707901001
Iteration 32: train_loss 2.945483446121216
Iteration 33: train_loss 2.8511128425598145
Iteration 34: train_loss 2.852652072906494
Iteration 35: train_loss 2.775831699371338
Iteration 36: train_loss 3.009754180908203
Iteration 37: train_loss 2.887965679168701
Iteration 38: train_loss 3.0271947383880615
Iteration 39: train_loss 2.906126022338867
Iteration 40: train_loss 2.986229181289673
Epoch 151: train_avg_loss 2.8923156917095185 eval_avg_acc: 0.22604863428926922 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:36:14] [32mIntermediate result: 0.22604863428926922  (Index 150)[0m
================Epoch: 152================
Iteration 1: train_loss 2.8354861736297607
Iteration 2: train_loss 2.769805431365967
Iteration 3: train_loss 2.9569883346557617
Iteration 4: train_loss 2.918388843536377
Iteration 5: train_loss 2.9407472610473633
Iteration 6: train_loss 3.0054609775543213
Iteration 7: train_loss 2.865936756134033
Iteration 8: train_loss 3.011107921600342
Iteration 9: train_loss 3.0412964820861816
Iteration 10: train_loss 2.927593231201172
Iteration 11: train_loss 2.9290714263916016
Iteration 12: train_loss 2.996861219406128
Iteration 13: train_loss 2.971403121948242
Iteration 14: train_loss 2.8973817825317383
Iteration 15: train_loss 2.9354496002197266
Iteration 16: train_loss 2.955249547958374
Iteration 17: train_loss 2.9827425479888916
Iteration 18: train_loss 2.9853103160858154
Iteration 19: train_loss 2.7690937519073486
Iteration 20: train_loss 2.9414732456207275
Iteration 21: train_loss 2.917980670928955
Iteration 22: train_loss 2.8972463607788086
Iteration 23: train_loss 2.9918453693389893
Iteration 24: train_loss 2.7979848384857178
Iteration 25: train_loss 2.9536707401275635
Iteration 26: train_loss 2.892404079437256
Iteration 27: train_loss 2.902590751647949
Iteration 28: train_loss 2.870229959487915
Iteration 29: train_loss 2.956585645675659
Iteration 30: train_loss 2.924269676208496
Iteration 31: train_loss 2.8806262016296387
Iteration 32: train_loss 2.9403090476989746
Iteration 33: train_loss 2.7993061542510986
Iteration 34: train_loss 2.7826197147369385
Iteration 35: train_loss 2.825728416442871
Iteration 36: train_loss 2.7884488105773926
Iteration 37: train_loss 2.8059256076812744
Iteration 38: train_loss 3.0225815773010254
Iteration 39: train_loss 2.941754102706909
Iteration 40: train_loss 2.9164228439331055
Epoch 152: train_avg_loss 2.9111344635486605 eval_avg_acc: 0.22410932112359325 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:36:26] [32mIntermediate result: 0.22410932112359325  (Index 151)[0m
================Epoch: 153================
Iteration 1: train_loss 3.059971570968628
Iteration 2: train_loss 2.9579524993896484
Iteration 3: train_loss 2.950259208679199
Iteration 4: train_loss 2.821265459060669
Iteration 5: train_loss 2.852092981338501
Iteration 6: train_loss 2.8611600399017334
Iteration 7: train_loss 2.9494576454162598
Iteration 8: train_loss 2.994224786758423
Iteration 9: train_loss 2.883009910583496
Iteration 10: train_loss 2.9243457317352295
Iteration 11: train_loss 2.9002435207366943
Iteration 12: train_loss 3.0033726692199707
Iteration 13: train_loss 2.8803091049194336
Iteration 14: train_loss 2.8080697059631348
Iteration 15: train_loss 2.944216251373291
Iteration 16: train_loss 2.9408178329467773
Iteration 17: train_loss 2.9425439834594727
Iteration 18: train_loss 2.853532314300537
Iteration 19: train_loss 2.9597818851470947
Iteration 20: train_loss 2.8146965503692627
Iteration 21: train_loss 2.8308024406433105
Iteration 22: train_loss 2.833730697631836
Iteration 23: train_loss 2.8471169471740723
Iteration 24: train_loss 2.889831304550171
Iteration 25: train_loss 2.8490357398986816
Iteration 26: train_loss 2.764052152633667
Iteration 27: train_loss 2.814833164215088
Iteration 28: train_loss 2.8582985401153564
Iteration 29: train_loss 2.7943778038024902
Iteration 30: train_loss 2.7965404987335205
Iteration 31: train_loss 2.8194468021392822
Iteration 32: train_loss 2.9120969772338867
Iteration 33: train_loss 2.7776236534118652
Iteration 34: train_loss 2.9075710773468018
Iteration 35: train_loss 2.8476061820983887
Iteration 36: train_loss 2.7721548080444336
Iteration 37: train_loss 2.7480154037475586
Iteration 38: train_loss 2.767998218536377
Iteration 39: train_loss 2.7959399223327637
Iteration 40: train_loss 3.0023486614227295
Epoch 153: train_avg_loss 2.8732686161994936 eval_avg_acc: 0.22509084070889446 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:36:39] [32mIntermediate result: 0.22509084070889446  (Index 152)[0m
================Epoch: 154================
Iteration 1: train_loss 2.743424415588379
Iteration 2: train_loss 2.7789876461029053
Iteration 3: train_loss 2.8369555473327637
Iteration 4: train_loss 2.931593894958496
Iteration 5: train_loss 2.98232364654541
Iteration 6: train_loss 2.7192888259887695
Iteration 7: train_loss 2.8089451789855957
Iteration 8: train_loss 2.8539936542510986
Iteration 9: train_loss 2.8212504386901855
Iteration 10: train_loss 2.8658699989318848
Iteration 11: train_loss 2.7987568378448486
Iteration 12: train_loss 2.949547052383423
Iteration 13: train_loss 2.7260355949401855
Iteration 14: train_loss 2.7779877185821533
Iteration 15: train_loss 2.7981107234954834
Iteration 16: train_loss 2.820523738861084
Iteration 17: train_loss 2.8158652782440186
Iteration 18: train_loss 2.9251937866210938
Iteration 19: train_loss 2.7355234622955322
Iteration 20: train_loss 2.8507261276245117
Iteration 21: train_loss 2.844874143600464
Iteration 22: train_loss 2.895393133163452
Iteration 23: train_loss 2.8383705615997314
Iteration 24: train_loss 2.8385772705078125
Iteration 25: train_loss 2.9246599674224854
Iteration 26: train_loss 3.016620397567749
Iteration 27: train_loss 2.8840177059173584
Iteration 28: train_loss 2.8603830337524414
Iteration 29: train_loss 2.834064483642578
Iteration 30: train_loss 2.87507963180542
Iteration 31: train_loss 2.8645436763763428
Iteration 32: train_loss 2.8619987964630127
Iteration 33: train_loss 2.8675754070281982
Iteration 34: train_loss 2.8786509037017822
Iteration 35: train_loss 2.854668378829956
Iteration 36: train_loss 2.8076915740966797
Iteration 37: train_loss 2.7473154067993164
Iteration 38: train_loss 2.9139175415039062
Iteration 39: train_loss 2.907531976699829
Iteration 40: train_loss 2.408895492553711
Epoch 154: train_avg_loss 2.836643326282501 eval_avg_acc: 0.22377482106332316 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:36:50] [32mIntermediate result: 0.22377482106332316  (Index 153)[0m
================Epoch: 155================
Iteration 1: train_loss 2.92803955078125
Iteration 2: train_loss 2.9577934741973877
Iteration 3: train_loss 2.8709347248077393
Iteration 4: train_loss 2.85561466217041
Iteration 5: train_loss 2.938894510269165
Iteration 6: train_loss 2.8613216876983643
Iteration 7: train_loss 3.0082340240478516
Iteration 8: train_loss 2.743795394897461
Iteration 9: train_loss 2.9559450149536133
Iteration 10: train_loss 2.8174562454223633
Iteration 11: train_loss 2.864830255508423
Iteration 12: train_loss 2.920814037322998
Iteration 13: train_loss 2.842658042907715
Iteration 14: train_loss 2.924126148223877
Iteration 15: train_loss 2.8447768688201904
Iteration 16: train_loss 2.85593318939209
Iteration 17: train_loss 2.9552881717681885
Iteration 18: train_loss 2.80857253074646
Iteration 19: train_loss 2.962855100631714
Iteration 20: train_loss 3.0022292137145996
Iteration 21: train_loss 2.8048627376556396
Iteration 22: train_loss 2.941920042037964
Iteration 23: train_loss 2.881967782974243
Iteration 24: train_loss 2.9387521743774414
Iteration 25: train_loss 2.7587785720825195
Iteration 26: train_loss 2.915031671524048
Iteration 27: train_loss 2.8157806396484375
Iteration 28: train_loss 2.756633758544922
Iteration 29: train_loss 2.7766928672790527
Iteration 30: train_loss 2.7486650943756104
Iteration 31: train_loss 2.9262022972106934
Iteration 32: train_loss 2.7176101207733154
Iteration 33: train_loss 2.8450162410736084
Iteration 34: train_loss 2.8716399669647217
Iteration 35: train_loss 2.831667423248291
Iteration 36: train_loss 2.928025722503662
Iteration 37: train_loss 2.8195910453796387
Iteration 38: train_loss 2.921039581298828
Iteration 39: train_loss 2.7371392250061035
Iteration 40: train_loss 2.516120433807373
Epoch 155: train_avg_loss 2.8593312561511994 eval_avg_acc: 0.2239238687120908 eval_avg_r 0.0 eval_avg_p 0.0
[2022-10-12 01:37:02] [32mIntermediate result: 0.2239238687120908  (Index 154)[0m
================Epoch: 156================
Iteration 1: train_loss 2.7578673362731934
Iteration 2: train_loss 2.8954553604125977
Iteration 3: train_loss 2.7554280757904053
Iteration 4: train_loss 2.8669321537017822
Iteration 5: train_loss 2.923001766204834
Iteration 6: train_loss 2.967949867248535
Iteration 7: train_loss 2.979987859725952
Iteration 8: train_loss 2.899160385131836
Iteration 9: train_loss 2.85404896736145
Iteration 10: train_loss 2.9308958053588867
Iteration 11: train_loss 2.8320131301879883
Iteration 12: train_loss 2.7231013774871826
Iteration 13: train_loss 2.913311004638672
Iteration 14: train_loss 2.855593204498291
Iteration 15: train_loss 2.9793033599853516
Iteration 16: train_loss 2.880284070968628
Iteration 17: train_loss 2.986891508102417
Iteration 18: train_loss 2.983595848083496
Iteration 19: train_loss 2.9280455112457275
Iteration 20: train_loss 2.9360511302948
Iteration 21: train_loss 2.819802761077881
Iteration 22: train_loss 2.9706485271453857
Iteration 23: train_loss 3.032174825668335
Iteration 24: train_loss 3.0472664833068848
Iteration 25: train_loss 2.8190414905548096
Iteration 26: train_loss 2.9084317684173584
Iteration 27: train_loss 2.909257173538208
Iteration 28: train_loss 2.8944411277770996
Iteration 29: train_loss 2.967198610305786
Iteration 30: train_loss 2.905864715576172
Iteration 31: train_loss 2.8353395462036133
